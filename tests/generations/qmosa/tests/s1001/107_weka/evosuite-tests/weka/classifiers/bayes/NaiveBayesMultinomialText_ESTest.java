/*
 * This file was automatically generated by EvoSuite
 * Fri Nov 15 04:57:27 GMT 2019
 */

package weka.classifiers.bayes;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.File;
import java.util.ArrayList;
import java.util.Collection;
import java.util.LinkedList;
import java.util.List;
import java.util.Locale;
import java.util.Properties;
import java.util.Set;
import java.util.Vector;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.Random;
import org.evosuite.runtime.System;
import org.evosuite.runtime.mock.java.io.MockFile;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.bayes.NaiveBayesMultinomialText;
import weka.classifiers.functions.LinearRegression;
import weka.classifiers.functions.SGDText;
import weka.classifiers.functions.supportVector.Kernel;
import weka.classifiers.functions.supportVector.PrecomputedKernelMatrixKernel;
import weka.classifiers.lazy.IBk;
import weka.classifiers.meta.CostSensitiveClassifier;
import weka.classifiers.meta.FilteredClassifier;
import weka.classifiers.misc.SerializedClassifier;
import weka.core.Attribute;
import weka.core.BinarySparseInstance;
import weka.core.Capabilities;
import weka.core.DenseInstance;
import weka.core.Environment;
import weka.core.FindWithCapabilities;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.ProtectedProperties;
import weka.core.SparseInstance;
import weka.core.Stopwords;
import weka.core.TestInstances;
import weka.core.neighboursearch.CoverTree;
import weka.core.neighboursearch.balltrees.BallNode;
import weka.core.stemmers.IteratedLovinsStemmer;
import weka.core.stemmers.NullStemmer;
import weka.core.stemmers.SnowballStemmer;
import weka.core.stemmers.Stemmer;
import weka.core.tokenizers.AlphabeticTokenizer;
import weka.core.tokenizers.NGramTokenizer;
import weka.core.tokenizers.Tokenizer;
import weka.core.tokenizers.WordTokenizer;
import weka.estimators.NormalEstimator;
import weka.filters.MultiFilter;
import weka.filters.supervised.attribute.Discretize;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class NaiveBayesMultinomialText_ESTest extends NaiveBayesMultinomialText_ESTest_scaffolding {

  /**
  //Test case number: 0
  /*Coverage entropy=2.515225435791386
  */
  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate("If true, ignores all words that are on the stoplist.");
      capabilities0.getClassCapabilities();
      naiveBayesMultinomialText0.m_normalize = true;
      instances0.meanOrMode(0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      testInstances0.setNumRelationalNominalValues(1017);
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("weka/core/Capabilities.props");
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.STRING_CLASS;
      capabilities1.enableDependency(capabilities_Capability0);
      SnowballStemmer.main(testInstances0.DEFAULT_WORDS);
      ArrayList<Locale.LanguageRange> arrayList0 = new ArrayList<Locale.LanguageRange>();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      naiveBayesMultinomialText0.periodicPruningTipText();
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2867), doubleArray0);
      binarySparseInstance0.toStringNoWeight();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.normTipText();
      double[] doubleArray1 = naiveBayesMultinomialText0.m_probOfClass;
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.getNormalizeDocLength();
      naiveBayesMultinomialText0.useStopListTipText();
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 1
  /*Coverage entropy=2.7204225901862604
  */
  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = TestInstances.DEFAULT_WORDS;
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/tmp/:y!{W)yqN<Enz1q0[dg");
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      byte[] byteArray0 = new byte[9];
      byteArray0[0] = (byte)1;
      byteArray0[1] = (byte) (-4);
      byteArray0[2] = (byte) (-105);
      byteArray0[3] = (byte)13;
      byteArray0[4] = (byte) (-110);
      byteArray0[5] = (byte) (-7);
      byteArray0[6] = (byte) (-1);
      byteArray0[7] = (byte)11;
      byteArray0[8] = (byte)63;
      FileSystemHandling.appendDataToFile(evoSuiteFile0, byteArray0);
      MockFile mockFile0 = new MockFile("The year of publication or, for an unpublished work, the year it was written. Generally it should consist of four numerals, such as 1984, although the standard styles can handle any year whose last four nonpunctuation characters are numerals, such as `hbox{(about 1984)}'.");
      mockFile0.deleteOnExit();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      MockFile mockFile1 = new MockFile(mockFile0, "The year of publication or, for an unpublished work, the year it was written. Generally it should consist of four numerals, such as 1984, although the standard styles can handle any year whose last four nonpunctuation characters are numerals, such as `hbox{(about 1984)}'.");
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      File file0 = precomputedKernelMatrixKernel0.getKernelMatrixFile();
      MockFile mockFile2 = new MockFile(file0, ":y!{W)yqN<Enz");
      naiveBayesMultinomialText0.setOptions(stringArray0);
      naiveBayesMultinomialText0.useStopListTipText();
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 2
  /*Coverage entropy=2.253297930766516
  */
  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      double[] doubleArray0 = new double[6];
      SparseInstance sparseInstance0 = new SparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      SparseInstance sparseInstance1 = new SparseInstance((-2926.0), doubleArray0);
      sparseInstance1.toStringNoWeight(9);
      double[] doubleArray1 = new double[8];
      Attribute attribute0 = new Attribute("cid", 325);
      sparseInstance0.setValue(attribute0, (double) (-2867));
      doubleArray1[0] = (double) (-2867);
      sparseInstance1.s_numericAfterDecimalPoint = (-2867);
      doubleArray1[2] = (double) (-2867);
      doubleArray1[3] = (-99.0);
      doubleArray1[4] = (double) (-2867);
      doubleArray1[5] = 7.577107088453127;
      doubleArray1[6] = (double) (-2867);
      sparseInstance1.setValue(attribute0, 23.0);
      doubleArray1[7] = (-99.0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance(1.0, doubleArray1);
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance((SparseInstance) binarySparseInstance2);
      naiveBayesMultinomialText0.setPeriodicPruning((-2867));
      FileSystemHandling.setPermissions((EvoSuiteFile) null, false, false, false);
      NullStemmer nullStemmer0 = new NullStemmer();
      naiveBayesMultinomialText0.pruneDictionary();
      naiveBayesMultinomialText0.stemmerTipText();
      assertEquals((-2867), naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 3
  /*Coverage entropy=2.3999282121258494
  */
  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string0);
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      SparseInstance sparseInstance0 = new SparseInstance(550);
      TestInstances testInstances1 = TestInstances.forCapabilities(capabilities0);
      testInstances0.assign(testInstances1);
      instances0.add((Instance) sparseInstance0);
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.EMPTY_NOMINAL_ATTRIBUTES;
      capabilities1.enableAllClasses();
      capabilities1.enable(capabilities_Capability0);
      instances0.meanOrMode(0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("weka/core/Capabilities.props");
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      Capabilities.Capability capabilities_Capability1 = Capabilities.Capability.STRING_CLASS;
      capabilities1.enableDependency(capabilities_Capability1);
      SnowballStemmer.main(testInstances0.DEFAULT_WORDS);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      ArrayList<Locale.LanguageRange> arrayList0 = new ArrayList<Locale.LanguageRange>();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertArrayEquals(new double[] {0.6818181818181819, 0.3181818181818182}, doubleArray0, 0.01);
      
      naiveBayesMultinomialText0.periodicPruningTipText();
      LinkedList<Locale.LanguageRange> linkedList0 = new LinkedList<Locale.LanguageRange>();
      naiveBayesMultinomialText0.stemmerTipText();
      String string1 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t15.0\nclass2\t7.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\t\nover\t7.38905609893065\t7.38905609893065\t\nthe\t7.38905609893065\t7.38905609893065\t\nThe\t20.085536923187668\t2.718281828459045\t\nquick\t20.085536923187668\t7.38905609893065\t\nlazy\t20.085536923187668\t2.718281828459045\t\njumps\t20.085536923187668\t7.38905609893065\t\nbrown\t7.38905609893065\t7.38905609893065\t\ndog\t20.085536923187668\t2.718281828459045\t\nfox\t7.38905609893065\t7.38905609893065\t\n", string1);
      
      TestInstances testInstances2 = new TestInstances();
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText0.normTipText();
      String string2 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string2);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 4
  /*Coverage entropy=2.8224483900748307
  */
  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      double[] doubleArray0 = new double[6];
      SparseInstance sparseInstance0 = new SparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      SparseInstance sparseInstance1 = new SparseInstance((-2926.0), doubleArray0);
      sparseInstance1.toStringNoWeight(9);
      String[] stringArray0 = new String[6];
      stringArray0[0] = "{}";
      stringArray0[1] = "The tokenizing algorithm to use on the strings.";
      stringArray0[2] = "{}";
      stringArray0[3] = "The tokenizing algorithm to use on the strings.";
      stringArray0[4] = "The tokenizing algorithm to use on the strings.";
      stringArray0[5] = "The tokenizing algorithm to use on the strings.";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      double[] doubleArray1 = new double[8];
      Attribute attribute0 = new Attribute("cid", 325);
      sparseInstance0.setValue(attribute0, (double) (-2867));
      doubleArray1[0] = (double) (-2867);
      sparseInstance1.s_numericAfterDecimalPoint = (-2867);
      doubleArray1[2] = (double) (-2867);
      doubleArray1[3] = (-99.0);
      doubleArray1[4] = (double) (-2867);
      doubleArray1[5] = 7.577107088453127;
      doubleArray1[6] = (double) (-2867);
      sparseInstance1.setValue(attribute0, 23.0);
      doubleArray1[7] = (-99.0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance(1.0, doubleArray1);
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance((SparseInstance) binarySparseInstance2);
      naiveBayesMultinomialText0.setPeriodicPruning((-2867));
      FileSystemHandling.setPermissions((EvoSuiteFile) null, false, false, false);
      NullStemmer nullStemmer0 = new NullStemmer();
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals((-2867), naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 5
  /*Coverage entropy=2.209406559005219
  */
  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      double[] doubleArray0 = new double[6];
      SparseInstance sparseInstance0 = new SparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2867), doubleArray0);
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance((Instance) binarySparseInstance1);
      binarySparseInstance3.setValueSparse((-2867), (-2867));
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      BinarySparseInstance binarySparseInstance4 = new BinarySparseInstance((SparseInstance) binarySparseInstance3);
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities0.find();
      Attribute attribute0 = new Attribute("The LNorm to use for document length normalization.", vector0);
      sparseInstance0.setValue(attribute0, 7.577107088453127);
      BinarySparseInstance binarySparseInstance5 = new BinarySparseInstance((-558.70303076), doubleArray0);
      BinarySparseInstance binarySparseInstance6 = new BinarySparseInstance((SparseInstance) binarySparseInstance5);
      naiveBayesMultinomialText0.setPeriodicPruning(3);
      FileSystemHandling.setPermissions((EvoSuiteFile) null, true, true, false);
      NullStemmer nullStemmer0 = new NullStemmer();
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.pruneDictionary();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 6
  /*Coverage entropy=2.650442143368543
  */
  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) snowballStemmer0;
      naiveBayesMultinomialText0.getOptions();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      boolean boolean0 = false;
      double[] doubleArray0 = new double[1];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(3.0, doubleArray0);
      binarySparseInstance0.toStringNoWeight();
      Discretize discretize0 = new Discretize();
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance(0.0, doubleArray0);
      int[] intArray0 = new int[7];
      intArray0[0] = (-2867);
      intArray0[1] = (-2867);
      intArray0[2] = (-2867);
      intArray0[3] = (-2867);
      intArray0[4] = (-2867);
      intArray0[5] = (-2867);
      intArray0[6] = (-2867);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance(0.0, intArray0, (-2867));
      // Undeclared exception!
      try { 
        binarySparseInstance0.hasMissingValue();
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 7
  /*Coverage entropy=2.900015079480596
  */
  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.getOptions();
      NormalEstimator normalEstimator0 = new NormalEstimator(1893.49472292);
      Capabilities capabilities0 = normalEstimator0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      int int0 = 0;
      instances0.meanOrMode(0);
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // weka.classifiers.bayes.NaiveBayesMultinomialText: Class attribute not set!
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 8
  /*Coverage entropy=2.2092080154282336
  */
  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.stemmerTipText();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      SerializedClassifier serializedClassifier1 = new SerializedClassifier();
      serializedClassifier1.getModelFile();
      byte[] byteArray0 = new byte[9];
      byteArray0[0] = (byte)47;
      byteArray0[1] = (byte)112;
      byteArray0[2] = (byte)47;
      byteArray0[3] = (byte)20;
      byteArray0[4] = (byte)47;
      byteArray0[5] = (byte)26;
      byteArray0[6] = (byte)47;
      byteArray0[7] = (byte)47;
      byteArray0[8] = (byte)47;
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "(}y");
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      Kernel.makeCopies(precomputedKernelMatrixKernel0, 409);
      precomputedKernelMatrixKernel0.getKernelMatrixFile();
      naiveBayesMultinomialText0.setStopwords((File) null);
      String string0 = naiveBayesMultinomialText0.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string0);
      
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      String string1 = naiveBayesMultinomialText0.stemmerTipText();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", string1);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 9
  /*Coverage entropy=3.108872986584512
  */
  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      FileSystemHandling.setPermissions(evoSuiteFile0, false, false, false);
      File file0 = MockFile.createTempFile("\tpow often to prune th3 dictionary of ow frequeny words (default = 0, i.e. don't prune)", "\tpow often to prune th3 dictionary of ow frequeny words (default = 0, i.e. don't prune)");
      MockFile mockFile0 = new MockFile(file0, "The norm of the instances after normalization.");
      naiveBayesMultinomialText0.setOptions(stringArray0);
      MockFile.createTempFile("The tokenizing algorithm to use on the strings.", "gqpx");
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      MockFile mockFile1 = new MockFile(file0, "\tpow often to prune th3 dictionary of ow frequeny words (default = 0, i.e. don't prune)");
      file0.createNewFile();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(12, stringArray0.length);
      
      MockFile mockFile2 = new MockFile(mockFile1, "gqpx");
      naiveBayesMultinomialText0.getStopwords();
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 10
  /*Coverage entropy=2.4920216367960215
  */
  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getOtherCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate("If true, ignores all words that are on the stoplist.");
      capabilities0.getClassCapabilities();
      instances0.meanOrMode(0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      testInstances0.setNumRelationalNominalValues(1017);
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      Capabilities.forInstances(instances0, false);
      testInstances0.setNumNominal(213);
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.DATE_CLASS;
      capabilities1.enableDependency(capabilities_Capability0);
      SnowballStemmer.main(testInstances0.DEFAULT_WORDS);
      ArrayList<Locale.LanguageRange> arrayList0 = new ArrayList<Locale.LanguageRange>();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      naiveBayesMultinomialText0.periodicPruningTipText();
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2867), doubleArray0);
      binarySparseInstance0.toStringNoWeight();
      naiveBayesMultinomialText0.stopwordsTipText();
      // Undeclared exception!
      try { 
        binarySparseInstance0.isMissingSparse((-2867));
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // -2867
         //
         verifyException("weka.core.BinarySparseInstance", e);
      }
  }

  /**
  //Test case number: 11
  /*Coverage entropy=2.299429391468317
  */
  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      double[] doubleArray0 = new double[6];
      SparseInstance sparseInstance0 = new SparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-3185.3162203732), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      Stopwords stopwords0 = new Stopwords();
      naiveBayesMultinomialText0.m_stopwords = stopwords0;
      File file0 = MockFile.createTempFile("gqpx", "");
      naiveBayesMultinomialText0.setStopwords(file0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2867), doubleArray0);
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance((Instance) binarySparseInstance0);
      double[] doubleArray1 = new double[2];
      doubleArray1[0] = (-99.0);
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/tmp");
      binarySparseInstance3.setValueSparse(47, (-3185.3162203732));
      FileSystemHandling.appendStringToFile(evoSuiteFile0, "");
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      doubleArray1[1] = (-2689.2212981);
      BinarySparseInstance binarySparseInstance4 = new BinarySparseInstance((-341.81061666), doubleArray1);
      BinarySparseInstance binarySparseInstance5 = new BinarySparseInstance((Instance) sparseInstance0);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance3);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 12
  /*Coverage entropy=2.4762856317335182
  */
  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate("If true, ignores all words that are on the stoplist.");
      capabilities0.getClassCapabilities();
      LinkedList<Integer> linkedList0 = new LinkedList<Integer>();
      Integer integer0 = new Integer(1);
      linkedList0.addLast(integer0);
      instances0.removeAll(linkedList0);
      naiveBayesMultinomialText0.m_normalize = false;
      testInstances0.setNumNominalValues((-2845));
      instances0.meanOrMode(0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      testInstances0.setNumRelationalNominalValues(1017);
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("weka/core/Capabilities.props");
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.DATE_CLASS;
      capabilities1.enableDependency(capabilities_Capability0);
      ArrayList<Locale.LanguageRange> arrayList0 = new ArrayList<Locale.LanguageRange>();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      naiveBayesMultinomialText0.periodicPruningTipText();
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2867), doubleArray0);
      assertArrayEquals(new double[] {0.6818181818181819, 0.3181818181818182}, doubleArray0, 0.01);
      
      binarySparseInstance0.toStringNoWeight();
      naiveBayesMultinomialText0.stopwordsTipText();
      double[] doubleArray1 = naiveBayesMultinomialText0.m_probOfClass;
      String string0 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string0);
      
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      String string1 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string1);
      
      String string2 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string2);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 13
  /*Coverage entropy=2.4222597422021748
  */
  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.EMPTY_NOMINAL_ATTRIBUTES;
      capabilities1.enable(capabilities_Capability0);
      instances0.meanOrMode(0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("weka/core/Capabilities.props");
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      Capabilities.Capability capabilities_Capability1 = Capabilities.Capability.STRING_CLASS;
      capabilities1.enableDependency(capabilities_Capability1);
      SnowballStemmer.main(testInstances0.DEFAULT_WORDS);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      ArrayList<Locale.LanguageRange> arrayList0 = new ArrayList<Locale.LanguageRange>();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertArrayEquals(new double[] {0.6818181818181819, 0.3181818181818182}, doubleArray0, 0.01);
      
      naiveBayesMultinomialText0.periodicPruningTipText();
      LinkedList<Locale.LanguageRange> linkedList0 = new LinkedList<Locale.LanguageRange>();
      naiveBayesMultinomialText0.stemmerTipText();
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t15.0\nclass2\t7.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\t\nover\t7.38905609893065\t7.38905609893065\t\nthe\t7.38905609893065\t7.38905609893065\t\nThe\t20.085536923187668\t2.718281828459045\t\nquick\t20.085536923187668\t7.38905609893065\t\nlazy\t20.085536923187668\t2.718281828459045\t\njumps\t20.085536923187668\t7.38905609893065\t\nbrown\t7.38905609893065\t7.38905609893065\t\ndog\t20.085536923187668\t2.718281828459045\t\nfox\t7.38905609893065\t7.38905609893065\t\n", string0);
      
      TestInstances testInstances1 = new TestInstances();
      naiveBayesMultinomialText0.tokenizerTipText();
      String string1 = naiveBayesMultinomialText0.normTipText();
      assertEquals("The norm of the instances after normalization.", string1);
      
      naiveBayesMultinomialText0.tokenizeInstance(binarySparseInstance1, false);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 14
  /*Coverage entropy=1.830738805564335
  */
  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_tokenizer = null;
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.getTokenizer();
      naiveBayesMultinomialText0.getStemmer();
      int[] intArray0 = new int[1];
      intArray0[0] = 0;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-122.79621), intArray0, 0);
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance0);
      sparseInstance0.toString(617, 10000);
      // Undeclared exception!
      try { 
        sparseInstance0.hasMissingValue();
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 15
  /*Coverage entropy=2.070976373972562
  */
  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props");
      FileSystemHandling.createFolder(evoSuiteFile0);
      capabilities0.getAttributeCapabilities();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("If true, ignores all words that are on the stoplist.", arrayList0, 1636);
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // weka.classifiers.bayes.NaiveBayesMultinomialText: No attributes!
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 16
  /*Coverage entropy=2.5109208888644443
  */
  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      SerializedClassifier serializedClassifier1 = new SerializedClassifier();
      serializedClassifier1.getModelFile();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.NOMINAL_CLASS;
      capabilities1.getOtherCapabilities();
      capabilities1.disableDependency(capabilities_Capability0);
      SGDText sGDText0 = new SGDText();
      sGDText0.setPeriodicPruning((-1));
      sGDText0.getStopwords();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.globalInfo();
      naiveBayesMultinomialText0.setLNorm((-2));
      naiveBayesMultinomialText0.LNormTipText();
      LinkedList<Locale.LanguageRange> linkedList0 = new LinkedList<Locale.LanguageRange>();
      Environment environment0 = Environment.getSystemWide();
      Set<String> set0 = environment0.getVariableNames();
      Locale.FilteringMode locale_FilteringMode0 = Locale.FilteringMode.IGNORE_EXTENDED_RANGES;
      Locale.filterTags((List<Locale.LanguageRange>) linkedList0, (Collection<String>) set0, locale_FilteringMode0);
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.getLNorm();
      naiveBayesMultinomialText0.normTipText();
      assertEquals((-2.0), naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 17
  /*Coverage entropy=3.046132223739781
  */
  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      double[] doubleArray0 = new double[6];
      SparseInstance sparseInstance0 = new SparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      File file0 = MockFile.createTempFile("The tokenizing algorithm to use on the strings.", ",Erf/\"UDVjE,7VF");
      naiveBayesMultinomialText0.setStopwords(file0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2867), doubleArray0);
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance((Instance) binarySparseInstance2);
      binarySparseInstance3.setValueSparse(0, 15.0);
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      BinarySparseInstance binarySparseInstance4 = new BinarySparseInstance(1349.92715, doubleArray0);
      binarySparseInstance3.toStringNoWeight();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.normTipText();
      BinarySparseInstance binarySparseInstance5 = new BinarySparseInstance((Instance) binarySparseInstance3);
      LinkedList<Locale.LanguageRange> linkedList0 = new LinkedList<Locale.LanguageRange>();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.getStopwords();
      MockFile mockFile0 = new MockFile("{}");
      MockFile mockFile1 = new MockFile(mockFile0, "The file containing the stopwords (if this is a directory then the default ones are used).");
      mockFile1.createNewFile();
      naiveBayesMultinomialText0.setOptions((String[]) null);
      MockFile mockFile2 = new MockFile(mockFile0, "");
      assertFalse(mockFile2.equals((Object)file0));
  }

  /**
  //Test case number: 18
  /*Coverage entropy=2.7914234446919735
  */
  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.getRevision();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      naiveBayesMultinomialText0.m_data = null;
      byte[] byteArray0 = new byte[3];
      byteArray0[0] = (byte)1;
      byteArray0[1] = (byte)8;
      byteArray0[2] = (byte)66;
      FileSystemHandling.appendDataToFile(evoSuiteFile0, byteArray0);
      EvoSuiteFile evoSuiteFile1 = new EvoSuiteFile("kernelMatrix.matrix");
      FileSystemHandling.appendStringToFile(evoSuiteFile1, "If true, ignores all words that are on the stoplist.");
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      File file0 = precomputedKernelMatrixKernel0.getKernelMatrixFile();
      naiveBayesMultinomialText0.setStopwords(file0);
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.getMinWordFrequency();
      naiveBayesMultinomialText0.getStopwords();
      double[] doubleArray0 = new double[9];
      doubleArray0[1] = (double) (byte)1;
      doubleArray0[2] = (-1.0);
      doubleArray0[3] = (double) (byte)8;
  }

  /**
  //Test case number: 19
  /*Coverage entropy=1.830738805564335
  */
  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_leplace = 2.0;
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      naiveBayesMultinomialText0.listOptions();
      naiveBayesMultinomialText0.getNorm();
      IBk iBk0 = new IBk((-685));
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", arrayList0, 4);
      double[] doubleArray0 = new double[4];
      doubleArray0[0] = 1.0;
      doubleArray0[1] = (double) 4;
      doubleArray0[2] = (double) (-685);
      doubleArray0[3] = (double) (-685);
      Instances instances1 = iBk0.pruneToK(instances0, doubleArray0, 68);
      assertNull(instances1);
  }

  /**
  //Test case number: 20
  /*Coverage entropy=2.0800261477282183
  */
  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      Capabilities capabilities0 = naiveBayesMultinomialText1.getCapabilities();
      capabilities0.getAttributeCapabilities();
      MultiFilter multiFilter0 = new MultiFilter();
      Capabilities capabilities1 = multiFilter0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      try { 
        naiveBayesMultinomialText2.buildClassifier(instances0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // weka.classifiers.bayes.NaiveBayesMultinomialText: Cannot handle relational attributes!
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 21
  /*Coverage entropy=2.41257681572198
  */
  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_useStopList = false;
      naiveBayesMultinomialText0.setLNorm(0.0);
      naiveBayesMultinomialText0.m_leplace = 0.0;
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, (byte[]) null);
      naiveBayesMultinomialText0.m_norm = 0.0;
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "ZR\bzO");
      naiveBayesMultinomialText0.pruneDictionary();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      naiveBayesMultinomialText0.getMinWordFrequency();
      naiveBayesMultinomialText0.m_normalize = true;
      naiveBayesMultinomialText0.m_normalize = true;
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      naiveBayesMultinomialText0.m_tokenizer = (Tokenizer) wordTokenizer0;
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.listOptions();
      assertEquals(0.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 22
  /*Coverage entropy=3.327867061194284
  */
  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      File file0 = MockFile.createTempFile("\tpow often to prune th3 dictionary of ow frequeny words (default = 0, i.e. don't prune)", "\tpow often to prune th3 dictionary of ow frequeny words (default = 0, i.e. don't prune)");
      MockFile mockFile0 = new MockFile(file0, "The norm of the instances after normalization.");
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      Stemmer stemmer0 = naiveBayesMultinomialText1.getStemmer();
      naiveBayesMultinomialText0.m_stemmer = stemmer0;
      naiveBayesMultinomialText0.setOptions(stringArray0);
      File file1 = MockFile.createTempFile("The tokenizing algorithm to use on the strings.", "gqpx");
      naiveBayesMultinomialText0.setStopwords(file1);
      file0.getAbsolutePath();
      MockFile.createTempFile("BP$B", "\tpow often to prune th3 dictionary of ow frequeny words (default = 0, i.e. don't prune)");
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      double[] doubleArray0 = new double[4];
      doubleArray0[0] = 2.0;
      doubleArray0[1] = 0.0;
      doubleArray0[2] = 2.0;
      doubleArray0[3] = 0.0;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0.0, doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 23
  /*Coverage entropy=3.336702332303382
  */
  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = Locale.getISOLanguages();
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      Stemmer stemmer0 = naiveBayesMultinomialText0.getStemmer();
      naiveBayesMultinomialText0.setStemmer(stemmer0);
      NaiveBayesMultinomialText.main(stringArray0);
      naiveBayesMultinomialText0.normTipText();
      naiveBayesMultinomialText0.setPeriodicPruning((-2042));
      naiveBayesMultinomialText0.setOptions(stringArray0);
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.globalInfo();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      naiveBayesMultinomialText0.getPeriodicPruning();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.toString();
      naiveBayesMultinomialText0.getMinWordFrequency();
      double double0 = naiveBayesMultinomialText0.getMinWordFrequency();
      assertEquals((-2042), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, double0, 0.01);
  }

  /**
  //Test case number: 24
  /*Coverage entropy=2.5355410969957366
  */
  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      double[] doubleArray0 = new double[6];
      SparseInstance sparseInstance0 = new SparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      File file0 = MockFile.createTempFile("The tokenizing algorithm to use on the strings.", "If true, ignores all words that are on the stoplist.");
      naiveBayesMultinomialText0.setStopwords(file0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2867), doubleArray0);
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance((Instance) binarySparseInstance1);
      binarySparseInstance3.setValueSparse((-2867), (-2867));
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      BinarySparseInstance binarySparseInstance4 = new BinarySparseInstance(1117.9968, doubleArray0);
      FileSystemHandling.shouldAllThrowIOExceptions();
      binarySparseInstance3.toStringNoWeight();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.normTipText();
      SparseInstance sparseInstance1 = new SparseInstance(0);
      BinarySparseInstance binarySparseInstance5 = new BinarySparseInstance((Instance) sparseInstance1);
      Properties properties0 = new Properties();
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties1 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties2 = new ProtectedProperties(protectedProperties0);
      Attribute attribute0 = new Attribute("", protectedProperties2);
      attribute0.enumerateValues();
      binarySparseInstance5.value(attribute0);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance1);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 25
  /*Coverage entropy=2.4305268740410804
  */
  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      double[] doubleArray0 = new double[6];
      SparseInstance sparseInstance0 = new SparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      File file0 = MockFile.createTempFile("The tokenizing algorithm to use on the strings.", "If true, ignores all words that are on the stoplist.");
      file0.toURI();
      naiveBayesMultinomialText0.setStopwords(file0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2867), doubleArray0);
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance((Instance) binarySparseInstance1);
      binarySparseInstance3.setValueSparse((-2867), (-2867));
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      BinarySparseInstance binarySparseInstance4 = new BinarySparseInstance(1117.9968, doubleArray0);
      binarySparseInstance3.toStringNoWeight();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.normTipText();
      SparseInstance sparseInstance1 = new SparseInstance(0);
      BinarySparseInstance binarySparseInstance5 = new BinarySparseInstance((Instance) sparseInstance1);
      Properties properties0 = new Properties();
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties1 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties2 = new ProtectedProperties(protectedProperties0);
      Attribute attribute0 = new Attribute("", protectedProperties2);
      attribute0.enumerateValues();
      binarySparseInstance5.value(attribute0);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance(binarySparseInstance0, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 26
  /*Coverage entropy=3.2842313242432812
  */
  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.stemmerTipText();
      Capabilities capabilities0 = new Capabilities(naiveBayesMultinomialText0);
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.setUseWordFrequencies(false);
      SGDText sGDText0 = new SGDText();
      Stemmer stemmer0 = sGDText0.getStemmer();
      naiveBayesMultinomialText0.setStemmer(stemmer0);
      NaiveBayesMultinomialText.main(testInstances0.DEFAULT_WORDS);
      naiveBayesMultinomialText0.normTipText();
      naiveBayesMultinomialText0.setPeriodicPruning(392);
      naiveBayesMultinomialText0.setOptions(testInstances0.DEFAULT_WORDS);
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      int int0 = naiveBayesMultinomialText0.m_periodicP;
      naiveBayesMultinomialText0.setOptions(testInstances0.DEFAULT_WORDS);
      naiveBayesMultinomialText0.getPeriodicPruning();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.getMinWordFrequency();
      assertEquals(392, naiveBayesMultinomialText0.getPeriodicPruning());
      
      double double0 = naiveBayesMultinomialText1.getMinWordFrequency();
      assertEquals(3.0, double0, 0.01);
  }

  /**
  //Test case number: 27
  /*Coverage entropy=2.846848237035269
  */
  @Test(timeout = 4000)
  public void test27()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.getOptions();
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      
      naiveBayesMultinomialText0.reset();
      naiveBayesMultinomialText0.m_stemmer = null;
      double[] doubleArray0 = new double[2];
      doubleArray0[0] = 0.0;
      doubleArray0[1] = 2637.566;
      String string1 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string1);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(10, stringArray0.length);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 28
  /*Coverage entropy=2.287932242653669
  */
  @Test(timeout = 4000)
  public void test28()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      TestInstances testInstances1 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances1.generate();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.RELATIONAL_ATTRIBUTES;
      capabilities1.enableDependency(capabilities_Capability0);
      SnowballStemmer.main(testInstances1.DEFAULT_WORDS);
      ArrayList<Locale.LanguageRange> arrayList0 = new ArrayList<Locale.LanguageRange>();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities2 = new FindWithCapabilities();
      findWithCapabilities0.setNotCapabilities(capabilities1);
      FindWithCapabilities findWithCapabilities3 = new FindWithCapabilities();
      findWithCapabilities2.getMatches();
      SnowballStemmer.main(testInstances0.DEFAULT_WORDS);
      ArrayList<Locale.LanguageRange> arrayList1 = new ArrayList<Locale.LanguageRange>();
      FindWithCapabilities findWithCapabilities4 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities5 = new FindWithCapabilities();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.toString();
      TestInstances testInstances2 = new TestInstances();
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText0.normTipText();
      System.setCurrentTimeMillis(0L);
      testInstances2.setNumRelationalNominal((-2));
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      Instances instances1 = null;
      try {
        instances1 = new Instances(instances0, (-1), (-2));
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // Parameters first and/or toCopy out of range
         //
         verifyException("weka.core.Instances", e);
      }
  }

  /**
  //Test case number: 29
  /*Coverage entropy=3.515133849178615
  */
  @Test(timeout = 4000)
  public void test29()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[1];
      stringArray0[0] = "s5%r?*W}U8dfIG";
      naiveBayesMultinomialText0.setUseStopList(true);
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/tmp/s5%r?*W}U8dfIG0\tSpecifies list of columns to act on. First and last are \n\tvalid indexes.\n\t(default: first-last)");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.setMinWordFrequency(362.8033184);
      naiveBayesMultinomialText0.getUseWordFrequencies();
      NaiveBayesMultinomialText.main(stringArray0);
      naiveBayesMultinomialText0.normTipText();
      naiveBayesMultinomialText0.setPeriodicPruning(701);
      naiveBayesMultinomialText0.setOptions(stringArray0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.m_lowercaseTokens = true;
      naiveBayesMultinomialText1.useWordFrequenciesTipText();
      naiveBayesMultinomialText1.getOptions();
      double double0 = naiveBayesMultinomialText0.getMinWordFrequency();
      assertEquals(362.8033184, double0, 0.01);
  }

  /**
  //Test case number: 30
  /*Coverage entropy=2.042632211710285
  */
  @Test(timeout = 4000)
  public void test30()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      double[] doubleArray0 = new double[6];
      SparseInstance sparseInstance0 = new SparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      SparseInstance sparseInstance1 = new SparseInstance((-2926.0), doubleArray0);
      double[] doubleArray1 = new double[8];
      Attribute attribute0 = new Attribute("cid", 325);
      sparseInstance0.setValue(attribute0, (double) (-2867));
      doubleArray1[0] = (double) (-2867);
      sparseInstance1.s_numericAfterDecimalPoint = (-2867);
      doubleArray1[2] = (double) (-2867);
      doubleArray1[3] = (-99.0);
      doubleArray1[4] = (double) (-2867);
      doubleArray1[5] = 7.577107088453127;
      doubleArray1[6] = (double) (-2867);
      sparseInstance1.setValue(attribute0, 23.0);
      doubleArray1[7] = (-99.0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance(1.0, doubleArray1);
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance((SparseInstance) binarySparseInstance2);
      naiveBayesMultinomialText0.setPeriodicPruning(640);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance2);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 31
  /*Coverage entropy=3.4305796491377345
  */
  @Test(timeout = 4000)
  public void test31()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      int[] intArray0 = new int[7];
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      naiveBayesMultinomialText0.getUseWordFrequencies();
      NullStemmer nullStemmer0 = new NullStemmer();
      String[] stringArray0 = new String[8];
      stringArray0[0] = "{.~rB[nGLi@'`Xhh6";
      stringArray0[1] = "The tokenizing algorithm to use on the strings.";
      stringArray0[2] = "The tokenizing algorithm to use on the strings.";
      stringArray0[3] = "The tokenizing algorithm to use on the strings.";
      stringArray0[4] = "{.~rB[nGLi@'`Xhh6";
      stringArray0[5] = "j4)H@<FgZ*\"";
      stringArray0[6] = "{.~rB[nGLi@'`Xhh6";
      stringArray0[7] = "The tokenizing algorithm to use on the strings.";
      NaiveBayesMultinomialText.main(stringArray0);
      naiveBayesMultinomialText0.normTipText();
      naiveBayesMultinomialText0.setPeriodicPruning(1512);
      naiveBayesMultinomialText0.setOptions(stringArray0);
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.globalInfo();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.getPeriodicPruning();
      naiveBayesMultinomialText1.setOptions(stringArray0);
      naiveBayesMultinomialText1.getPeriodicPruning();
      naiveBayesMultinomialText0.toString();
      assertEquals(1512, naiveBayesMultinomialText0.getPeriodicPruning());
      
      String string0 = naiveBayesMultinomialText1.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string0);
  }

  /**
  //Test case number: 32
  /*Coverage entropy=2.738840717855686
  */
  @Test(timeout = 4000)
  public void test32()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_normalize = true;
      naiveBayesMultinomialText0.getOptions();
      EvoSuiteFile evoSuiteFile0 = null;
      boolean boolean0 = false;
      naiveBayesMultinomialText0.m_probOfWordGivenClass = null;
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      EvoSuiteFile evoSuiteFile1 = new EvoSuiteFile("/tmp/The tokenizing algorithm to use on the strings.0,Erf");
      FileSystemHandling.appendLineToFile(evoSuiteFile1, "truly");
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = 1349.92715;
      doubleArray0[1] = 1349.92715;
      doubleArray0[2] = 1349.92715;
      doubleArray0[3] = 1349.92715;
      doubleArray0[4] = 1349.92715;
      doubleArray0[5] = 1349.92715;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1349.92715, doubleArray0);
      // Undeclared exception!
      try { 
        binarySparseInstance0.toStringNoWeight();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.SparseInstance", e);
      }
  }

  /**
  //Test case number: 33
  /*Coverage entropy=1.8408398156653452
  */
  @Test(timeout = 4000)
  public void test33()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      Instance instance0 = null;
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) null, true);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 34
  /*Coverage entropy=1.543056733112554
  */
  @Test(timeout = 4000)
  public void test34()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[1];
      stringArray0[0] = null;
      try { 
        naiveBayesMultinomialText1.setOptions(stringArray0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 35
  /*Coverage entropy=3.108262855629014
  */
  @Test(timeout = 4000)
  public void test35()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      File file0 = MockFile.createTempFile("\tpow often to prune th3 dictionary of ow frequeny words (default = 0, i.e. don't prune)", "\tpow often to prune th3 dictionary of ow frequeny words (default = 0, i.e. don't prune)");
      MockFile mockFile0 = new MockFile(file0, "The norm of the instances after normalization.");
      naiveBayesMultinomialText0.setOptions(stringArray0);
      File file1 = MockFile.createTempFile("The tokenizing algorithm to use on the strings.", "gqpx");
      naiveBayesMultinomialText0.setStopwords(file1);
      file1.toPath();
      mockFile0.deleteOnExit();
      MockFile mockFile1 = new MockFile(file0, "-------------\t");
      MockFile mockFile2 = new MockFile(mockFile0, "");
      file0.mkdirs();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      MockFile mockFile3 = new MockFile(mockFile1, "The tokenizing algorithm to use on the strings.");
      file0.getCanonicalFile();
      file0.mkdirs();
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      precomputedKernelMatrixKernel0.getKernelMatrixFile();
      mockFile0.renameTo(mockFile1);
      MockFile mockFile4 = new MockFile((File) null, "gqpx");
      mockFile2.createNewFile();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      file0.toURL();
      MockFile mockFile5 = new MockFile((File) null, "The norm of the instances after normalization.");
      assertFalse(mockFile5.equals((Object)file1));
  }

  /**
  //Test case number: 36
  /*Coverage entropy=2.557702411245053
  */
  @Test(timeout = 4000)
  public void test36()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.LNormTipText();
      naiveBayesMultinomialText1.setNormalizeDocLength(true);
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      naiveBayesMultinomialText3.setStemmer(iteratedLovinsStemmer0);
      naiveBayesMultinomialText3.tokenizerTipText();
      naiveBayesMultinomialText3.stopwordsTipText();
      System.setCurrentTimeMillis(0L);
  }

  /**
  //Test case number: 37
  /*Coverage entropy=2.4607350178931684
  */
  @Test(timeout = 4000)
  public void test37()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate("If true, ignores all words that are on the stoplist.");
      capabilities1.getOtherCapabilities();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      naiveBayesMultinomialText0.m_normalize = true;
      instances0.meanOrMode(0);
      naiveBayesMultinomialText0.setLowercaseTokens(false);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      testInstances0.setNumRelationalNominalValues(1017);
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("weka/core/Capabilities.props");
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.STRING_CLASS;
      capabilities1.enableDependency(capabilities_Capability0);
      naiveBayesMultinomialText0.m_useStopList = false;
      SnowballStemmer.main(testInstances0.DEFAULT_WORDS);
      ArrayList<Locale.LanguageRange> arrayList0 = new ArrayList<Locale.LanguageRange>();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.normTipText();
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.getStemmer();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
  }

  /**
  //Test case number: 38
  /*Coverage entropy=1.2961346570528898
  */
  @Test(timeout = 4000)
  public void test38()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = null;
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, (String) null);
      double[] doubleArray0 = new double[4];
      doubleArray0[1] = 0.0;
      int[] intArray0 = new int[4];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0.0, intArray0, 0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance1);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 39
  /*Coverage entropy=0.9623351446188083
  */
  @Test(timeout = 4000)
  public void test39()  throws Throwable  {
      FileSystemHandling.setPermissions((EvoSuiteFile) null, true, false, true);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_periodicP = 1332;
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance((Instance) null, false);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 40
  /*Coverage entropy=2.982659286444667
  */
  @Test(timeout = 4000)
  public void test40()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[1];
      stringArray0[0] = "s5%r?*W}U8dfIG";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText0.stemmerTipText();
      MockFile mockFile0 = new MockFile("The stemming algorithm to use on the words.", "s5%r?*W}U8dfIG");
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      naiveBayesMultinomialText0.getUseWordFrequencies();
      naiveBayesMultinomialText0.getUseStopList();
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      File file0 = precomputedKernelMatrixKernel0.getKernelMatrixFile();
      naiveBayesMultinomialText0.setStopwords(file0);
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      naiveBayesMultinomialText0.stopwordsTipText();
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
  }

  /**
  //Test case number: 41
  /*Coverage entropy=3.246682803294346
  */
  @Test(timeout = 4000)
  public void test41()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      double[] doubleArray0 = new double[6];
      SparseInstance sparseInstance0 = new SparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      File file0 = MockFile.createTempFile("The tokenizing algorithm to use on the strings.", ",Erf/\"UDVjE,7VF");
      naiveBayesMultinomialText0.setStopwords(file0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2867), doubleArray0);
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance((Instance) binarySparseInstance2);
      binarySparseInstance3.setValueSparse(0, 15.0);
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      BinarySparseInstance binarySparseInstance4 = new BinarySparseInstance(1349.92715, doubleArray0);
      binarySparseInstance3.toStringNoWeight();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.normTipText();
      BinarySparseInstance binarySparseInstance5 = new BinarySparseInstance((Instance) binarySparseInstance3);
      Properties properties0 = new Properties();
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties1 = new ProtectedProperties(protectedProperties0);
      ProtectedProperties protectedProperties2 = new ProtectedProperties(protectedProperties1);
      naiveBayesMultinomialText0.setLNorm((-2867));
      Attribute attribute0 = new Attribute("nWQU];I", protectedProperties2);
      attribute0.enumerateValues();
      binarySparseInstance5.value(attribute0);
      naiveBayesMultinomialText0.getOptions();
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance5);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 42
  /*Coverage entropy=1.5981863871455346
  */
  @Test(timeout = 4000)
  public void test42()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = null;
      naiveBayesMultinomialText0.getUseWordFrequencies();
      DenseInstance denseInstance0 = new DenseInstance(7);
      double[] doubleArray0 = new double[4];
      doubleArray0[0] = (double) (-2867);
      doubleArray0[1] = 0.0;
      doubleArray0[2] = (double) 7;
      doubleArray0[3] = (double) (-2867);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(7, doubleArray0);
      int[] intArray0 = new int[4];
      intArray0[0] = (-2867);
      intArray0[1] = (-2867);
      intArray0[2] = (-2867);
      intArray0[3] = 7;
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance(0.0, intArray0, (-2867));
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((Instance) binarySparseInstance1);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance2);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 43
  /*Coverage entropy=2.505443429095587
  */
  @Test(timeout = 4000)
  public void test43()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      double[] doubleArray0 = new double[6];
      SparseInstance sparseInstance0 = new SparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      MockFile mockFile0 = new MockFile("The tokenizing algorithm to use on the strings.", ",Erf/\"UDVjE,7VF");
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2867), doubleArray0);
      binarySparseInstance0.setMissing((-2867));
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance((Instance) binarySparseInstance2);
      binarySparseInstance3.setValueSparse(0, 15.0);
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      BinarySparseInstance binarySparseInstance4 = new BinarySparseInstance(1349.92715, doubleArray0);
      binarySparseInstance3.toStringNoWeight();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.normTipText();
      BinarySparseInstance binarySparseInstance5 = new BinarySparseInstance((Instance) binarySparseInstance3);
      Properties properties0 = new Properties();
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties1 = new ProtectedProperties(protectedProperties0);
      ProtectedProperties protectedProperties2 = new ProtectedProperties(protectedProperties1);
      Attribute attribute0 = new Attribute("nWQU];I", protectedProperties2);
      binarySparseInstance5.value(attribute0);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance5);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 44
  /*Coverage entropy=2.423039622091642
  */
  @Test(timeout = 4000)
  public void test44()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string0);
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances.forCapabilities(capabilities1);
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.RELATIONAL_ATTRIBUTES;
      capabilities1.enableDependency(capabilities_Capability0);
      SnowballStemmer snowballStemmer0 = new SnowballStemmer(" ");
      SerializedClassifier serializedClassifier1 = new SerializedClassifier();
      capabilities0.dependencies();
      capabilities0.capabilities();
      SerializedClassifier serializedClassifier2 = new SerializedClassifier();
      FilteredClassifier filteredClassifier0 = new FilteredClassifier();
      Capabilities capabilities2 = filteredClassifier0.getCapabilities();
      capabilities2.capabilities();
      LinearRegression linearRegression0 = new LinearRegression();
      linearRegression0.getCapabilities();
      Capabilities.Capability capabilities_Capability1 = Capabilities.Capability.EMPTY_NOMINAL_CLASS;
      capabilities0.enableDependency(capabilities_Capability1);
      LinkedList<Locale.LanguageRange> linkedList0 = new LinkedList<Locale.LanguageRange>();
      Environment.getSystemWide();
      LinkedList<String> linkedList1 = new LinkedList<String>();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      String string1 = naiveBayesMultinomialText1.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string1);
      
      naiveBayesMultinomialText1.setUseStopList(false);
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      String string2 = naiveBayesMultinomialText1.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string2);
      
      String string3 = naiveBayesMultinomialText2.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string3);
      
      naiveBayesMultinomialText1.normTipText();
      naiveBayesMultinomialText1.stopwordsTipText();
      String string4 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string4);
      
      naiveBayesMultinomialText1.normalizeDocLengthTipText();
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
  }

  /**
  //Test case number: 45
  /*Coverage entropy=2.8836404106149494
  */
  @Test(timeout = 4000)
  public void test45()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      double[] doubleArray0 = new double[6];
      SparseInstance sparseInstance0 = new SparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-99.0), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      File file0 = MockFile.createTempFile("The tokenizing algorithm to use on the strings.", ",Erf/\"UDVjE,7VF");
      naiveBayesMultinomialText0.setStopwords(file0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2867), doubleArray0);
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance((Instance) binarySparseInstance2);
      binarySparseInstance3.setValueSparse(0, 15.0);
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      binarySparseInstance3.toStringNoWeight();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.normTipText();
      BinarySparseInstance binarySparseInstance4 = new BinarySparseInstance((Instance) binarySparseInstance3);
      Properties properties0 = new Properties();
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties1 = new ProtectedProperties(protectedProperties0);
      ProtectedProperties protectedProperties2 = new ProtectedProperties(protectedProperties1);
      Attribute attribute0 = new Attribute("nWQU];I", protectedProperties2);
      attribute0.enumerateValues();
      binarySparseInstance4.value(attribute0);
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.periodicPruningTipText();
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
  }

  /**
  //Test case number: 46
  /*Coverage entropy=3.2696544884954752
  */
  @Test(timeout = 4000)
  public void test46()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      File file0 = costSensitiveClassifier0.getOnDemandDirectory();
      File file1 = MockFile.createTempFile(":y!{W)yqN<Enz", "The year of publication or, for an unpublished work, the year it was written. Generally it should consist of four numerals, such as 1984, although the standard styles can handle any year whose last four nonpunctuation characters are numerals, such as `hbox{(about 1984)}'.", file0);
      file1.mkdir();
      costSensitiveClassifier0.setOnDemandDirectory(file1);
      MockFile mockFile0 = new MockFile(file0, "");
      MockFile mockFile1 = new MockFile(":y!{W)yqN<Enz", "");
      mockFile0.deleteOnExit();
      MockFile mockFile2 = new MockFile("The year of publication or, for an unpublished work, the year it was written. Generally it should consist of four numerals, such as 1984, although the standard styles can handle any year whose last four nonpunctuation characters are numerals, such as `hbox{(about 1984)}'.");
      mockFile0.toPath();
      mockFile2.deleteOnExit();
      mockFile0.delete();
      MockFile mockFile3 = new MockFile("", "The year of publication or, for an unpublished work, the year it was written. Generally it should consist of four numerals, such as 1984, although the standard styles can handle any year whose last four nonpunctuation characters are numerals, such as `hbox{(about 1984)}'.");
      MockFile mockFile4 = new MockFile(mockFile0, ":y!{W)yqN<Enz");
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      MockFile mockFile5 = new MockFile(mockFile2, "The year of publication or, for an unpublished work, the year it was written. Generally it should consist of four numerals, such as 1984, although the standard styles can handle any year whose last four nonpunctuation characters are numerals, such as `hbox{(about 1984)}'.");
      File file2 = file0.getCanonicalFile();
      mockFile3.mkdirs();
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      File file3 = precomputedKernelMatrixKernel0.getKernelMatrixFile();
      file3.setReadable(false, false);
      mockFile2.getCanonicalPath();
      mockFile2.setReadable(true, true);
      naiveBayesMultinomialText0.getUseStopList();
      MockFile mockFile6 = new MockFile(file3, ":y!{W)yqN<Enz");
      mockFile5.createNewFile();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      file2.toURL();
      MockFile mockFile7 = new MockFile(file0, "PT");
      System.setCurrentTimeMillis(2080L);
  }

  /**
  //Test case number: 47
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test47()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.getLowercaseTokens();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 48
  /*Coverage entropy=2.68644405381145
  */
  @Test(timeout = 4000)
  public void test48()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      naiveBayesMultinomialText0.m_data = null;
      byte[] byteArray0 = new byte[3];
      byteArray0[0] = (byte)1;
      byteArray0[1] = (byte)8;
      byteArray0[2] = (byte)66;
      FileSystemHandling.appendDataToFile(evoSuiteFile0, byteArray0);
      EvoSuiteFile evoSuiteFile1 = new EvoSuiteFile("kernelMatrix.matrix");
      FileSystemHandling.appendStringToFile(evoSuiteFile1, "If true, ignores all words that are on the stoplist.");
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      File file0 = precomputedKernelMatrixKernel0.getKernelMatrixFile();
      naiveBayesMultinomialText0.setStopwords(file0);
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      naiveBayesMultinomialText0.periodicPruningTipText();
      double double0 = naiveBayesMultinomialText0.getNorm();
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, double0, 0.01);
  }

  /**
  //Test case number: 49
  /*Coverage entropy=1.5271837172395382
  */
  @Test(timeout = 4000)
  public void test49()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      AbstractClassifier.makeCopies(naiveBayesMultinomialText0, 6201);
      FileSystemHandling.setPermissions(evoSuiteFile0, false, false, false);
      File file0 = MockFile.createTempFile("\tpow often to prune th3 dictionary of ow frequeny words (default = 0, i.e. don't prune)", "\tpow often to prune th3 dictionary of ow frequeny words (default = 0, i.e. don't prune)");
      file0.deleteOnExit();
      MockFile mockFile0 = new MockFile(file0, "The norm of the instances after normalization.");
      file0.toURL();
      file0.renameTo(mockFile0);
      naiveBayesMultinomialText0.setLNorm(6201);
      file0.setExecutable(true);
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      MockFile mockFile1 = new MockFile("The norm of the instances after normalization.", "The tokenizing algorithm to use on the strings.");
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      MockFile mockFile2 = new MockFile(file0, "\tpow often to prune th3 dictionary of ow frequeny words (default = 0, i.e. don't prune)");
      mockFile2.toURI();
      MockFile mockFile3 = new MockFile(mockFile2, "gqpx");
      assertFalse(mockFile3.equals((Object)file0));
  }

  /**
  //Test case number: 50
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test50()  throws Throwable  {
      FileSystemHandling.setPermissions((EvoSuiteFile) null, false, false, true);
      FileSystemHandling.setPermissions((EvoSuiteFile) null, true, false, true);
      byte[] byteArray0 = new byte[1];
      byteArray0[0] = (byte)73;
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      boolean boolean0 = naiveBayesMultinomialText0.getNormalizeDocLength();
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(boolean0);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 51
  /*Coverage entropy=2.534059615514255
  */
  @Test(timeout = 4000)
  public void test51()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setOptions((String[]) null);
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      naiveBayesMultinomialText0.setTokenizer(alphabeticTokenizer0);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText0.setMinWordFrequency((-1.0));
      assertEquals((-1.0), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 52
  /*Coverage entropy=2.2654436797543704
  */
  @Test(timeout = 4000)
  public void test52()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      naiveBayesMultinomialText2.setStemmer(iteratedLovinsStemmer0);
      String string1 = naiveBayesMultinomialText2.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string1);
      
      naiveBayesMultinomialText2.stopwordsTipText();
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      System.setCurrentTimeMillis(0L);
      String string2 = naiveBayesMultinomialText2.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string2);
      
      String string3 = naiveBayesMultinomialText0.globalInfo();
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", string3);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 53
  /*Coverage entropy=2.68644405381145
  */
  @Test(timeout = 4000)
  public void test53()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      Locale.getISOLanguages();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/tmp/:y!{W)yqN<Enz1q0[dg");
      FileSystemHandling.appendStringToFile(evoSuiteFile0, "\"%JBW<2 (+#$)|4");
      File file0 = naiveBayesMultinomialText0.getStopwords();
      naiveBayesMultinomialText0.setStopwords(file0);
      double[] doubleArray0 = new double[2];
      CoverTree coverTree0 = new CoverTree();
      String string0 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string0);
      
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-1899.2823), doubleArray0);
      binarySparseInstance0.toStringNoWeight();
      String string1 = naiveBayesMultinomialText0.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string1);
      
      String string2 = naiveBayesMultinomialText0.normTipText();
      assertEquals("The norm of the instances after normalization.", string2);
      
      String string3 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string3);
      
      String string4 = naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string4);
      
      String string5 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string5);
      
      String string6 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string6);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 54
  /*Coverage entropy=3.2764811465437393
  */
  @Test(timeout = 4000)
  public void test54()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      byte[] byteArray0 = new byte[9];
      byteArray0[0] = (byte)1;
      byteArray0[1] = (byte) (-12);
      byteArray0[2] = (byte)44;
      byteArray0[3] = (byte)30;
      String string0 = naiveBayesMultinomialText0.normTipText();
      assertEquals("The norm of the instances after normalization.", string0);
      
      String[] stringArray0 = new String[6];
      stringArray0[0] = "If true, ignores all words that are on the stoplist.";
      stringArray0[1] = "If true, ignores all words that are on the stoplist.";
      stringArray0[2] = "The norm of the instances after normalization.";
      stringArray0[3] = "If true, ignores all words that are on the stoplist.";
      stringArray0[4] = "If true, ignores all words that are on the stoplist.";
      stringArray0[5] = "The norm of the instances after normalization.";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      String string1 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string1);
      
      naiveBayesMultinomialText0.getNorm();
      String string2 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string2);
      
      String string3 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string3);
      
      int int0 = naiveBayesMultinomialText0.m_periodicP;
      naiveBayesMultinomialText0.setOptions(stringArray0);
      int int1 = naiveBayesMultinomialText0.getPeriodicPruning();
      assertEquals(0, int1);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      String string4 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string4);
      
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText1.getMinWordFrequency();
      double double0 = naiveBayesMultinomialText0.getMinWordFrequency();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, double0, 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      String string5 = naiveBayesMultinomialText1.stopwordsTipText();
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string5);
  }

  /**
  //Test case number: 55
  /*Coverage entropy=1.830738805564335
  */
  @Test(timeout = 4000)
  public void test55()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      Stemmer stemmer0 = naiveBayesMultinomialText0.getStemmer();
      naiveBayesMultinomialText0.m_stemmer = stemmer0;
      naiveBayesMultinomialText0.getUseStopList();
      naiveBayesMultinomialText0.getTokenizer();
      BallNode ballNode0 = new BallNode((-4554));
      naiveBayesMultinomialText0.m_leplace = (double) 0;
      BallNode ballNode1 = new BallNode(54, (-1), 0, (Instance) null, 1332.967381);
      BallNode ballNode2 = new BallNode((-1), 0, 54);
      ballNode2.m_End = (-1);
  }

  /**
  //Test case number: 56
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test56()  throws Throwable  {
      Random.setNextRandom(15);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (double) 15;
      doubleArray0[1] = (double) 15;
      doubleArray0[2] = (double) 15;
      doubleArray0[3] = 23.0;
      doubleArray0[4] = (double) 15;
      doubleArray0[5] = (double) 15;
      DenseInstance denseInstance0 = new DenseInstance(15, doubleArray0);
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) denseInstance0, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 57
  /*Coverage entropy=1.5271837172395382
  */
  @Test(timeout = 4000)
  public void test57()  throws Throwable  {
      FileSystemHandling.setPermissions((EvoSuiteFile) null, true, false, true);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double[] doubleArray0 = new double[2];
      doubleArray0[0] = 0.0;
      doubleArray0[1] = 0.0;
      int[] intArray0 = new int[8];
      intArray0[0] = 68;
      intArray0[1] = (-3178);
      intArray0[2] = 9;
      intArray0[3] = (-2085);
      intArray0[4] = (-1695);
      intArray0[5] = 30;
      intArray0[6] = 13;
      intArray0[7] = 20;
      SparseInstance sparseInstance0 = new SparseInstance(0.0, doubleArray0, intArray0, (-806));
      sparseInstance0.isMissing(68);
      try { 
        naiveBayesMultinomialText0.updateClassifier(sparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 58
  /*Coverage entropy=3.44442088876986
  */
  @Test(timeout = 4000)
  public void test58()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.m_stemmer = null;
      naiveBayesMultinomialText0.tokenizerTipText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      FileSystemHandling.appendLineToFile(evoSuiteFile0, "The tokenizing algorithm to use on the strings.");
      naiveBayesMultinomialText0.setUseWordFrequencies(false);
      naiveBayesMultinomialText0.setStemmer((Stemmer) null);
      NaiveBayesMultinomialText.main(stringArray0);
      naiveBayesMultinomialText0.normTipText();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      int int0 = naiveBayesMultinomialText0.m_periodicP;
      naiveBayesMultinomialText0.setOptions(stringArray0);
      int int1 = naiveBayesMultinomialText0.m_periodicP;
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      double[] doubleArray0 = new double[8];
      doubleArray0[1] = (double) 0;
      doubleArray0[2] = (-2715.98);
      doubleArray0[3] = 1.0;
      doubleArray0[4] = (double) 0;
      doubleArray0[6] = (double) 0;
      doubleArray0[7] = (double) 0;
      naiveBayesMultinomialText1.m_probOfClass = doubleArray0;
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText1.toString();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 59
  /*Coverage entropy=1.8408398156653452
  */
  @Test(timeout = 4000)
  public void test59()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      IBk iBk0 = new IBk(0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      Instances instances0 = naiveBayesMultinomialText1.m_data;
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (double) 1;
      doubleArray0[1] = (double) 4;
      doubleArray0[2] = (double) 2;
      doubleArray0[3] = (double) 1;
      doubleArray0[4] = 0.0;
      doubleArray0[5] = 1132.97257868;
      iBk0.pruneToK((Instances) null, doubleArray0, 0);
      try { 
        naiveBayesMultinomialText0.buildClassifier((Instances) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 60
  /*Coverage entropy=3.2696544884954752
  */
  @Test(timeout = 4000)
  public void test60()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/tmp/:y!{W)yqN<Enz1q0[dg");
      FileSystemHandling.appendStringToFile(evoSuiteFile0, "\"%JBW<2 (+#$)|4");
      naiveBayesMultinomialText0.setNorm((-1.0));
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      MockFile mockFile0 = new MockFile(":y!{W)yqN<Enz", "");
      MockFile mockFile1 = new MockFile("The year of publication or, for an unpublished work, the year it was written. Generally it should consist of four numerals, such as 1984, although the standard styles can handle any year whose last four nonpunctuation characters are numerals, such as `hbox{(about 1984)}'.");
      mockFile1.deleteOnExit();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      MockFile mockFile2 = new MockFile(mockFile1, "The year of publication or, for an unpublished work, the year it was written. Generally it should consist of four numerals, such as 1984, although the standard styles can handle any year whose last four nonpunctuation characters are numerals, such as `hbox{(about 1984)}'.");
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 61
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test61()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double double0 = naiveBayesMultinomialText0.getLNorm();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(2.0, double0, 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }
}
