/*
 * This file was automatically generated by EvoSuite
 * Mon Nov 18 10:02:06 GMT 2019
 */

package weka.classifiers.bayes;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.File;
import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.Locale;
import java.util.Vector;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.Random;
import org.evosuite.runtime.System;
import org.evosuite.runtime.mock.java.io.MockFile;
import org.evosuite.runtime.mock.java.util.MockRandom;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.evosuite.runtime.util.SystemInUtil;
import org.junit.runner.RunWith;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.bayes.NaiveBayesMultinomialText;
import weka.classifiers.functions.SMOreg;
import weka.classifiers.functions.VotedPerceptron;
import weka.classifiers.functions.supportVector.PrecomputedKernelMatrixKernel;
import weka.classifiers.lazy.IBk;
import weka.classifiers.meta.CostSensitiveClassifier;
import weka.classifiers.meta.RegressionByDiscretization;
import weka.classifiers.misc.SerializedClassifier;
import weka.core.Attribute;
import weka.core.BinarySparseInstance;
import weka.core.Capabilities;
import weka.core.DenseInstance;
import weka.core.FindWithCapabilities;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.SparseInstance;
import weka.core.Stopwords;
import weka.core.TestInstances;
import weka.core.neighboursearch.LinearNNSearch;
import weka.core.stemmers.IteratedLovinsStemmer;
import weka.core.stemmers.LovinsStemmer;
import weka.core.stemmers.NullStemmer;
import weka.core.stemmers.SnowballStemmer;
import weka.core.stemmers.Stemmer;
import weka.core.tokenizers.AlphabeticTokenizer;
import weka.core.tokenizers.NGramTokenizer;
import weka.core.tokenizers.Tokenizer;
import weka.core.tokenizers.WordTokenizer;
import weka.estimators.NormalEstimator;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class NaiveBayesMultinomialText_ESTest extends NaiveBayesMultinomialText_ESTest_scaffolding {

  /**
  //Test case number: 0
  /*Coverage entropy=2.8493821196946767
  */
  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      MockFile mockFile0 = new MockFile("-E <attribute evaluator specification>", "qS3/(v");
      mockFile0.getAbsolutePath();
      costSensitiveClassifier0.setOnDemandDirectory(mockFile0);
      File file0 = costSensitiveClassifier0.getOnDemandDirectory();
      naiveBayesMultinomialText0.setStopwords(file0);
      LovinsStemmer lovinsStemmer0 = new LovinsStemmer();
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) lovinsStemmer0;
      double double0 = naiveBayesMultinomialText0.getNorm();
      assertEquals(1.0, double0, 0.01);
      
      boolean boolean0 = naiveBayesMultinomialText0.getNormalizeDocLength();
      FileSystemHandling.setPermissions((EvoSuiteFile) null, true, true, false);
      File file1 = naiveBayesMultinomialText0.getStopwords();
      String string0 = naiveBayesMultinomialText0.globalInfo();
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", string0);
      
      naiveBayesMultinomialText0.setStopwords(file1);
      assertTrue(file1.isFile());
      
      naiveBayesMultinomialText0.getStemmer();
      boolean boolean1 = naiveBayesMultinomialText0.getUseWordFrequencies();
      assertFalse(boolean1);
      
      boolean boolean2 = naiveBayesMultinomialText0.getUseStopList();
      assertTrue(boolean2 == boolean0);
      
      String string1 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string1);
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 1
  /*Coverage entropy=3.682829073994387
  */
  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      IteratedLovinsStemmer iteratedLovinsStemmer1 = new IteratedLovinsStemmer();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      double[] doubleArray0 = new double[9];
      Instances instances0 = naiveBayesMultinomialText0.m_data;
      naiveBayesMultinomialText0.m_data = null;
      doubleArray0[0] = (-2260.3718498926);
      doubleArray0[1] = (-2260.3718498926);
      doubleArray0[2] = (-2260.3718498926);
      doubleArray0[3] = (-2260.3718498926);
      doubleArray0[4] = (-2260.3718498926);
      TestInstances testInstances0 = new TestInstances();
      Instances instances1 = testInstances0.generate();
      naiveBayesMultinomialText0.m_norm = 1609.10157223;
      Instances instances2 = new Instances(instances1, (-1));
      TestInstances.main(stringArray0);
      SystemInUtil.addInputLine("gg'~B=pl>L{_");
      testInstances0.setNumRelationalDate(30);
      Instances instances3 = testInstances0.generate();
      naiveBayesMultinomialText0.buildClassifier(instances3);
      naiveBayesMultinomialText0.normTipText();
      naiveBayesMultinomialText0.toString();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      naiveBayesMultinomialText0.stemmerTipText();
      Integer integer0 = new Integer((-1172));
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      assertEquals(1609.10157223, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 2
  /*Coverage entropy=2.108906772045218
  */
  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      IBk iBk0 = new IBk();
      MockRandom mockRandom0 = new MockRandom();
      Instances instances1 = new Instances(instances0, (-2));
      double[] doubleArray0 = new double[5];
      doubleArray0[0] = (double) (-2);
      doubleArray0[1] = (double) 4;
      doubleArray0[2] = 2687.2548717;
      doubleArray0[3] = (double) 1;
      doubleArray0[4] = (double) 4;
      DenseInstance denseInstance0 = new DenseInstance(1, doubleArray0);
      instances0.add((Instance) denseInstance0);
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances0);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 3
  /*Coverage entropy=2.6133156453895694
  */
  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities0.find();
      Attribute attribute0 = new Attribute("Unable to create WEKA_HOMEo(", vector0, (-1725));
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)0;
      byteArray0[1] = (byte) (-1);
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.NO_CLASS;
      findWithCapabilities0.disableNot(capabilities_Capability0);
      findWithCapabilities0.find();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      RegressionByDiscretization regressionByDiscretization0 = new RegressionByDiscretization();
      Capabilities capabilities1 = regressionByDiscretization0.getCapabilities();
      TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      SMOreg sMOreg0 = new SMOreg();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      naiveBayesMultinomialText0.normTipText();
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t9.0\nclass2\t4.0\nclass3\t7.0\nclass4\t4.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\nover\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nthe\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nThe\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nquick\t7.38905609893065\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nlazy\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\njumps\t20.085536923187668\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nbrown\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\ndog\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nfox\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\n", string0);
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.stemmerTipText();
      Integer integer0 = new Integer(3);
      naiveBayesMultinomialText1.LNormTipText();
      naiveBayesMultinomialText1.stopwordsTipText();
      String string1 = naiveBayesMultinomialText1.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string1);
      
      String string2 = naiveBayesMultinomialText1.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string2);
      
      naiveBayesMultinomialText1.useStopListTipText();
      naiveBayesMultinomialText0.getLowercaseTokens();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      String string3 = naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string3);
      
      naiveBayesMultinomialText0.useStopListTipText();
      System.setCurrentTimeMillis(1);
      naiveBayesMultinomialText1.normalizeDocLengthTipText();
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
  }

  /**
  //Test case number: 4
  /*Coverage entropy=1.830738805564335
  */
  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      NormalEstimator normalEstimator0 = new NormalEstimator(0.0);
      Capabilities capabilities0 = normalEstimator0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      IBk iBk0 = new IBk((-2));
      MockRandom mockRandom0 = new MockRandom();
      Instances instances1 = new Instances(instances0, 4);
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances1);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // weka.classifiers.bayes.NaiveBayesMultinomialText: Class attribute not set!
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 5
  /*Coverage entropy=3.1239058144637433
  */
  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      Vector<String> vector0 = findWithCapabilities0.find();
      Attribute attribute0 = new Attribute("Unable to create WEKA_HOMEo(", vector0, (-1725));
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)0;
      byteArray0[1] = (byte) (-1);
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.NO_CLASS;
      findWithCapabilities0.disableNot(capabilities_Capability0);
      findWithCapabilities0.find();
      RegressionByDiscretization regressionByDiscretization0 = new RegressionByDiscretization();
      Capabilities capabilities0 = regressionByDiscretization0.getCapabilities();
      TestInstances.forCapabilities(capabilities0);
      SMOreg sMOreg0 = new SMOreg();
      naiveBayesMultinomialText0.normTipText();
      naiveBayesMultinomialText0.toString();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.stemmerTipText();
      Integer integer0 = new Integer(3);
      naiveBayesMultinomialText1.LNormTipText();
      naiveBayesMultinomialText1.stopwordsTipText();
      naiveBayesMultinomialText1.periodicPruningTipText();
      naiveBayesMultinomialText1.tokenizerTipText();
      naiveBayesMultinomialText1.useStopListTipText();
      naiveBayesMultinomialText0.getLowercaseTokens();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      
      System.setCurrentTimeMillis(1);
      String string0 = naiveBayesMultinomialText1.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string0);
  }

  /**
  //Test case number: 6
  /*Coverage entropy=2.2937278511558032
  */
  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      findWithCapabilities0.getOptions();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities1.find();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.RELATIONAL_ATTRIBUTES;
      findWithCapabilities1.disable(capabilities_Capability0);
      findWithCapabilities1.find();
      Attribute attribute0 = new Attribute("ever", vector0, (-1));
      FindWithCapabilities findWithCapabilities2 = new FindWithCapabilities();
      findWithCapabilities1.getOptions();
      FindWithCapabilities findWithCapabilities3 = new FindWithCapabilities();
      Capabilities.Capability capabilities_Capability1 = Capabilities.Capability.UNARY_ATTRIBUTES;
      attribute0.copy();
      findWithCapabilities1.disableNot(capabilities_Capability1);
      findWithCapabilities0.find();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances.forCapabilities(capabilities0);
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      SMOreg sMOreg0 = new SMOreg();
      Capabilities capabilities1 = capabilities0.getClassCapabilities();
      capabilities1.getAttributeCapabilities();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      findWithCapabilities1.find();
      int[] intArray0 = new int[8];
      intArray0[0] = (-2);
      intArray0[1] = 0;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(18.07062498535976, intArray0, (-2));
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertArrayEquals(new double[] {0.3750000000000001, 0.16666666666666669, 0.29166666666666663, 0.16666666666666669}, doubleArray0, 0.01);
      
      String string0 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string0);
      
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      String string1 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string1);
      
      String string2 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string2);
      
      String string3 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string3);
      
      naiveBayesMultinomialText0.useStopListTipText();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 7
  /*Coverage entropy=2.6272535078010946
  */
  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      testInstances0.setNumInstancesRelational((-2151));
      TestInstances testInstances1 = new TestInstances();
      Instances instances0 = testInstances1.generate();
      IBk iBk0 = new IBk();
      MockRandom mockRandom0 = new MockRandom();
      Instances instances1 = testInstances1.generate(" ");
      double[] doubleArray0 = new double[9];
      Instances instances2 = new Instances(instances1, 1);
      doubleArray0[1] = (double) (-1);
      TestInstances.main(testInstances1.DEFAULT_WORDS);
      naiveBayesMultinomialText0.m_periodicP = (-1);
      doubleArray0[2] = (double) (-2);
      doubleArray0[4] = (double) (-1);
      SystemInUtil.addInputLine("-W");
      doubleArray0[5] = 0.0;
      doubleArray0[6] = (double) (-1);
      doubleArray0[7] = (double) (-1);
      doubleArray0[8] = (double) (-1);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      naiveBayesMultinomialText0.debugTipText();
      naiveBayesMultinomialText0.normTipText();
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t12.0\nclass2\t10.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\t\n", string0);
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      String string1 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string1);
      
      Integer integer0 = new Integer((-2));
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.periodicPruningTipText();
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 8
  /*Coverage entropy=2.1604379183518763
  */
  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities0.find();
      Attribute attribute0 = new Attribute("Unable to create WEKA_HOME (", vector0, (-1725));
      byte[] byteArray0 = new byte[2];
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "Cover trees for nearest neighbor");
      byteArray0[1] = (byte) (-1);
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (double) 1;
      doubleArray0[1] = (double) 0;
      doubleArray0[2] = (double) 3;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = (double) 2;
      doubleArray0[5] = (double) 1;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance1);
      binarySparseInstance0.setMissing(attribute0);
      int[] intArray0 = new int[5];
      intArray0[0] = (-1725);
      intArray0[1] = (-1725);
      intArray0[2] = (-1725);
      intArray0[1] = 0;
      naiveBayesMultinomialText0.m_useStopList = true;
      intArray0[4] = 1;
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate("Unable to create WEKA_HOME (");
      naiveBayesMultinomialText0.buildClassifier(instances0);
      AbstractClassifier.runClassifier(naiveBayesMultinomialText0, testInstances0.DEFAULT_WORDS);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 9
  /*Coverage entropy=2.2268974827716517
  */
  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.listOptions();
      TestInstances testInstances0 = new TestInstances();
      testInstances0.setNumInstancesRelational((-2151));
      TestInstances testInstances1 = new TestInstances();
      Instances instances0 = testInstances1.generate();
      IBk iBk0 = new IBk();
      MockRandom mockRandom0 = new MockRandom();
      instances0.resample(mockRandom0);
      double[] doubleArray0 = new double[9];
      testInstances1.generate();
      doubleArray0[0] = (double) (-1);
      doubleArray0[1] = (double) (-1);
      TestInstances.main(testInstances1.DEFAULT_WORDS);
      doubleArray0[2] = (double) (-2);
      doubleArray0[4] = (double) (-1);
      doubleArray0[5] = 0.0;
      int[] intArray0 = new int[8];
      intArray0[0] = 2;
      intArray0[1] = (-1);
      intArray0[2] = 4;
      intArray0[3] = 1;
      intArray0[4] = (-1);
      intArray0[5] = 1;
      intArray0[6] = (-1);
      intArray0[7] = (-2);
      SparseInstance sparseInstance0 = new SparseInstance(0.0, doubleArray0, intArray0, 4);
      instances0.add((Instance) sparseInstance0);
      doubleArray0[6] = (double) (-1);
      doubleArray0[7] = (double) (-1);
      doubleArray0[8] = (double) (-1);
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances0);
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
         //
         // Index: 2, Size: 2
         //
         verifyException("java.util.ArrayList", e);
      }
  }

  /**
  //Test case number: 10
  /*Coverage entropy=2.9089952807029498
  */
  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      FileSystemHandling.shouldAllThrowIOExceptions();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText1.normTipText();
      assertEquals("The norm of the instances after normalization.", string0);
      
      testInstances0.setNumInstances(3460);
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      double[] doubleArray0 = new double[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-2), doubleArray0);
      Instance instance0 = linearNNSearch0.nearestNeighbour(binarySparseInstance0);
      double[] doubleArray1 = naiveBayesMultinomialText0.distributionForInstance(instance0);
      assertArrayEquals(new double[] {0.5454545454545454, 0.4545454545454546}, doubleArray1, 0.01);
      
      naiveBayesMultinomialText0.stemmerTipText();
      Integer integer0 = new Integer(4771);
      String string1 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string1);
      
      naiveBayesMultinomialText1.useWordFrequenciesTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      String string2 = naiveBayesMultinomialText1.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string2);
      
      naiveBayesMultinomialText2.tokenizerTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText3.stopwordsTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText4 = new NaiveBayesMultinomialText();
      String string3 = naiveBayesMultinomialText4.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string3);
      
      String string4 = naiveBayesMultinomialText1.minWordFrequencyTipText();
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string4);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
  }

  /**
  //Test case number: 11
  /*Coverage entropy=2.764239412225641
  */
  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.listOptions();
      TestInstances testInstances0 = new TestInstances();
      testInstances0.setNumInstancesRelational((-2151));
      TestInstances testInstances1 = new TestInstances();
      Instances instances0 = testInstances1.generate();
      IBk iBk0 = new IBk();
      MockRandom mockRandom0 = new MockRandom();
      instances0.resample(mockRandom0);
      double[] doubleArray0 = new double[9];
      testInstances1.generate();
      doubleArray0[0] = (double) (-1);
      doubleArray0[1] = (double) (-1);
      TestInstances.main(testInstances1.DEFAULT_WORDS);
      doubleArray0[2] = (double) (-2);
      doubleArray0[4] = (double) (-1);
      doubleArray0[5] = 0.0;
      doubleArray0[6] = (double) (-1);
      doubleArray0[7] = (double) (-1);
      doubleArray0[8] = (double) (-1);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      naiveBayesMultinomialText0.debugTipText();
      naiveBayesMultinomialText0.setLNorm(0.0);
      naiveBayesMultinomialText0.normTipText();
      naiveBayesMultinomialText0.toString();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      naiveBayesMultinomialText0.stemmerTipText();
      System.setCurrentTimeMillis((-2));
      naiveBayesMultinomialText0.getLNorm();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.m_periodicP = 1;
      naiveBayesMultinomialText1.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertEquals(0.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 12
  /*Coverage entropy=2.7746000829682944
  */
  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      String[] stringArray0 = findWithCapabilities1.getOptions();
      FindWithCapabilities findWithCapabilities2 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities2.find();
      Attribute attribute0 = new Attribute("Unable to create WEKA_HOME (", vector0, (-1725));
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, (byte[]) null);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance0);
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      Tokenizer.runTokenizer(alphabeticTokenizer0, stringArray0);
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      wordTokenizer0.setDelimiters("");
      naiveBayesMultinomialText0.setTokenizer((Tokenizer) null);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.getOptions();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 13
  /*Coverage entropy=1.394993443200781
  */
  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities0.find();
      Attribute attribute0 = new Attribute("Unable to create WEKA_HOME (", vector0, (-1725));
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)0;
      byteArray0[1] = (byte)0;
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (double) 1;
      doubleArray0[1] = (double) 0;
      doubleArray0[2] = (double) 3;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = (double) 2;
      doubleArray0[5] = (double) 1;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance1);
      binarySparseInstance0.setMissing(attribute0);
      int[] intArray0 = new int[5];
      binarySparseInstance1.s_numericAfterDecimalPoint = (-1725);
      intArray0[1] = (-1725);
      intArray0[2] = (-1725);
      intArray0[1] = 0;
      naiveBayesMultinomialText0.m_useStopList = true;
      sparseInstance0.isMissing(1);
      intArray0[4] = 1;
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-1725), intArray0, (-1725));
      try { 
        naiveBayesMultinomialText0.distributionForInstance(sparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 14
  /*Coverage entropy=2.63320121977379
  */
  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      String[] stringArray0 = findWithCapabilities0.getOptions();
      FindWithCapabilities findWithCapabilities2 = new FindWithCapabilities();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.UNARY_ATTRIBUTES;
      findWithCapabilities0.disableNot(capabilities_Capability0);
      findWithCapabilities2.find();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      findWithCapabilities2.setFilename(".bsi");
      naiveBayesMultinomialText0.buildClassifier(instances0);
      naiveBayesMultinomialText0.setOptions(stringArray0);
      naiveBayesMultinomialText0.setStemmer(iteratedLovinsStemmer0);
      naiveBayesMultinomialText0.getStemmer();
      naiveBayesMultinomialText0.useStopListTipText();
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
  }

  /**
  //Test case number: 15
  /*Coverage entropy=1.543056733112554
  */
  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      String[] stringArray0 = findWithCapabilities0.getOptions();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      ArrayList<Locale.LanguageRange> arrayList0 = new ArrayList<Locale.LanguageRange>();
      List<String> list0 = Locale.filterTags((List<Locale.LanguageRange>) arrayList0, (Collection<String>) null);
      List<String> list1 = Locale.filterTags((List<Locale.LanguageRange>) arrayList0, (Collection<String>) list0);
      Locale.FilteringMode locale_FilteringMode0 = Locale.FilteringMode.IGNORE_EXTENDED_RANGES;
      List<String> list2 = Locale.filterTags((List<Locale.LanguageRange>) arrayList0, (Collection<String>) list1, locale_FilteringMode0);
      Attribute attribute0 = new Attribute("Loss function: ", list2, (-1725));
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte) (-1);
      byteArray0[1] = (byte) (-1);
      byteArray0[2] = (byte)0;
      byteArray0[3] = (byte)6;
      byteArray0[4] = (byte) (-1);
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      Tokenizer.runTokenizer(alphabeticTokenizer0, stringArray0);
      Tokenizer.runTokenizer(alphabeticTokenizer0, stringArray0);
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      wordTokenizer0.setDelimiters("@end");
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setStemmer((Stemmer) null);
      Stemmer stemmer0 = naiveBayesMultinomialText0.getStemmer();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertNotNull(stemmer0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 16
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities0.find();
      Attribute attribute0 = new Attribute("Unable to create WEKA_HOME (", vector0, (-1725));
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)0;
      byteArray0[1] = (byte) (-1);
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (double) 1;
      doubleArray0[1] = (double) 0;
      doubleArray0[2] = (double) 3;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = (double) 2;
      doubleArray0[5] = (double) 1;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance1);
      binarySparseInstance0.setMissing(attribute0);
      int[] intArray0 = new int[5];
      intArray0[0] = (-1725);
      intArray0[1] = (-2722);
      intArray0[2] = (-2722);
      intArray0[1] = 0;
      naiveBayesMultinomialText0.m_useStopList = true;
      intArray0[4] = 1;
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2722), intArray0, (-2722));
      boolean boolean0 = naiveBayesMultinomialText0.getNormalizeDocLength();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(boolean0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 17
  /*Coverage entropy=2.063983366979555
  */
  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.listOptions();
      TestInstances testInstances0 = new TestInstances();
      testInstances0.setNumInstancesRelational((-1));
      TestInstances testInstances1 = new TestInstances();
      testInstances1.generate(" ");
      IBk iBk0 = new IBk();
      double[] doubleArray0 = new double[9];
      doubleArray0[0] = (double) (-1);
      doubleArray0[1] = (double) (-1);
      TestInstances.main(testInstances1.DEFAULT_WORDS);
      doubleArray0[2] = (double) (-2);
      doubleArray0[4] = (double) (-1);
      doubleArray0[5] = 0.0;
      doubleArray0[6] = 3.0;
      doubleArray0[7] = (double) (-1);
      testInstances1.generate();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      findWithCapabilities0.setFilename("");
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("t]4gs", arrayList0, 5);
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // weka.classifiers.bayes.NaiveBayesMultinomialText: No attributes!
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 18
  /*Coverage entropy=1.8408398156653452
  */
  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities1.find();
      Attribute attribute0 = new Attribute("Unable to create WEKA_HOMEo(", vector0, 687);
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.NO_CLASS;
      findWithCapabilities0.disableNot(capabilities_Capability0);
      findWithCapabilities0.find();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances.forCapabilities(capabilities0);
      RegressionByDiscretization regressionByDiscretization0 = new RegressionByDiscretization();
      Capabilities capabilities1 = regressionByDiscretization0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      SMOreg sMOreg0 = new SMOreg();
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // weka.classifiers.bayes.NaiveBayesMultinomialText: Cannot handle numeric class!
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 19
  /*Coverage entropy=1.7766234846545668
  */
  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      FileSystemHandling.shouldAllThrowIOExceptions();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      String[] stringArray0 = new String[5];
      stringArray0[0] = "-M";
      stringArray0[1] = "-P";
      stringArray0[2] = "ci,|a:T_y9[gWw;#";
      stringArray0[3] = "The base for the expansion constant.";
      String string0 = "";
      stringArray0[4] = "";
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: NumberFormatException");
      
      } catch(NumberFormatException e) {
         //
         // For input string: \"ci,|a:T_y9[gWw;#\"
         //
         verifyException("java.lang.NumberFormatException", e);
      }
  }

  /**
  //Test case number: 20
  /*Coverage entropy=2.764239412225641
  */
  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      TestInstances testInstances1 = new TestInstances();
      Instances instances0 = testInstances1.generate();
      IBk iBk0 = new IBk();
      MockRandom mockRandom0 = new MockRandom();
      Instances instances1 = new Instances(instances0, (-2));
      double[] doubleArray0 = new double[9];
      Instances instances2 = new Instances(instances1, 1);
      TestInstances.main(testInstances1.DEFAULT_WORDS);
      doubleArray0[2] = (double) (-2);
      doubleArray0[4] = (double) (-1);
      SystemInUtil.addInputLine("-W");
      doubleArray0[5] = (double) (-2);
      doubleArray0[6] = (double) (-1);
      doubleArray0[7] = (double) (-1);
      doubleArray0[8] = (double) (-1);
      testInstances0.setNumRelationalDate((-1172));
      Instances instances3 = testInstances1.generate();
      naiveBayesMultinomialText0.buildClassifier(instances3);
      naiveBayesMultinomialText0.normTipText();
      naiveBayesMultinomialText0.toString();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      naiveBayesMultinomialText0.stemmerTipText();
      Integer integer0 = new Integer(4);
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) null, false);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 21
  /*Coverage entropy=1.5271837172395382
  */
  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      SystemInUtil.addInputLine("");
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      serializedClassifier0.getCurrentModel();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      iteratedLovinsStemmer0.stem("s;/#W$,v.$(C[#");
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      FileSystemHandling.appendStringToFile(evoSuiteFile0, " > ");
      String[] stringArray0 = new String[7];
      stringArray0[0] = "A work that is printed and bound, but without a named publisher or sponsoring institution.";
      stringArray0[1] = "A work that is printed and bound, but without a named publisher or sponsoring institution.";
      stringArray0[2] = "s;/#W$,v.$(C[#";
      stringArray0[3] = " > ";
      stringArray0[1] = "A work that is printed and bound, but without a named publisher or sponsoring institution.";
      File file0 = serializedClassifier0.getModelFile();
      file0.createNewFile();
      serializedClassifier0.setModelFile(file0);
      stringArray0[5] = " > ";
      stringArray0[6] = "s;/#W$,v.$(C[#";
      AbstractClassifier.runClassifier(serializedClassifier0, stringArray0);
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 22
  /*Coverage entropy=2.7590479998760493
  */
  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      serializedClassifier0.getCurrentModel();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      iteratedLovinsStemmer0.stemString("");
      iteratedLovinsStemmer0.stem("");
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      FileSystemHandling.appendStringToFile(evoSuiteFile0, "");
      VotedPerceptron votedPerceptron0 = new VotedPerceptron();
      String[] stringArray0 = new String[7];
      stringArray0[2] = "";
      stringArray0[1] = "s;/#W$,v.$(C[#";
      stringArray0[2] = "";
      IteratedLovinsStemmer.main(stringArray0);
      stringArray0[0] = "Unpruned tree and reduced error pruning can't be selected simultaneously!";
      stringArray0[4] = "Unpruned tree and reduced error pruning can't be selected simultaneously!";
      stringArray0[5] = "";
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      precomputedKernelMatrixKernel0.setChecksTurnedOff(false);
      File file0 = precomputedKernelMatrixKernel0.getKernelMatrixFile();
      naiveBayesMultinomialText0.setStopwords(file0);
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(14, stringArray1.length);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 23
  /*Coverage entropy=2.916008499871352
  */
  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      FileSystemHandling.shouldAllThrowIOExceptions();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      testInstances0.setNumInstances(3460);
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      double[] doubleArray0 = new double[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-2), doubleArray0);
      Instance instance0 = linearNNSearch0.nearestNeighbour(binarySparseInstance0);
      naiveBayesMultinomialText0.distributionForInstance(instance0);
      naiveBayesMultinomialText0.stemmerTipText();
      Integer integer0 = new Integer(4771);
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText1.useWordFrequenciesTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText2.stopwordsTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      naiveBayesMultinomialText3.minWordFrequencyTipText();
      naiveBayesMultinomialText3.tokenizeInstance(instance0, false);
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
  }

  /**
  //Test case number: 24
  /*Coverage entropy=1.2961346570528898
  */
  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      testInstances0.generate();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(3569);
      int[] intArray0 = new int[7];
      intArray0[0] = 3569;
      intArray0[1] = (-257);
      intArray0[4] = 5278;
      intArray0[5] = (-1);
      intArray0[6] = (-2722);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((-2), intArray0, (-2744));
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((SparseInstance) binarySparseInstance1);
      testInstances0.setNumNumeric((-257));
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance0);
      Attribute attribute0 = new Attribute(" ", " ", 0);
      binarySparseInstance0.setMissing(attribute0);
      sparseInstance0.isMissing(5278);
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance(0.693147181, intArray0, (-3689));
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance2);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 25
  /*Coverage entropy=3.2698986887396755
  */
  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      String[] stringArray0 = Locale.getISOCountries();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      ArrayList<Locale.LanguageRange> arrayList0 = new ArrayList<Locale.LanguageRange>();
      List<String> list0 = Locale.filterTags((List<Locale.LanguageRange>) arrayList0, (Collection<String>) null);
      naiveBayesMultinomialText0.reset();
      List<String> list1 = Locale.filterTags((List<Locale.LanguageRange>) arrayList0, (Collection<String>) list0);
      Locale.FilteringMode locale_FilteringMode0 = Locale.FilteringMode.EXTENDED_FILTERING;
      List<String> list2 = Locale.filterTags((List<Locale.LanguageRange>) arrayList0, (Collection<String>) list1, locale_FilteringMode0);
      Attribute attribute0 = new Attribute("Loss function: ", list2, (-1712));
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte) (-1);
      byteArray0[3] = (byte) (-30);
      byteArray0[4] = (byte) (-1);
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      Tokenizer.runTokenizer(alphabeticTokenizer0, stringArray0);
      Tokenizer.runTokenizer(alphabeticTokenizer0, stringArray0);
      String[] stringArray1 = new String[4];
      stringArray1[0] = "@end";
      stringArray1[1] = "@end";
      stringArray1[2] = "@end";
      alphabeticTokenizer0.hasMoreElements();
      stringArray1[3] = "@end";
      Tokenizer.runTokenizer(alphabeticTokenizer0, stringArray1);
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      wordTokenizer0.setDelimiters("@end");
      naiveBayesMultinomialText0.setTokenizer(alphabeticTokenizer0);
      String[] stringArray2 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.setOptions(stringArray2);
      Random.setNextRandom(56);
  }

  /**
  //Test case number: 26
  /*Coverage entropy=0.9623351446188083
  */
  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      findWithCapabilities0.find();
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)0;
      byteArray0[1] = (byte) (-1);
      double[] doubleArray0 = new double[6];
      doubleArray0[3] = 0.0;
      int[] intArray0 = new int[5];
      naiveBayesMultinomialText0.m_useStopList = true;
      naiveBayesMultinomialText0.setStopwords((File) null);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 27
  /*Coverage entropy=1.830738805564335
  */
  @Test(timeout = 4000)
  public void test27()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities0.find();
      String string0 = "Unable to create WEKA_HOME (";
      int int0 = (-1725);
      Attribute attribute0 = new Attribute("Unable to create WEKA_HOME (", vector0, (-1725));
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)0;
      byteArray0[1] = (byte)0;
      try { 
        naiveBayesMultinomialText0.buildClassifier((Instances) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 28
  /*Coverage entropy=1.2961346570528898
  */
  @Test(timeout = 4000)
  public void test28()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities0.find();
      Attribute attribute0 = new Attribute("Unable to create WEKA_HOME (", vector0, (-1725));
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)0;
      byteArray0[1] = (byte) (-1);
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (double) 1;
      doubleArray0[1] = (double) 0;
      doubleArray0[2] = (double) 3;
      doubleArray0[3] = 0.0;
      doubleArray0[3] = (double) 2;
      doubleArray0[5] = (double) 1;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance1);
      int[] intArray0 = new int[5];
      intArray0[0] = (int) (byte) (-1);
      intArray0[1] = 2;
      intArray0[2] = (-2722);
      intArray0[3] = (-2722);
      intArray0[4] = (int) (byte) (-1);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-1.0), intArray0, (byte) (-1));
      binarySparseInstance0.deleteAttributeAt(4);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 29
  /*Coverage entropy=2.3117676775595015
  */
  @Test(timeout = 4000)
  public void test29()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities0.find();
      Attribute attribute0 = new Attribute("Unable to create WEKA_HOME (", vector0, (-1725));
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)0;
      byteArray0[1] = (byte)0;
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (double) 1;
      doubleArray0[1] = (double) 0;
      doubleArray0[2] = (double) 3;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = (double) 2;
      doubleArray0[5] = (double) 1;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1);
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      File file0 = serializedClassifier0.getModelFile();
      MockFile mockFile0 = new MockFile(file0, "");
      mockFile0.setReadable(true, true);
      SerializedClassifier serializedClassifier1 = new SerializedClassifier();
      serializedClassifier1.setModelFile(mockFile0);
      String[] stringArray0 = new String[7];
      stringArray0[0] = "-tokenizer";
      stringArray0[1] = "real";
      stringArray0[2] = "real";
      stringArray0[3] = "string";
      stringArray0[4] = "integer";
      stringArray0[5] = "@attribute";
      stringArray0[6] = "@end";
      AbstractClassifier.runClassifier(naiveBayesMultinomialText0, stringArray0);
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: ClassNotFoundException");
      
      } catch(ClassNotFoundException e) {
      }
  }

  /**
  //Test case number: 30
  /*Coverage entropy=1.830738805564335
  */
  @Test(timeout = 4000)
  public void test30()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      wordTokenizer0.getRevision();
      wordTokenizer0.setDelimiters("");
      naiveBayesMultinomialText0.setTokenizer(wordTokenizer0);
      String[] stringArray0 = wordTokenizer0.getOptions();
      double[] doubleArray0 = new double[9];
      doubleArray0[0] = (-1.0);
      doubleArray0[1] = (-4401.1754697);
      doubleArray0[2] = (-1802.58875);
      doubleArray0[4] = 741.454;
      doubleArray0[5] = 1195.93262;
      Tokenizer.runTokenizer(wordTokenizer0, stringArray0);
      wordTokenizer0.getRevision();
      FileSystemHandling.shouldAllThrowIOExceptions();
      doubleArray0[7] = 4307.2;
      doubleArray0[8] = (-1.0);
      naiveBayesMultinomialText0.m_probOfClass = doubleArray0;
      naiveBayesMultinomialText0.stopwordsTipText();
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.toString();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 31
  /*Coverage entropy=2.253297930766516
  */
  @Test(timeout = 4000)
  public void test31()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      naiveBayesMultinomialText0.m_normalize = false;
      double[] doubleArray0 = new double[9];
      doubleArray0[0] = 493.701115;
      doubleArray0[1] = 0.001;
      naiveBayesMultinomialText0.m_minWordP = 493.701115;
      doubleArray0[2] = (-2307.4981371);
      doubleArray0[3] = 2.0;
      doubleArray0[4] = (-3112.485122);
      naiveBayesMultinomialText0.setNorm((-1564.8840137008065));
      doubleArray0[5] = 190.91051893;
      doubleArray0[6] = 0.0;
      doubleArray0[7] = 0.0;
      doubleArray0[8] = 0.0;
      naiveBayesMultinomialText0.m_probOfClass = doubleArray0;
      naiveBayesMultinomialText0.setUseStopList(false);
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      double double0 = naiveBayesMultinomialText0.getNorm();
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals((-1564.8840137008065), double0, 0.01);
  }

  /**
  //Test case number: 32
  /*Coverage entropy=2.916008499871352
  */
  @Test(timeout = 4000)
  public void test32()  throws Throwable  {
      FileSystemHandling.shouldAllThrowIOExceptions();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      testInstances0.setNumInstances(3460);
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      double[] doubleArray0 = new double[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-2), doubleArray0);
      Instance instance0 = linearNNSearch0.nearestNeighbour(binarySparseInstance0);
      naiveBayesMultinomialText0.distributionForInstance(instance0);
      naiveBayesMultinomialText0.stemmerTipText();
      Integer integer0 = new Integer(4771);
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText1.useWordFrequenciesTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText2.stopwordsTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText3.minWordFrequencyTipText();
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText3.tokenizeInstance(binarySparseInstance0, true);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 33
  /*Coverage entropy=2.9462916981154974
  */
  @Test(timeout = 4000)
  public void test33()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.createFolder(evoSuiteFile0);
      testInstances0.setNumInstances(3460);
      int[] intArray0 = new int[9];
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      intArray0[1] = 3460;
      intArray0[3] = (-1);
      naiveBayesMultinomialText1.stemmerTipText();
      Integer integer0 = new Integer(3460);
      naiveBayesMultinomialText1.normalizeDocLengthTipText();
      naiveBayesMultinomialText1.useWordFrequenciesTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.useStopListTipText();
      naiveBayesMultinomialText1.tokenizerTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText2.tokenizeInstance((Instance) null, true);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 34
  /*Coverage entropy=1.2961346570528898
  */
  @Test(timeout = 4000)
  public void test34()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      String[] stringArray0 = Locale.getISOCountries();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      ArrayList<Locale.LanguageRange> arrayList0 = new ArrayList<Locale.LanguageRange>();
      List<String> list0 = Locale.filterTags((List<Locale.LanguageRange>) arrayList0, (Collection<String>) null);
      List<String> list1 = Locale.filterTags((List<Locale.LanguageRange>) arrayList0, (Collection<String>) list0);
      Locale.FilteringMode locale_FilteringMode0 = Locale.FilteringMode.EXTENDED_FILTERING;
      List<String> list2 = Locale.filterTags((List<Locale.LanguageRange>) arrayList0, (Collection<String>) list1, locale_FilteringMode0);
      Attribute attribute0 = new Attribute("Loss function: ", list2, (-1712));
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte) (-1);
      byteArray0[3] = (byte)6;
      byteArray0[4] = (byte) (-1);
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      Tokenizer.runTokenizer(alphabeticTokenizer0, stringArray0);
      Tokenizer.runTokenizer(alphabeticTokenizer0, stringArray0);
      String[] stringArray1 = new String[4];
      stringArray1[0] = "@end";
      stringArray1[1] = "@end";
      stringArray1[2] = "@end";
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-1712), (int[]) null, (byte)6);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 35
  /*Coverage entropy=1.3788419678046633
  */
  @Test(timeout = 4000)
  public void test35()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      findWithCapabilities0.find();
      File file0 = MockFile.createTempFile("Unable to create WEKA_HOME (", "Unable to create WEKA_HOME (");
      naiveBayesMultinomialText0.setStopwords(file0);
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      FileSystemHandling.setPermissions(evoSuiteFile0, false, false, false);
      FileSystemHandling.appendStringToFile(evoSuiteFile0, "");
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
  }

  /**
  //Test case number: 36
  /*Coverage entropy=2.2584261358947213
  */
  @Test(timeout = 4000)
  public void test36()  throws Throwable  {
      FileSystemHandling.createFolder((EvoSuiteFile) null);
      TestInstances testInstances0 = new TestInstances();
      testInstances0.generate();
      FileSystemHandling.setPermissions((EvoSuiteFile) null, false, false, true);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      String string1 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string1);
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      Integer integer0 = new Integer((-2));
      String string2 = naiveBayesMultinomialText0.normTipText();
      assertEquals("The norm of the instances after normalization.", string2);
      
      String string3 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string3);
      
      String string4 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", string4);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 37
  /*Coverage entropy=2.946317206367417
  */
  @Test(timeout = 4000)
  public void test37()  throws Throwable  {
      FileSystemHandling.createFolder((EvoSuiteFile) null);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      testInstances0.setNumInstances(3460);
      int[] intArray0 = new int[9];
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "@data");
      intArray0[1] = (-403);
      intArray0[1] = 3460;
      naiveBayesMultinomialText2.setUseWordFrequencies(false);
      Integer integer0 = new Integer(2337);
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText1.useWordFrequenciesTipText();
      naiveBayesMultinomialText2.useWordFrequenciesTipText();
      naiveBayesMultinomialText2.periodicPruningTipText();
      naiveBayesMultinomialText1.tokenizerTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      SystemInUtil.addInputLine("");
      naiveBayesMultinomialText3.m_useStopList = false;
      naiveBayesMultinomialText3.useStopListTipText();
      naiveBayesMultinomialText3.LNormTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText4 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText4.m_minWordP = (double) 3460;
      naiveBayesMultinomialText4.stopwordsTipText();
      System.setCurrentTimeMillis((-2206L));
  }

  /**
  //Test case number: 38
  /*Coverage entropy=3.3132218432896665
  */
  @Test(timeout = 4000)
  public void test38()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities0.find();
      Attribute attribute0 = new Attribute("Unable to create WEKA_HOME (", vector0, (-1725));
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)0;
      byteArray0[1] = (byte) (-1);
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (double) 1;
      doubleArray0[1] = (double) 0;
      doubleArray0[2] = (double) 3;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = (double) 2;
      doubleArray0[5] = (double) 1;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance1);
      binarySparseInstance0.setMissing(attribute0);
      int[] intArray0 = new int[5];
      intArray0[0] = (-1725);
      intArray0[1] = (-2722);
      intArray0[2] = (-2722);
      intArray0[1] = 0;
      naiveBayesMultinomialText0.m_useStopList = true;
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      wordTokenizer0.setDelimiters("");
      naiveBayesMultinomialText0.setTokenizer(wordTokenizer0);
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // No value given for -delimiters option.
         //
         verifyException("weka.core.Utils", e);
      }
  }

  /**
  //Test case number: 39
  /*Coverage entropy=2.638868765760842
  */
  @Test(timeout = 4000)
  public void test39()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[7];
      stringArray0[0] = "normalize";
      stringArray0[1] = "";
      stringArray0[2] = " ";
      stringArray0[3] = "-W";
      stringArray0[4] = "%A<V/2E6s7',B|?;5[";
      stringArray0[5] = "6dVy),";
      stringArray0[6] = "T/3]hqvM3dcaCA";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      DenseInstance denseInstance0 = new DenseInstance(111);
      denseInstance0.toStringNoWeight(111);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(denseInstance0);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 40
  /*Coverage entropy=3.269394275120508
  */
  @Test(timeout = 4000)
  public void test40()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_normalize = true;
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
  }

  /**
  //Test case number: 41
  /*Coverage entropy=2.946317206367417
  */
  @Test(timeout = 4000)
  public void test41()  throws Throwable  {
      FileSystemHandling.createFolder((EvoSuiteFile) null);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      testInstances0.setNumInstances(3460);
      int[] intArray0 = new int[9];
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "@data");
      intArray0[1] = (-403);
      naiveBayesMultinomialText0.setMinWordFrequency(3460);
      intArray0[1] = 3460;
      Integer integer0 = new Integer(2337);
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals(3460.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText1.useWordFrequenciesTipText();
      naiveBayesMultinomialText2.useWordFrequenciesTipText();
      naiveBayesMultinomialText2.periodicPruningTipText();
      naiveBayesMultinomialText1.tokenizerTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText3.m_useStopList = false;
      naiveBayesMultinomialText3.useStopListTipText();
      naiveBayesMultinomialText3.LNormTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText4 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText4.m_minWordP = (double) 3460;
      naiveBayesMultinomialText4.stopwordsTipText();
      assertFalse(naiveBayesMultinomialText4.getNormalizeDocLength());
  }

  /**
  //Test case number: 42
  /*Coverage entropy=2.41257681572198
  */
  @Test(timeout = 4000)
  public void test42()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      naiveBayesMultinomialText0.debugTipText();
      naiveBayesMultinomialText0.setLNorm((-1780.0547743026948));
      naiveBayesMultinomialText0.normTipText();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      costSensitiveClassifier0.setSeed(50);
      naiveBayesMultinomialText0.stemmerTipText();
      System.setCurrentTimeMillis(2);
      naiveBayesMultinomialText0.getLNorm();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      assertEquals((-1780.0547743026948), naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 43
  /*Coverage entropy=2.675485446901437
  */
  @Test(timeout = 4000)
  public void test43()  throws Throwable  {
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      SerializedClassifier serializedClassifier1 = new SerializedClassifier();
      serializedClassifier1.getCurrentModel();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      iteratedLovinsStemmer0.stemString("");
      iteratedLovinsStemmer0.stem("");
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, ")|5,wBD3+1L>");
      VotedPerceptron votedPerceptron0 = new VotedPerceptron();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string0);
      
      String[] stringArray0 = new String[4];
      stringArray0[0] = "";
      stringArray0[1] = "";
      stringArray0[2] = "If true, ignores all words that are on the stoplist.";
      stringArray0[3] = "";
      NaiveBayesMultinomialText.main(stringArray0);
      String string1 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string1);
      
      String string2 = naiveBayesMultinomialText0.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string2);
      
      SystemInUtil.addInputLine("");
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      String string3 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string3);
      
      String string4 = naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string4);
      
      boolean boolean0 = naiveBayesMultinomialText0.getNormalizeDocLength();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(boolean0);
  }

  /**
  //Test case number: 44
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test44()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double double0 = naiveBayesMultinomialText0.getLNorm();
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(2.0, double0, 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 45
  /*Coverage entropy=2.423050301858823
  */
  @Test(timeout = 4000)
  public void test45()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities0.find();
      Attribute attribute0 = new Attribute("Unable to create WEKA_HOMEo(", vector0, (-1725));
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)0;
      byteArray0[1] = (byte) (-1);
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.NO_CLASS;
      findWithCapabilities0.disableNot(capabilities_Capability0);
      findWithCapabilities0.find();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      RegressionByDiscretization regressionByDiscretization0 = new RegressionByDiscretization();
      Capabilities capabilities1 = regressionByDiscretization0.getCapabilities();
      TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      capabilities0.getOtherCapabilities();
      SMOreg sMOreg0 = new SMOreg();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.stemmerTipText();
      Integer integer0 = new Integer(1186);
      String string0 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string0);
      
      String string1 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string1);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.useStopListTipText();
      naiveBayesMultinomialText1.tokenizerTipText();
      String string2 = naiveBayesMultinomialText0.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string2);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      Stopwords stopwords0 = new Stopwords();
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      naiveBayesMultinomialText3.m_tokenizer = (Tokenizer) nGramTokenizer0;
      naiveBayesMultinomialText3.m_stopwords = stopwords0;
      naiveBayesMultinomialText2.lowercaseTokensTipText();
      String string3 = naiveBayesMultinomialText2.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string3);
      
      int int0 = naiveBayesMultinomialText3.getPeriodicPruning();
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
      assertEquals(0, int0);
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 46
  /*Coverage entropy=3.0098030568249623
  */
  @Test(timeout = 4000)
  public void test46()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      NullStemmer nullStemmer0 = new NullStemmer();
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) nullStemmer0;
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      testInstances0.setNumInstances(3460);
      int[] intArray0 = new int[9];
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      intArray0[1] = 3460;
      intArray0[3] = (-1);
      naiveBayesMultinomialText1.stemmerTipText();
      Integer integer0 = new Integer(3460);
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText1.useWordFrequenciesTipText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props");
      FileSystemHandling.appendLineToFile(evoSuiteFile0, "How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances");
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.useStopListTipText();
      naiveBayesMultinomialText1.tokenizerTipText();
      naiveBayesMultinomialText2.LNormTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText3.setLowercaseTokens(true);
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 47
  /*Coverage entropy=2.368373327680306
  */
  @Test(timeout = 4000)
  public void test47()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      findWithCapabilities0.find();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      String[] stringArray0 = new String[5];
      stringArray0[0] = "W_$ivVzn@Jy9(bZ";
      stringArray0[1] = ":m";
      stringArray0[2] = "Divergence: ";
      stringArray0[3] = "-normalize";
      stringArray0[4] = "-stemmer";
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // No value given for -stemmer option.
         //
         verifyException("weka.core.Utils", e);
      }
  }

  /**
  //Test case number: 48
  /*Coverage entropy=0.9623351446188083
  */
  @Test(timeout = 4000)
  public void test48()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      File file0 = costSensitiveClassifier0.getOnDemandDirectory();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      naiveBayesMultinomialText0.setStopwords(file0);
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      FileSystemHandling.setPermissions(evoSuiteFile0, false, false, true);
      FileSystemHandling.appendStringToFile(evoSuiteFile0, "U");
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
  }

  /**
  //Test case number: 49
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test49()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      costSensitiveClassifier0.getOnDemandDirectory();
      naiveBayesMultinomialText0.getStopwords();
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 50
  /*Coverage entropy=3.1262368167947456
  */
  @Test(timeout = 4000)
  public void test50()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      SystemInUtil.addInputLine("");
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      serializedClassifier0.getCurrentModel();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      iteratedLovinsStemmer0.stem("s;/#W$,v.$(C[#");
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      FileSystemHandling.appendStringToFile(evoSuiteFile0, " > ");
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      String string1 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string1);
      
      Integer integer0 = new Integer(2);
      String string2 = naiveBayesMultinomialText1.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string2);
      
      String string3 = naiveBayesMultinomialText1.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string3);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      String string4 = naiveBayesMultinomialText3.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string4);
      
      String string5 = naiveBayesMultinomialText3.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string5);
      
      naiveBayesMultinomialText1.useStopListTipText();
      naiveBayesMultinomialText2.getLowercaseTokens();
      String string6 = naiveBayesMultinomialText3.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string6);
      
      String string7 = naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string7);
      
      String string8 = naiveBayesMultinomialText2.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string8);
      
      System.setCurrentTimeMillis(1L);
      String string9 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string9);
      
      String string10 = naiveBayesMultinomialText0.normTipText();
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", string10);
      
      naiveBayesMultinomialText1.pruneDictionary();
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
  }

  /**
  //Test case number: 51
  /*Coverage entropy=2.419594359581629
  */
  @Test(timeout = 4000)
  public void test51()  throws Throwable  {
      FileSystemHandling.shouldAllThrowIOExceptions();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      testInstances0.setNumInstances(3460);
      naiveBayesMultinomialText1.stemmerTipText();
      Integer integer0 = new Integer(3460);
      naiveBayesMultinomialText1.normalizeDocLengthTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.useStopListTipText();
      double[] doubleArray0 = new double[9];
      doubleArray0[0] = (double) (-1);
      doubleArray0[1] = (double) 3460;
      doubleArray0[2] = (double) (-1);
      doubleArray0[3] = (double) 3460;
      doubleArray0[4] = (double) (-1);
      doubleArray0[5] = (double) 3460;
      doubleArray0[6] = (-1315.195635);
      doubleArray0[7] = (double) (-1);
      doubleArray0[8] = (double) (-1);
      SparseInstance sparseInstance0 = new SparseInstance(3460, doubleArray0);
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) sparseInstance0, true);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 52
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test52()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double double0 = naiveBayesMultinomialText0.getMinWordFrequency();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, double0, 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 53
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test53()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      Tokenizer tokenizer0 = naiveBayesMultinomialText0.getTokenizer();
      String[] stringArray0 = new String[4];
      stringArray0[0] = "KvZ^!";
      stringArray0[1] = "R72ee\u0000.1";
      stringArray0[2] = "F~Bj\"N/&,i";
      stringArray0[3] = "_NBv{Nog";
      Tokenizer.runTokenizer(tokenizer0, stringArray0);
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
  }

  /**
  //Test case number: 54
  /*Coverage entropy=3.0098030568249623
  */
  @Test(timeout = 4000)
  public void test54()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      testInstances0.setNumInstances(3460);
      int[] intArray0 = new int[9];
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      intArray0[1] = 3460;
      intArray0[3] = (-1);
      naiveBayesMultinomialText1.stemmerTipText();
      Integer integer0 = new Integer(3460);
      naiveBayesMultinomialText1.normalizeDocLengthTipText();
      naiveBayesMultinomialText1.useWordFrequenciesTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.useStopListTipText();
      naiveBayesMultinomialText1.tokenizerTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText3.setPeriodicPruning((-2));
      assertEquals((-2), naiveBayesMultinomialText3.getPeriodicPruning());
      
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 55
  /*Coverage entropy=2.5795799859834236
  */
  @Test(timeout = 4000)
  public void test55()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      findWithCapabilities0.getOptions();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      Vector<String> vector0 = findWithCapabilities1.find();
      FindWithCapabilities findWithCapabilities2 = new FindWithCapabilities();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.ONLY_MULTIINSTANCE;
      findWithCapabilities2.disable(capabilities_Capability0);
      findWithCapabilities0.find();
      Attribute attribute0 = new Attribute("measureCacheHits", vector0, (-1));
      FindWithCapabilities findWithCapabilities3 = new FindWithCapabilities();
      findWithCapabilities3.getOptions();
      FindWithCapabilities findWithCapabilities4 = new FindWithCapabilities();
      Capabilities.Capability capabilities_Capability1 = Capabilities.Capability.RELATIONAL_ATTRIBUTES;
      findWithCapabilities3.disableNot(capabilities_Capability1);
      findWithCapabilities3.find();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      SMOreg sMOreg0 = new SMOreg();
      SMOreg sMOreg1 = new SMOreg();
      Capabilities capabilities1 = sMOreg1.getCapabilities();
      capabilities0.or(capabilities1);
      capabilities1.getAttributeCapabilities();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      naiveBayesMultinomialText0.setOptions((String[]) null);
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("");
      naiveBayesMultinomialText0.setStemmer(snowballStemmer0);
      naiveBayesMultinomialText0.getStemmer();
      DenseInstance denseInstance0 = new DenseInstance(1);
      try { 
        naiveBayesMultinomialText0.updateClassifier(denseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 56
  /*Coverage entropy=2.437392092898178
  */
  @Test(timeout = 4000)
  public void test56()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      testInstances0.setNumInstancesRelational((-2));
      TestInstances testInstances1 = new TestInstances();
      Instances instances0 = testInstances1.generate();
      IBk iBk0 = new IBk();
      MockRandom mockRandom0 = new MockRandom();
      Instances instances1 = new Instances(instances0, (-2));
      Instances instances2 = new Instances(instances1, 1);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      naiveBayesMultinomialText0.debugTipText();
      int[] intArray0 = new int[0];
      testInstances1.setNumInstances(1141);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      Random.setNextRandom((-3330));
      naiveBayesMultinomialText1.useStopListTipText();
      naiveBayesMultinomialText1.stopwordsTipText();
      naiveBayesMultinomialText0.normTipText();
      try { 
        naiveBayesMultinomialText1.updateClassifier((Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 57
  /*Coverage entropy=3.269394275120508
  */
  @Test(timeout = 4000)
  public void test57()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_leplace = 1.0E-12;
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
  }

  /**
  //Test case number: 58
  /*Coverage entropy=2.9724529474573735
  */
  @Test(timeout = 4000)
  public void test58()  throws Throwable  {
      FileSystemHandling.shouldAllThrowIOExceptions();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      testInstances0.setNumInstances(3504);
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.setPermissions(evoSuiteFile0, true, true, true);
      double[] doubleArray0 = new double[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-2), doubleArray0);
      Instance instance0 = linearNNSearch0.nearestNeighbour(binarySparseInstance0);
      naiveBayesMultinomialText0.distributionForInstance(instance0);
      naiveBayesMultinomialText1.setNorm(15.0);
      naiveBayesMultinomialText0.stemmerTipText();
      Integer integer0 = new Integer(4771);
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText1.useWordFrequenciesTipText();
      naiveBayesMultinomialText1.useStopListTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.stopwordsTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText3.lowercaseTokensTipText();
      naiveBayesMultinomialText1.minWordFrequencyTipText();
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
  }

  /**
  //Test case number: 59
  /*Coverage entropy=3.269394275120508
  */
  @Test(timeout = 4000)
  public void test59()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.m_norm = 2.0;
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      SparseInstance sparseInstance0 = new SparseInstance(1070);
      byte[] byteArray0 = new byte[0];
      boolean boolean0 = FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      assertFalse(boolean0);
  }
}
