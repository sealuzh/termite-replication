/*
 * This file was automatically generated by EvoSuite
 * Mon Nov 18 20:56:41 GMT 2019
 */

package weka.classifiers.bayes;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.shaded.org.mockito.Mockito.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.File;
import java.net.URI;
import java.util.ArrayList;
import java.util.LinkedHashMap;
import java.util.LinkedList;
import java.util.Locale;
import java.util.Properties;
import java.util.function.Consumer;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.Random;
import org.evosuite.runtime.System;
import org.evosuite.runtime.ViolatedAssumptionAnswer;
import org.evosuite.runtime.mock.java.io.MockFile;
import org.evosuite.runtime.mock.java.net.MockURI;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;
import weka.attributeSelection.PrincipalComponents;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.bayes.NaiveBayesMultinomialText;
import weka.classifiers.functions.SGDText;
import weka.classifiers.functions.supportVector.PrecomputedKernelMatrixKernel;
import weka.classifiers.lazy.IBk;
import weka.classifiers.lazy.KStar;
import weka.classifiers.meta.CostSensitiveClassifier;
import weka.classifiers.misc.InputMappedClassifier;
import weka.core.AbstractInstance;
import weka.core.Attribute;
import weka.core.BinarySparseInstance;
import weka.core.Capabilities;
import weka.core.DenseInstance;
import weka.core.FindWithCapabilities;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.ManhattanDistance;
import weka.core.ProtectedProperties;
import weka.core.SparseInstance;
import weka.core.Stopwords;
import weka.core.TestInstances;
import weka.core.stemmers.IteratedLovinsStemmer;
import weka.core.stemmers.LovinsStemmer;
import weka.core.stemmers.SnowballStemmer;
import weka.core.stemmers.Stemmer;
import weka.core.tokenizers.NGramTokenizer;
import weka.core.tokenizers.Tokenizer;
import weka.core.tokenizers.WordTokenizer;
import weka.filters.AllFilter;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class NaiveBayesMultinomialText_ESTest extends NaiveBayesMultinomialText_ESTest_scaffolding {

  /**
  //Test case number: 0
  /*Coverage entropy=2.3987758862227255
  */
  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText0.globalInfo();
      naiveBayesMultinomialText0.setLNorm((-3.559375487000125));
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification");
      naiveBayesMultinomialText0.m_t = (-314.9517100224);
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification");
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier2 = new InputMappedClassifier();
      inputMappedClassifier1.getModelHeader(instances0);
      TestInstances testInstances1 = new TestInstances();
      testInstances1.getData();
      Instances instances1 = testInstances0.getData();
      naiveBayesMultinomialText0.buildClassifier(instances1);
      FileSystemHandling.createFolder((EvoSuiteFile) null);
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      Properties properties0 = new Properties();
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties1 = new ProtectedProperties(protectedProperties0);
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities2 = new FindWithCapabilities();
      findWithCapabilities0.getMatches();
      TestInstances testInstances2 = new TestInstances();
      FindWithCapabilities findWithCapabilities3 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities4 = new FindWithCapabilities();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.stopwordsTipText();
      boolean boolean0 = naiveBayesMultinomialText0.getUseStopList();
      assertEquals((-3.559375487000125), naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(boolean0);
  }

  /**
  //Test case number: 1
  /*Coverage entropy=2.271908876847366
  */
  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.normTipText();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      capabilities0.dependencies();
      Instances instances0 = testInstances0.generate("om");
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier2 = new InputMappedClassifier();
      inputMappedClassifier2.getModelHeader(instances0);
      TestInstances testInstances1 = new TestInstances();
      testInstances1.getData();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      FileSystemHandling.createFolder((EvoSuiteFile) null);
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities2 = new FindWithCapabilities();
      findWithCapabilities2.getMatches();
      TestInstances testInstances2 = new TestInstances();
      FindWithCapabilities findWithCapabilities3 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities4 = new FindWithCapabilities();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText0.LNormTipText();
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 2
  /*Coverage entropy=2.5206031304922814
  */
  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      String string0 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string0);
      
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      testInstances0.setNumInstancesRelational((-761));
      Instances instances0 = testInstances0.generate("20");
      Instances instances1 = new Instances(instances0);
      Instances.main(testInstances0.DEFAULT_WORDS);
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      naiveBayesMultinomialText0.setPeriodicPruning(14);
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      Instances instances2 = inputMappedClassifier1.getModelHeader(instances1);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      naiveBayesMultinomialText0.normTipText();
      PrincipalComponents principalComponents0 = new PrincipalComponents();
      testInstances0.getData();
      Instances instances3 = new Instances(instances2);
      naiveBayesMultinomialText0.buildClassifier(instances3);
      FileSystemHandling.createFolder(evoSuiteFile0);
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      Properties properties0 = new Properties();
      FileSystemHandling.appendLineToFile(evoSuiteFile0, "@relation");
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties1 = new ProtectedProperties(protectedProperties0);
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      String string1 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t1.0\nclass2\t1.0\nclass3\t1.0\nclass4\t1.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\n", string1);
      
      String string2 = naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string2);
      
      String string3 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string3);
      
      naiveBayesMultinomialText0.getCapabilities();
      String string4 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string4);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 3
  /*Coverage entropy=3.2160164202279433
  */
  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_norm = (-1.0);
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.setStopwords((File) null);
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.setStopwords((File) null);
      naiveBayesMultinomialText0.getNorm();
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      naiveBayesMultinomialText0.setTokenizer(wordTokenizer0);
      naiveBayesMultinomialText0.stemmerTipText();
      System.setCurrentTimeMillis(2091L);
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.LNormTipText();
      int[] intArray0 = new int[9];
      intArray0[0] = (-822);
      intArray0[1] = (-1);
      intArray0[2] = 449;
      intArray0[3] = 1668;
      intArray0[4] = 0;
      intArray0[5] = 1585;
      intArray0[6] = 2469;
      intArray0[7] = (-1709);
      intArray0[8] = 9;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1073.92464806, intArray0, 75);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance(binarySparseInstance0, true);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 4
  /*Coverage entropy=2.7204225901862604
  */
  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_periodicP = 7;
      FileSystemHandling.setPermissions((EvoSuiteFile) null, false, false, false);
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      naiveBayesMultinomialText0.getPeriodicPruning();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      String[] stringArray0 = new String[6];
      stringArray0[0] = "If true then document length is normalized according to the settings for norm and lnorm";
      stringArray0[1] = "If true then document length is normalized according to the settings for norm and lnorm";
      stringArray0[2] = "5G";
      stringArray0[3] = "Whether to convert all tokens to lowercase";
      stringArray0[4] = "Whether to convert all tokens to lowercase";
      stringArray0[5] = "If true then document length is normalized according to the settings for norm and lnorm";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(7, naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 5
  /*Coverage entropy=3.1431901876608284
  */
  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      String string0 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string0);
      
      naiveBayesMultinomialText0.normTipText();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("");
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      Instances.main(testInstances0.DEFAULT_WORDS);
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier2 = new InputMappedClassifier();
      Instances instances1 = inputMappedClassifier0.getModelHeader(instances0);
      naiveBayesMultinomialText0.buildClassifier(instances1);
      String string1 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string1);
      
      PrincipalComponents principalComponents0 = new PrincipalComponents();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.setOptions(testInstances0.DEFAULT_WORDS);
      naiveBayesMultinomialText1.getRevision();
      int[] intArray0 = new int[2];
      intArray0[0] = (-1);
      intArray0[1] = (-2553);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0.0, intArray0, (-1));
      naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      String string2 = naiveBayesMultinomialText1.useWordFrequenciesTipText();
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", string2);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      
      String string3 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", string3);
  }

  /**
  //Test case number: 6
  /*Coverage entropy=2.4175929936686043
  */
  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.normTipText();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("If true, ignores all words that are on the stoplist.");
      Instances instances1 = new Instances(instances0);
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier2 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier3 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier4 = new InputMappedClassifier();
      inputMappedClassifier4.getModelHeader(instances1);
      TestInstances testInstances1 = new TestInstances();
      testInstances0.getData();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      FileSystemHandling.createFolder(evoSuiteFile0);
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      Properties properties0 = new Properties();
      properties0.keySet();
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties1 = new ProtectedProperties(protectedProperties0);
      int[] intArray0 = new int[5];
      intArray0[0] = (-2);
      intArray0[1] = (-2);
      intArray0[2] = (-1);
      intArray0[3] = (-2);
      intArray0[4] = (-1);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-0.4375), intArray0, (-1431655765));
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertArrayEquals(new double[] {0.3750000000000001, 0.16666666666666669, 0.29166666666666663, 0.16666666666666669}, doubleArray0, 0.01);
      
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      String string0 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string0);
      
      String string1 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", string1);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 7
  /*Coverage entropy=2.502808529414704
  */
  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      naiveBayesMultinomialText1.setUseStopList(true);
      naiveBayesMultinomialText0.stopwordsTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.m_lowercaseTokens = true;
      naiveBayesMultinomialText2.setLNorm((-1020.67028));
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "The file containing the stopwords (if this is a directory then the default ones are used).");
      Stopwords stopwords0 = new Stopwords();
      naiveBayesMultinomialText1.m_stopwords = stopwords0;
      Random.setNextRandom((-3897));
      Locale.getISOCountries();
      try { 
        naiveBayesMultinomialText1.distributionForInstance((Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 8
  /*Coverage entropy=2.1280269765913093
  */
  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText0.globalInfo();
      naiveBayesMultinomialText0.setLNorm((-3.559375487000125));
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification");
      naiveBayesMultinomialText0.m_t = (-314.9517100224);
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification");
      Instances instances1 = new Instances(instances0);
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      Consumer<Object> consumer0 = (Consumer<Object>) mock(Consumer.class, new ViolatedAssumptionAnswer());
      instances0.forEach(consumer0);
      InputMappedClassifier inputMappedClassifier2 = new InputMappedClassifier();
      inputMappedClassifier1.getModelHeader(instances1);
      TestInstances testInstances1 = new TestInstances();
      testInstances1.getData();
      Instances instances2 = testInstances0.getData();
      naiveBayesMultinomialText0.buildClassifier(instances2);
      FileSystemHandling.createFolder((EvoSuiteFile) null);
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      Properties properties0 = new Properties();
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties1 = new ProtectedProperties(protectedProperties0);
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities2 = new FindWithCapabilities();
      findWithCapabilities0.getMatches();
      TestInstances testInstances2 = new TestInstances();
      FindWithCapabilities findWithCapabilities3 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities4 = new FindWithCapabilities();
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
  }

  /**
  //Test case number: 9
  /*Coverage entropy=2.41257681572198
  */
  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setNorm(0.0);
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.setLNorm((-3.559375487000125));
      naiveBayesMultinomialText0.m_t = (-314.9517100224);
      Random.setNextRandom((-2120));
      Locale.getISOCountries();
      TestInstances testInstances0 = new TestInstances();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.getCapabilities();
      assertEquals((-3.559375487000125), naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 10
  /*Coverage entropy=2.40367970452956
  */
  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "");
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.normTipText();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("If true, ignores all words that are on the stoplist.");
      Instances instances1 = new Instances(instances0);
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier2 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier3 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier4 = new InputMappedClassifier();
      inputMappedClassifier4.getModelHeader(instances1);
      TestInstances testInstances1 = new TestInstances();
      testInstances0.getData();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      FileSystemHandling.createFolder(evoSuiteFile0);
      testInstances0.generate("@data");
      testInstances1.getData();
      naiveBayesMultinomialText0.buildClassifier(instances1);
      FileSystemHandling.createFolder((EvoSuiteFile) null);
      String string0 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string0);
      
      Properties properties0 = new Properties();
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, ".bsi");
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties1 = new ProtectedProperties(properties0);
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      String string1 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t9.0\nclass2\t4.0\nclass3\t7.0\nclass4\t4.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\nover\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nthe\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nThe\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nquick\t7.38905609893065\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nlazy\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\njumps\t20.085536923187668\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nbrown\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\ndog\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nfox\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\n", string1);
      
      String string2 = naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string2);
      
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.getCapabilities();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      String string3 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", string3);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 11
  /*Coverage entropy=2.5251257075920477
  */
  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      String string0 = naiveBayesMultinomialText0.normTipText();
      assertEquals("The norm of the instances after normalization.", string0);
      
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("If true, ignores all words that are on the stoplist.");
      testInstances0.generate(".bsi");
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier2 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier3 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier4 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier5 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier6 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier7 = new InputMappedClassifier();
      Instances instances1 = inputMappedClassifier7.getModelHeader(instances0);
      TestInstances testInstances1 = new TestInstances();
      testInstances1.setNumString((-1));
      testInstances0.generate("@data");
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      testInstances1.getData();
      naiveBayesMultinomialText0.buildClassifier(instances1);
      String string1 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string1);
      
      Properties properties0 = new Properties();
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties1 = new ProtectedProperties(properties0);
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      String string2 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t1.0\nclass2\t1.0\nclass3\t1.0\nclass4\t1.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\n", string2);
      
      String string3 = naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string3);
      
      String string4 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string4);
      
      naiveBayesMultinomialText0.getCapabilities();
      String string5 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string5);
      
      String string6 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", string6);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 12
  /*Coverage entropy=2.405123999770017
  */
  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.normTipText();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.normTipText();
      FileSystemHandling fileSystemHandling1 = new FileSystemHandling();
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText1.stopwordsTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.setLNorm(4.0);
      FileSystemHandling.appendLineToFile(evoSuiteFile0, ",SZKR");
      Random.setNextRandom((-3564));
      byte[] byteArray0 = new byte[3];
      byteArray0[0] = (byte)2;
      byteArray0[1] = (byte)16;
      byteArray0[2] = (byte) (-63);
      FileSystemHandling.appendDataToFile(evoSuiteFile0, byteArray0);
      FileSystemHandling.setPermissions(evoSuiteFile0, false, false, false);
      int[] intArray0 = new int[1];
      intArray0[0] = (-3564);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(4.0, intArray0, 868);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 13
  /*Coverage entropy=1.6868977693384444
  */
  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FileSystemHandling.appendLineToFile(evoSuiteFile0, "weka.classifiers.bayes.NaiveBayesMultinomialText");
      Random.setNextRandom(1499);
      FileSystemHandling.setPermissions(evoSuiteFile0, false, true, false);
      Locale.getISOCountries();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance((Instance) null, true);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 14
  /*Coverage entropy=3.3142487148788504
  */
  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      naiveBayesMultinomialText0.setUseStopList(true);
      Attribute attribute0 = new Attribute("", 11);
      attribute0.indexOfValue("p>k&V2UTL&cB|M[tJ])");
      attribute0.setWeight(11);
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      attribute0.equalsMsg((Object) null);
      naiveBayesMultinomialText0.setOptions(stringArray0);
      Random.setNextRandom(163);
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) null, true);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 15
  /*Coverage entropy=2.063983366979555
  */
  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      naiveBayesMultinomialText0.listOptions();
      String[] stringArray0 = TestInstances.DEFAULT_WORDS;
      naiveBayesMultinomialText0.getLowercaseTokens();
      naiveBayesMultinomialText0.getPeriodicPruning();
      String[] stringArray1 = new String[2];
      try { 
        AbstractClassifier.forName(">{Yvh}rz<$@", stringArray1);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Can't find class called: >{Yvh}rz<$@
         //
         verifyException("weka.core.Utils", e);
      }
  }

  /**
  //Test case number: 16
  /*Coverage entropy=3.3438881085370094
  */
  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte)12;
      byteArray0[1] = (byte)24;
      byteArray0[2] = (byte)88;
      byteArray0[3] = (byte)24;
      String[] stringArray0 = new String[6];
      stringArray0[0] = "Whether to convert all tokens to lowercase";
      stringArray0[1] = "Whether to convert all tokens to lowercase";
      stringArray0[2] = "Whether to convert all tokens to lowercase";
      stringArray0[3] = "Whether to convert all tokens to lowercase";
      stringArray0[4] = "Whether to convert all tokens to lowercase";
      stringArray0[5] = "Whether to convert all tokens to lowercase";
      naiveBayesMultinomialText1.setOptions(stringArray0);
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("");
      SnowballStemmer snowballStemmer1 = new SnowballStemmer();
      snowballStemmer0.stem("Whether to convert all tokens to lowercase");
      naiveBayesMultinomialText1.getOptions();
      SnowballStemmer snowballStemmer2 = new SnowballStemmer();
      snowballStemmer2.toString();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("Whether to convert all tokens to lowercase", arrayList0, 60);
      naiveBayesMultinomialText1.setOptions(stringArray0);
      LinkedList<String> linkedList0 = new LinkedList<String>();
      Attribute attribute0 = new Attribute("P", linkedList0, (byte)12);
      arrayList0.add(attribute0);
      System.setCurrentTimeMillis((-114L));
      Random.setNextRandom((byte)24);
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // weka.classifiers.bayes.NaiveBayesMultinomialText: Class attribute not set!
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 17
  /*Coverage entropy=2.253297930766516
  */
  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_lnorm = Double.NEGATIVE_INFINITY;
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.getLNorm();
      naiveBayesMultinomialText0.m_normalize = false;
      naiveBayesMultinomialText0.globalInfo();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      naiveBayesMultinomialText0.getStemmer();
      int int0 = (-545);
      int int1 = 74;
      int[] intArray0 = new int[6];
      intArray0[0] = 74;
      intArray0[1] = (-545);
      intArray0[2] = (-545);
      intArray0[3] = 74;
      intArray0[4] = (-545);
      intArray0[5] = 74;
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value");
      // Undeclared exception!
      try { 
        Instances.mergeInstances(instances0, instances0);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // Attribute names are not unique! Causes: 'Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this valueNominal1' 'Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this valueClass' 
         //
         verifyException("weka.core.Instances", e);
      }
  }

  /**
  //Test case number: 18
  /*Coverage entropy=3.023537389978894
  */
  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("No value given for -");
      naiveBayesMultinomialText1.m_wordFrequencies = true;
      SparseInstance sparseInstance0 = new SparseInstance(449);
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities2 = new FindWithCapabilities();
      findWithCapabilities0.getMatches();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate("@~k9tu-");
      Instances instances1 = new Instances(instances0, 46);
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.buildClassifier(instances0);
      naiveBayesMultinomialText1.getPeriodicPruning();
      naiveBayesMultinomialText2.setOptions(testInstances0.DEFAULT_WORDS);
      naiveBayesMultinomialText1.getUseWordFrequencies();
      naiveBayesMultinomialText1.lowercaseTokensTipText();
      assertTrue(naiveBayesMultinomialText1.getUseWordFrequencies());
  }

  /**
  //Test case number: 19
  /*Coverage entropy=1.7766234846545668
  */
  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.reset();
      naiveBayesMultinomialText0.setPeriodicPruning(4);
      naiveBayesMultinomialText0.pruneDictionary();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.MISSING_CLASS_VALUES;
      findWithCapabilities0.disableNot(capabilities_Capability0);
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      ProtectedProperties protectedProperties0 = null;
      try {
        protectedProperties0 = new ProtectedProperties((Properties) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.ProtectedProperties", e);
      }
  }

  /**
  //Test case number: 20
  /*Coverage entropy=1.4682921994113465
  */
  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_t = (-3607.559375487);
      naiveBayesMultinomialText0.setPeriodicPruning(4);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.pruneDictionary();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 21
  /*Coverage entropy=1.4682921994113465
  */
  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_t = (-3607.55937549);
      naiveBayesMultinomialText0.setPeriodicPruning(4);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.pruneDictionary();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 22
  /*Coverage entropy=2.2584261358947213
  */
  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.normTipText();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("weka/core/Capabilities.props");
      Instances instances1 = new Instances(instances0);
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier2 = new InputMappedClassifier();
      double[] doubleArray0 = new double[0];
      DenseInstance denseInstance0 = new DenseInstance(1272.469894, doubleArray0);
      instances0.add((Instance) denseInstance0);
      InputMappedClassifier inputMappedClassifier3 = new InputMappedClassifier();
      inputMappedClassifier3.getModelHeader(instances1);
      TestInstances testInstances1 = new TestInstances();
      testInstances1.getData();
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances0);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 23
  /*Coverage entropy=3.120459247679771
  */
  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      System.setCurrentTimeMillis(996L);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      MockFile mockFile0 = (MockFile)naiveBayesMultinomialText0.m_stopwordsFile;
      naiveBayesMultinomialText0.m_stopwordsFile = (File) mockFile0;
      naiveBayesMultinomialText0.m_t = (double) 996L;
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.pruneDictionary();
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      naiveBayesMultinomialText0.getStemmer();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.getCapabilities();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.getStemmer();
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.getStemmer();
      naiveBayesMultinomialText0.LNormTipText();
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
  }

  /**
  //Test case number: 24
  /*Coverage entropy=3.2825458008391193
  */
  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, (String) null);
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "'~3XD@h");
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setLNorm(0.1);
      AbstractClassifier.makeCopy(naiveBayesMultinomialText0);
      FileSystemHandling.setPermissions((EvoSuiteFile) null, false, false, false);
      naiveBayesMultinomialText0.listOptions();
      naiveBayesMultinomialText0.pruneDictionary();
      naiveBayesMultinomialText0.getPeriodicPruning();
      naiveBayesMultinomialText0.m_leplace = (double) 0;
      File file0 = naiveBayesMultinomialText0.getStopwords();
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.LNormTipText();
      FileSystemHandling fileSystemHandling1 = new FileSystemHandling();
      naiveBayesMultinomialText0.setStopwords(file0);
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.setStopwords(file0);
      naiveBayesMultinomialText0.getOptions();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      naiveBayesMultinomialText0.toString();
      assertEquals(0.1, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 25
  /*Coverage entropy=3.247577057709727
  */
  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setNormalizeDocLength(false);
      naiveBayesMultinomialText0.m_periodicP = 0;
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      String[] stringArray0 = new String[9];
      stringArray0[0] = "D'omLmWe";
      stringArray0[1] = "=p|%>!~\"c`MN(";
      naiveBayesMultinomialText0.m_lnorm = (double) 0;
      naiveBayesMultinomialText0.m_useStopList = true;
      stringArray0[2] = "j";
      stringArray0[3] = "q*h";
      AbstractClassifier.makeCopies(naiveBayesMultinomialText0, 0);
      stringArray0[4] = "";
      stringArray0[5] = "";
      stringArray0[6] = "-norm";
      stringArray0[7] = "ingly";
      stringArray0[8] = "YPks.Oj*yb2[ZMf>";
      naiveBayesMultinomialText0.setOptions((String[]) null);
      naiveBayesMultinomialText0.getUseStopList();
      naiveBayesMultinomialText0.getNormalizeDocLength();
      naiveBayesMultinomialText0.getLNorm();
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.setNorm(0.0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      Tokenizer tokenizer0 = naiveBayesMultinomialText1.getTokenizer();
      naiveBayesMultinomialText0.setTokenizer(tokenizer0);
      assertEquals(0.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 26
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double[] doubleArray0 = new double[3];
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte)38;
      byteArray0[1] = (byte)24;
      byteArray0[2] = (byte)88;
      byteArray0[4] = (byte)82;
      naiveBayesMultinomialText0.m_probOfClass = doubleArray0;
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.toString();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 27
  /*Coverage entropy=2.070976373972562
  */
  @Test(timeout = 4000)
  public void test27()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      String string0 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string0);
      
      String string1 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string1);
      
      String string2 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string2);
      
      naiveBayesMultinomialText0.setStemmer((Stemmer) null);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 28
  /*Coverage entropy=1.5271837172395382
  */
  @Test(timeout = 4000)
  public void test28()  throws Throwable  {
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("DHvo");
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setPeriodicPruning((-2507));
      naiveBayesMultinomialText0.pruneDictionary();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.EMPTY_NOMINAL_ATTRIBUTES;
      findWithCapabilities0.disableNot(capabilities_Capability0);
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      Properties properties0 = new Properties();
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      Properties properties1 = new Properties();
      Properties properties2 = new Properties();
      Properties properties3 = new Properties();
      ProtectedProperties protectedProperties1 = new ProtectedProperties(protectedProperties0);
      Properties properties4 = new Properties();
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "");
      ProtectedProperties protectedProperties2 = new ProtectedProperties(properties0);
      ProtectedProperties protectedProperties3 = new ProtectedProperties(protectedProperties1);
      ProtectedProperties protectedProperties4 = new ProtectedProperties(protectedProperties3);
      FindWithCapabilities findWithCapabilities2 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities3 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities4 = new FindWithCapabilities();
      findWithCapabilities3.getMatches();
      Capabilities.Capability capabilities_Capability1 = Capabilities.Capability.STRING_ATTRIBUTES;
      findWithCapabilities4.enableNot(capabilities_Capability1);
      TestInstances testInstances0 = new TestInstances();
      FindWithCapabilities findWithCapabilities5 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities6 = new FindWithCapabilities();
      assertFalse(findWithCapabilities6.equals((Object)findWithCapabilities1));
  }

  /**
  //Test case number: 29
  /*Coverage entropy=3.2560814664349573
  */
  @Test(timeout = 4000)
  public void test29()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      Attribute attribute0 = new Attribute("weights.length != numInstances.", 11);
      attribute0.indexOfValue("numeric");
      String[] stringArray0 = new String[8];
      stringArray0[0] = "relational";
      stringArray0[1] = "relational";
      stringArray0[2] = "relational";
      stringArray0[3] = "relational";
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      attribute0.equalsMsg(arrayList0);
      naiveBayesMultinomialText0.setOptions(stringArray1);
      Random.setNextRandom(17);
      File file0 = MockFile.createTempFile("org.tartarus.snowball.ext", "@end");
      naiveBayesMultinomialText0.setStopwords(file0);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
  }

  /**
  //Test case number: 30
  /*Coverage entropy=2.751120856085606
  */
  @Test(timeout = 4000)
  public void test30()  throws Throwable  {
      FileSystemHandling.shouldAllThrowIOExceptions();
      Attribute attribute0 = new Attribute("weights.length != numInstances.", 11);
      KStar kStar0 = new KStar();
      kStar0.listOptions();
      attribute0.indexOfValue("weights.length != numInstances.");
      attribute0.enumerateValues();
      String[] stringArray0 = new String[1];
      stringArray0[0] = "@attribute";
      AbstractClassifier.runClassifier(kStar0, stringArray0);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setPeriodicPruning(0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.m_stopwordsFile = null;
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText1.getOptions();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 31
  /*Coverage entropy=0.9004024235381879
  */
  @Test(timeout = 4000)
  public void test31()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("", arrayList0, 2380);
      double[] doubleArray0 = new double[3];
      doubleArray0[0] = (double) 2380;
      doubleArray0[1] = (double) 2380;
      doubleArray0[2] = (double) 2380;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-306.3071765959158), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance(449);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance(binarySparseInstance1, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 32
  /*Coverage entropy=2.157517450062315
  */
  @Test(timeout = 4000)
  public void test32()  throws Throwable  {
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("DHvo");
      SparseInstance sparseInstance0 = new SparseInstance(449);
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      IBk iBk0 = new IBk();
      findWithCapabilities0.setHandler(iBk0);
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      findWithCapabilities0.setFilename("DHvo");
      snowballStemmer0.stem("DHvo");
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = 0.0;
      int[] intArray0 = new int[8];
      intArray0[0] = 449;
      intArray0[1] = 449;
      intArray0[2] = 449;
      intArray0[3] = 449;
      intArray0[4] = 449;
      intArray0[5] = 449;
      intArray0[6] = 449;
      intArray0[7] = 449;
      SparseInstance sparseInstance1 = new SparseInstance(0.0, doubleArray0, intArray0, 185);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(sparseInstance1);
      TestInstances testInstances0 = new TestInstances();
      testInstances0.setNumRelationalDate(449);
      Instances instances0 = testInstances0.generate("org.tartarus.snowball");
      Instances instances1 = new Instances(instances0);
      LinkedHashMap<LovinsStemmer, Integer> linkedHashMap0 = new LinkedHashMap<LovinsStemmer, Integer>();
      instances1.add((Instance) sparseInstance0);
      instances0.setRelationName("");
      testInstances0.getData();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.buildClassifier(instances1);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 33
  /*Coverage entropy=2.157517450062315
  */
  @Test(timeout = 4000)
  public void test33()  throws Throwable  {
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("DHvo");
      SparseInstance sparseInstance0 = new SparseInstance(449);
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      IBk iBk0 = new IBk();
      findWithCapabilities0.setHandler(iBk0);
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      findWithCapabilities0.setFilename("DHvo");
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = 0.0;
      int[] intArray0 = new int[8];
      intArray0[0] = 449;
      intArray0[1] = 449;
      intArray0[2] = 449;
      intArray0[3] = 449;
      intArray0[4] = 449;
      intArray0[5] = 449;
      intArray0[6] = 449;
      intArray0[7] = 449;
      SparseInstance sparseInstance1 = new SparseInstance(0.0, doubleArray0, intArray0, 185);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(sparseInstance1);
      TestInstances testInstances0 = new TestInstances();
      testInstances0.setNumRelationalDate(449);
      Instances instances0 = testInstances0.generate("org.tartarus.snowball");
      Instances instances1 = new Instances(instances0);
      LinkedHashMap<LovinsStemmer, Integer> linkedHashMap0 = new LinkedHashMap<LovinsStemmer, Integer>();
      instances1.add((Instance) sparseInstance0);
      instances0.setRelationName("");
      testInstances0.getData();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      naiveBayesMultinomialText0.buildClassifier(instances1);
      Random.setNextRandom((-1424));
  }

  /**
  //Test case number: 34
  /*Coverage entropy=2.5531257522061517
  */
  @Test(timeout = 4000)
  public void test34()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      byte[] byteArray0 = new byte[4];
      byteArray0[0] = (byte) (-73);
      byteArray0[1] = (byte) (-49);
      URI uRI0 = MockURI.aFileURI;
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/tmp/foo.bar");
      FileSystemHandling.appendLineToFile(evoSuiteFile0, "gf/5v*&~)bpj7$H!]");
      MockFile mockFile0 = new MockFile(uRI0);
      mockFile0.setWritable(true);
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.normTipText();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("The norm of the instances after normalization.");
      SnowballStemmer snowballStemmer1 = new SnowballStemmer();
      snowballStemmer1.stem("The norm of the instances after normalization.");
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setStemmer(snowballStemmer1);
      naiveBayesMultinomialText0.LNormTipText();
      AllFilter allFilter0 = new AllFilter();
      Capabilities capabilities0 = allFilter0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      Capabilities.forInstances(instances0, true);
      Instances instances1 = testInstances0.generate("Ik0eEn!r");
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances1);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // weka.classifiers.bayes.NaiveBayesMultinomialText: Cannot handle relational attributes!
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 35
  /*Coverage entropy=3.2469270035385462
  */
  @Test(timeout = 4000)
  public void test35()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[1];
      stringArray0[0] = "The independent probability of a class\n";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.setPermissions(evoSuiteFile0, false, false, false);
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      String[] stringArray1 = new String[3];
      stringArray1[0] = "The independent probability of a class\n";
      stringArray1[1] = "The independent probability of a class\n";
      stringArray1[2] = "-lnorm";
      AbstractClassifier.runClassifier(naiveBayesMultinomialText1, stringArray1);
      naiveBayesMultinomialText1.getOptions();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      snowballStemmer0.toString();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      try { 
        naiveBayesMultinomialText1.setOptions(stringArray1);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // No value given for -lnorm option.
         //
         verifyException("weka.core.Utils", e);
      }
  }

  /**
  //Test case number: 36
  /*Coverage entropy=2.180061011878331
  */
  @Test(timeout = 4000)
  public void test36()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double[] doubleArray0 = new double[3];
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte)38;
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      findWithCapabilities0.setFilename("");
      findWithCapabilities0.getMatches();
      int[] intArray0 = new int[8];
      intArray0[0] = (int) (byte)38;
      intArray0[1] = (int) (byte)24;
      intArray0[2] = (int) (byte)38;
      intArray0[3] = (int) (byte)38;
      intArray0[4] = 185;
      intArray0[5] = 1;
      intArray0[6] = 185;
      intArray0[7] = 185;
      SparseInstance sparseInstance0 = new SparseInstance((byte)38, doubleArray0, intArray0, (byte)38);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(sparseInstance0);
      TestInstances testInstances0 = new TestInstances();
      testInstances0.setNumRelationalDate((byte)38);
      TestInstances testInstances1 = new TestInstances();
      Instances instances0 = testInstances1.generate((String) null);
      Instances instances1 = new Instances(instances0);
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      Instances instances2 = inputMappedClassifier0.getModelHeader(instances1);
      instances1.add((Instance) sparseInstance0);
      instances2.add((Instance) binarySparseInstance0);
      testInstances0.getData();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      try { 
        naiveBayesMultinomialText1.buildClassifier(instances2);
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
         //
         // Index: 2, Size: 2
         //
         verifyException("java.util.ArrayList", e);
      }
  }

  /**
  //Test case number: 37
  /*Coverage entropy=1.4682921994113465
  */
  @Test(timeout = 4000)
  public void test37()  throws Throwable  {
      int int0 = 4;
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setPeriodicPruning(4);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.pruneDictionary();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 38
  /*Coverage entropy=3.054745275828884
  */
  @Test(timeout = 4000)
  public void test38()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[1];
      MockFile mockFile0 = new MockFile("JO?~9!0<2.z", "JO?~9!0<2.z");
      mockFile0.setWritable(false);
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      wordTokenizer0.listOptions();
      naiveBayesMultinomialText0.setTokenizer(wordTokenizer0);
      AbstractClassifier.runClassifier(naiveBayesMultinomialText0, stringArray0);
      naiveBayesMultinomialText0.setPeriodicPruning((-86));
      naiveBayesMultinomialText0.getOptions();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      double[] doubleArray0 = new double[3];
      doubleArray0[0] = (double) 3234;
      doubleArray0[1] = (double) 3234;
      doubleArray0[2] = 0.0;
      naiveBayesMultinomialText0.getStopwords();
      naiveBayesMultinomialText0.m_probOfClass = doubleArray0;
      SnowballStemmer snowballStemmer1 = new SnowballStemmer();
      snowballStemmer1.toString();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("", arrayList0, 3234);
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 39
  /*Coverage entropy=3.3576200078723977
  */
  @Test(timeout = 4000)
  public void test39()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      Attribute attribute0 = new Attribute("weights.length != numInstances.", 11);
      KStar kStar0 = new KStar();
      kStar0.listOptions();
      attribute0.indexOfValue((String) null);
      String[] stringArray0 = new String[8];
      stringArray0[0] = "relational";
      stringArray0[1] = "relational";
      stringArray0[2] = "relational";
      stringArray0[3] = null;
      stringArray0[4] = "numeric";
      stringArray0[5] = "integer";
      stringArray0[6] = "numeric";
      stringArray0[7] = "date";
      AbstractClassifier.runClassifier(kStar0, stringArray0);
      naiveBayesMultinomialText0.setPeriodicPruning((-1));
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      snowballStemmer0.toString();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      LinkedList<ManhattanDistance> linkedList0 = new LinkedList<ManhattanDistance>();
      arrayList0.containsAll(linkedList0);
      Instances instances0 = new Instances("YJg8", arrayList0, 0);
      naiveBayesMultinomialText0.setOptions(stringArray1);
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // weka.classifiers.bayes.NaiveBayesMultinomialText: No attributes!
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 40
  /*Coverage entropy=2.2584261358947213
  */
  @Test(timeout = 4000)
  public void test40()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_periodicP = (-1278);
      naiveBayesMultinomialText0.m_t = (double) (-1278);
      naiveBayesMultinomialText0.reset();
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.getPeriodicPruning();
      String[] stringArray0 = new String[5];
      stringArray0[0] = "9122";
      stringArray0[1] = "9122";
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      naiveBayesMultinomialText0.setTokenizer(nGramTokenizer0);
      naiveBayesMultinomialText0.m_periodicP = (-3027);
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 41
  /*Coverage entropy=2.015255863586861
  */
  @Test(timeout = 4000)
  public void test41()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.pruneDictionary();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.EMPTY_NOMINAL_ATTRIBUTES;
      String[] stringArray0 = new String[6];
      stringArray0[0] = "";
      stringArray0[1] = "noone";
      stringArray0[2] = "";
      stringArray0[3] = "-P";
      stringArray0[4] = "%8]`Mcl!";
      stringArray0[5] = "";
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: NumberFormatException");
      
      } catch(NumberFormatException e) {
         //
         // For input string: \"%8]`Mcl!\"
         //
         verifyException("java.lang.NumberFormatException", e);
      }
  }

  /**
  //Test case number: 42
  /*Coverage entropy=3.194996186179526
  */
  @Test(timeout = 4000)
  public void test42()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      MockFile mockFile0 = (MockFile)naiveBayesMultinomialText0.m_stopwordsFile;
      String[] stringArray0 = new String[3];
      stringArray0[0] = "";
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      
      stringArray0[1] = "3o)t$)vWPZ,V~w";
      stringArray0[2] = "";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.createFolder(evoSuiteFile0);
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      String[] stringArray1 = naiveBayesMultinomialText1.getOptions();
      assertEquals(12, stringArray1.length);
  }

  /**
  //Test case number: 43
  /*Coverage entropy=2.11030440814219
  */
  @Test(timeout = 4000)
  public void test43()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("weights.length != numInstances.");
      SparseInstance sparseInstance0 = new SparseInstance(3);
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      findWithCapabilities1.getMatches();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(sparseInstance0);
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate("org.tartarus.snowball");
      Instances instances1 = new Instances(instances0);
      instances1.add((Instance) binarySparseInstance0);
      Instances instances2 = testInstances0.getData();
      naiveBayesMultinomialText0.buildClassifier(instances2);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 44
  /*Coverage entropy=3.247577057709727
  */
  @Test(timeout = 4000)
  public void test44()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setNormalizeDocLength(false);
      naiveBayesMultinomialText0.m_periodicP = 0;
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      String[] stringArray0 = new String[9];
      stringArray0[0] = "D'omLmWe";
      stringArray0[1] = "=p|%>!~\"c`MN(";
      naiveBayesMultinomialText0.m_useStopList = true;
      stringArray0[2] = "j";
      stringArray0[3] = "q*h";
      AbstractClassifier.makeCopies(naiveBayesMultinomialText0, 0);
      stringArray0[4] = "";
      stringArray0[5] = "";
      stringArray0[6] = "-norm";
      stringArray0[7] = "ingly";
      stringArray0[8] = "YPks.Oj*yb2[ZMf>";
      naiveBayesMultinomialText0.setOptions((String[]) null);
      naiveBayesMultinomialText0.getUseStopList();
      naiveBayesMultinomialText0.getNormalizeDocLength();
      naiveBayesMultinomialText0.getLNorm();
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.setNorm(2.0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      Tokenizer tokenizer0 = naiveBayesMultinomialText1.getTokenizer();
      tokenizer0.setOptions(stringArray0);
      naiveBayesMultinomialText0.setTokenizer(tokenizer0);
      assertEquals(2.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 45
  /*Coverage entropy=3.2698986887396755
  */
  @Test(timeout = 4000)
  public void test45()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      Attribute attribute0 = new Attribute("weights.length != numInstances.", 11);
      KStar kStar0 = new KStar();
      kStar0.listOptions();
      attribute0.indexOfValue((String) null);
      String[] stringArray0 = new String[8];
      stringArray0[0] = "relational";
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      stringArray0[1] = "relational";
      stringArray0[2] = "relational";
      stringArray0[3] = null;
      stringArray0[4] = "numeric";
      stringArray0[5] = "integer";
      stringArray0[6] = "numeric";
      stringArray0[7] = "date";
      AbstractClassifier.runClassifier(kStar0, stringArray0);
      naiveBayesMultinomialText0.setPeriodicPruning((-1));
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("YJg8", arrayList0, 0);
      naiveBayesMultinomialText0.setOptions(stringArray1);
      Random.setNextRandom(3);
  }

  /**
  //Test case number: 46
  /*Coverage entropy=3.2722146523459616
  */
  @Test(timeout = 4000)
  public void test46()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[4];
      naiveBayesMultinomialText0.m_norm = (-2.559375487000125);
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.m_minWordP = 2231.752086978097;
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      naiveBayesMultinomialText0.setOptions(stringArray1);
      naiveBayesMultinomialText0.getOptions();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 47
  /*Coverage entropy=2.063983366979555
  */
  @Test(timeout = 4000)
  public void test47()  throws Throwable  {
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setNorm((-1266.030977399));
      naiveBayesMultinomialText0.setLNorm((-1020.67028));
      Random.setNextRandom((-331));
      Locale.getISOCountries();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals((-1020.67028), naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 48
  /*Coverage entropy=1.4682921994113465
  */
  @Test(timeout = 4000)
  public void test48()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      byte[] byteArray0 = new byte[4];
      byteArray0[0] = (byte) (-73);
      byteArray0[1] = (byte) (-49);
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      URI uRI0 = MockURI.aFileURI;
      MockFile mockFile0 = new MockFile(uRI0);
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      SGDText sGDText0 = new SGDText();
      Tokenizer tokenizer0 = sGDText0.getTokenizer();
      naiveBayesMultinomialText0.setTokenizer(tokenizer0);
      int[] intArray0 = new int[9];
      intArray0[0] = 0;
      intArray0[1] = 38;
      intArray0[2] = (int) (byte) (-49);
      intArray0[3] = 72;
      intArray0[4] = (int) (byte) (-49);
      intArray0[5] = 0;
      intArray0[6] = 12;
      intArray0[7] = 0;
      intArray0[8] = (int) (byte) (-49);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(22.0, intArray0, (-2029));
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
  }

  /**
  //Test case number: 49
  /*Coverage entropy=0.9623351446188083
  */
  @Test(timeout = 4000)
  public void test49()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      File file0 = costSensitiveClassifier0.getOnDemandDirectory();
      naiveBayesMultinomialText0.setStopwords(file0);
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.STRING_CLASS;
      findWithCapabilities0.disableNot(capabilities_Capability0);
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      Properties properties0 = new Properties();
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      Properties properties1 = new Properties();
      Properties properties2 = new Properties();
      Properties properties3 = new Properties();
      ProtectedProperties protectedProperties1 = new ProtectedProperties(properties3);
      Properties properties4 = new Properties();
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "Ugw/V(0");
      Properties properties5 = new Properties();
      ProtectedProperties protectedProperties2 = new ProtectedProperties(properties2);
      ProtectedProperties protectedProperties3 = new ProtectedProperties(properties0);
      FindWithCapabilities findWithCapabilities2 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities3 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities4 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities5 = new FindWithCapabilities();
      findWithCapabilities3.getMatches();
      findWithCapabilities2.enableNot(capabilities_Capability0);
      TestInstances testInstances0 = new TestInstances();
      FindWithCapabilities findWithCapabilities6 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities7 = new FindWithCapabilities();
      assertFalse(findWithCapabilities7.equals((Object)findWithCapabilities3));
  }

  /**
  //Test case number: 50
  /*Coverage entropy=2.970166511608329
  */
  @Test(timeout = 4000)
  public void test50()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[8];
      stringArray0[0] = "";
      stringArray0[1] = "This will normalize the class if it's numeric. This could help improve performance of the network, It normalizes the class to be between -1 and 1. Note that this is only internally, the output will be scaled back to the original range.";
      stringArray0[2] = "^%u5Z/]G(+.Nh";
      stringArray0[3] = "Z";
      naiveBayesMultinomialText0.m_wordFrequencies = false;
      stringArray0[4] = "zwo,B1i/Gdf^O)C-Q^m";
      stringArray0[5] = "-stopwords";
      naiveBayesMultinomialText0.setNorm(433.2);
      stringArray0[6] = "~i=JaD.,o";
      NaiveBayesMultinomialText.main(stringArray0);
      stringArray0[7] = "get";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      AbstractClassifier.makeCopy(naiveBayesMultinomialText0);
      naiveBayesMultinomialText0.setNormalizeDocLength(false);
      boolean boolean0 = naiveBayesMultinomialText0.m_useStopList;
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      naiveBayesMultinomialText0.setNorm(0.0);
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText0.getLowercaseTokens();
      naiveBayesMultinomialText0.globalInfo();
      assertEquals(0.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 51
  /*Coverage entropy=2.822061379926953
  */
  @Test(timeout = 4000)
  public void test51()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_useStopList = true;
      Attribute attribute0 = new Attribute("weights.length != numInstances.", 11);
      attribute0.indexOfValue("numeric");
      String[] stringArray0 = new String[8];
      stringArray0[0] = "relational";
      stringArray0[1] = "relational";
      naiveBayesMultinomialText0.getOptions();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      attribute0.equalsMsg(iteratedLovinsStemmer0);
      Instances instances0 = new Instances("[YDaVL3{q;b", arrayList0, 4);
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 52
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test52()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      Attribute attribute0 = new Attribute("weights.length != numInstances.", 11);
      attribute0.indexOfValue("numeric");
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "");
      String[] stringArray0 = new String[3];
      stringArray0[0] = "relational";
      String[] stringArray1 = new String[8];
      stringArray1[0] = "@end";
      stringArray1[1] = "-norm <num>";
      stringArray1[2] = "date";
      stringArray1[3] = "yh@o";
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities2 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities3 = new FindWithCapabilities();
      findWithCapabilities2.getMatches();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.NOMINAL_ATTRIBUTES;
      findWithCapabilities2.enableNot(capabilities_Capability0);
      TestInstances testInstances0 = new TestInstances();
      Random.setNextRandom(3);
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = (-1302.4619214372);
      DenseInstance denseInstance0 = new DenseInstance(729.176609490095, doubleArray0);
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) denseInstance0, true);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 53
  /*Coverage entropy=1.830738805564335
  */
  @Test(timeout = 4000)
  public void test53()  throws Throwable  {
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.EMPTY_NOMINAL_ATTRIBUTES;
      findWithCapabilities0.disableNot(capabilities_Capability0);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      try { 
        naiveBayesMultinomialText0.buildClassifier((Instances) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 54
  /*Coverage entropy=2.555632017870312
  */
  @Test(timeout = 4000)
  public void test54()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string0);
      
      naiveBayesMultinomialText0.getDebug();
      String string1 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string1);
      
      naiveBayesMultinomialText0.normTipText();
      String string2 = naiveBayesMultinomialText0.normTipText();
      assertEquals("The norm of the instances after normalization.", string2);
      
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("0Fsa");
      SnowballStemmer snowballStemmer1 = new SnowballStemmer();
      SnowballStemmer snowballStemmer2 = new SnowballStemmer();
      SnowballStemmer snowballStemmer3 = new SnowballStemmer("Xgra8^C'");
      String string3 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string3);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      String string4 = naiveBayesMultinomialText1.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string4);
      
      String string5 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string5);
      
      int int0 = naiveBayesMultinomialText1.getPeriodicPruning();
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals(0, int0);
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
  }

  /**
  //Test case number: 55
  /*Coverage entropy=0.9004024235381879
  */
  @Test(timeout = 4000)
  public void test55()  throws Throwable  {
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "DHvo");
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = 2702.04234753;
      doubleArray0[1] = 4080.9253;
      doubleArray0[2] = 4080.9253;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-2593.36913732), doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance(binarySparseInstance1, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 56
  /*Coverage entropy=1.2961346570528898
  */
  @Test(timeout = 4000)
  public void test56()  throws Throwable  {
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("DHvo");
      SparseInstance sparseInstance0 = new SparseInstance(449);
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      IBk iBk0 = new IBk();
      findWithCapabilities0.setHandler(iBk0);
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = 0.0;
      int[] intArray0 = new int[8];
      intArray0[0] = 449;
      intArray0[1] = 449;
      intArray0[2] = 1;
      intArray0[3] = 449;
      intArray0[4] = 449;
      intArray0[5] = 449;
      intArray0[7] = 449;
      SparseInstance sparseInstance1 = new SparseInstance(3007.26088717, doubleArray0, intArray0, 449);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(sparseInstance1);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance(1, intArray0, 449);
      SparseInstance sparseInstance2 = new SparseInstance(1918);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      binarySparseInstance1.insertAttributeAt(38);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((SparseInstance) binarySparseInstance1);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance2);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 57
  /*Coverage entropy=1.5271837172395382
  */
  @Test(timeout = 4000)
  public void test57()  throws Throwable  {
      FileSystemHandling.shouldAllThrowIOExceptions();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      Stopwords stopwords0 = new Stopwords();
      stopwords0.add("-lowercase");
      int[] intArray0 = new int[6];
      intArray0[0] = 298;
      intArray0[1] = 2573;
      intArray0[2] = 1;
      intArray0[3] = 2573;
      intArray0[4] = 2;
      intArray0[5] = 1;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0.0, intArray0, 10000);
      try { 
        naiveBayesMultinomialText0.updateClassifier(binarySparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 58
  /*Coverage entropy=1.5271837172395382
  */
  @Test(timeout = 4000)
  public void test58()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      byte[] byteArray0 = new byte[1];
      byteArray0[0] = (byte)10;
      FileSystemHandling.appendDataToFile(evoSuiteFile0, byteArray0);
      Attribute attribute0 = new Attribute("weights.length != numInstances.", 11);
      attribute0.indexOfValue("numeric");
      String[] stringArray0 = new String[8];
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = (double) 1;
      doubleArray0[1] = 1540.17566;
      doubleArray0[2] = (double) (-1);
      doubleArray0[3] = (double) 2;
      doubleArray0[4] = (double) 2;
      doubleArray0[5] = (double) 0;
      doubleArray0[6] = (double) (-1);
      doubleArray0[7] = (double) 0;
      naiveBayesMultinomialText0.m_wordsPerClass = doubleArray0;
      stringArray0[0] = "relational";
      stringArray0[1] = "relational";
      stringArray0[2] = "relational";
      stringArray0[3] = "relational";
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 59
  /*Coverage entropy=3.2698986887396755
  */
  @Test(timeout = 4000)
  public void test59()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      Attribute attribute0 = new Attribute("weights.length != numInstances.", 11);
      KStar kStar0 = new KStar();
      kStar0.listOptions();
      attribute0.indexOfValue((String) null);
      String[] stringArray0 = new String[8];
      stringArray0[0] = "relational";
      stringArray0[1] = "relational";
      stringArray0[2] = "relational";
      stringArray0[3] = null;
      stringArray0[4] = "numeric";
      stringArray0[5] = "integer";
      stringArray0[6] = "numeric";
      stringArray0[7] = "date";
      AbstractClassifier.runClassifier(kStar0, stringArray0);
      naiveBayesMultinomialText0.setPeriodicPruning((-1));
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      naiveBayesMultinomialText0.setMinWordFrequency(4);
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      naiveBayesMultinomialText0.setOptions(stringArray1);
      Random.setNextRandom(3);
  }

  /**
  //Test case number: 60
  /*Coverage entropy=3.2722146523459616
  */
  @Test(timeout = 4000)
  public void test60()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[4];
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.m_minWordP = 2231.752086978097;
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      naiveBayesMultinomialText0.setOptions(stringArray1);
      naiveBayesMultinomialText0.getOptions();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 61
  /*Coverage entropy=1.2961346570528898
  */
  @Test(timeout = 4000)
  public void test61()  throws Throwable  {
      String[] stringArray0 = new String[8];
      stringArray0[0] = "";
      stringArray0[1] = "This will normalize the class if it's numeric. This could help improve performance of the network, It normalizes the class to be between -1 and 1. Note that this is only internally, the output will be scaled back to the original range.";
      byte[] byteArray0 = new byte[8];
      byteArray0[1] = (byte)10;
      byteArray0[2] = (byte) (-117);
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (double) (byte)10;
      doubleArray0[1] = 0.0;
      doubleArray0[2] = (double) (byte) (-117);
      doubleArray0[3] = 3007.26088717;
      doubleArray0[4] = (double) (byte) (-117);
      doubleArray0[5] = (double) (byte)10;
      int[] intArray0 = new int[6];
      intArray0[0] = (int) (byte)10;
      intArray0[1] = (int) (byte)10;
      intArray0[2] = (int) (byte) (-117);
      intArray0[3] = (int) (byte) (-117);
      intArray0[4] = (int) (byte) (-117);
      intArray0[5] = (int) (byte) (-117);
      SparseInstance sparseInstance0 = new SparseInstance(0.0, doubleArray0, intArray0, (-2));
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(sparseInstance0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((byte)10, intArray0, (byte) (-117));
      SparseInstance sparseInstance1 = new SparseInstance(449);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance(10, intArray0, 38);
      binarySparseInstance2.insertAttributeAt(1);
      BinarySparseInstance binarySparseInstance3 = new BinarySparseInstance((SparseInstance) binarySparseInstance1);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance1);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 62
  /*Coverage entropy=1.6868977693384446
  */
  @Test(timeout = 4000)
  public void test62()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      naiveBayesMultinomialText0.m_probOfClass = null;
      naiveBayesMultinomialText0.setStopwords((File) null);
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      SGDText sGDText0 = new SGDText();
      sGDText0.getTokenizer();
      sGDText0.setLearningRate((-263.94868916));
      naiveBayesMultinomialText0.setTokenizer((Tokenizer) null);
      int[] intArray0 = new int[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0, intArray0, (-595));
      naiveBayesMultinomialText0.getStopwords();
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 63
  /*Coverage entropy=2.534059615514255
  */
  @Test(timeout = 4000)
  public void test63()  throws Throwable  {
      FileSystemHandling.shouldAllThrowIOExceptions();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FileSystemHandling.setPermissions((EvoSuiteFile) null, true, true, true);
      TestInstances testInstances0 = new TestInstances();
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      
      Instances instances0 = testInstances0.generate("");
      Instances instances1 = new Instances(instances0);
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      inputMappedClassifier0.getModelHeader(instances0);
      String[] stringArray0 = new String[4];
      stringArray0[0] = ".bsi";
      stringArray0[1] = "B{,w0SAtC5WzE5O/9F";
      stringArray0[2] = " ";
      stringArray0[3] = ".arff";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      Random.setNextRandom((-2));
      boolean boolean0 = naiveBayesMultinomialText0.getLowercaseTokens();
      assertFalse(boolean0);
  }

  /**
  //Test case number: 64
  /*Coverage entropy=1.6868977693384446
  */
  @Test(timeout = 4000)
  public void test64()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      naiveBayesMultinomialText0.m_probOfClass = null;
      FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      naiveBayesMultinomialText0.setStopwords((File) null);
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      naiveBayesMultinomialText0.setTokenizer((Tokenizer) null);
      int[] intArray0 = new int[0];
      Tokenizer tokenizer0 = naiveBayesMultinomialText0.getTokenizer();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertNull(tokenizer0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 65
  /*Coverage entropy=2.6772245773362195
  */
  @Test(timeout = 4000)
  public void test65()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      
      String string1 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string1);
      
      String string2 = naiveBayesMultinomialText0.normTipText();
      assertEquals("The norm of the instances after normalization.", string2);
      
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("The norm of the instances after normalization.");
      SnowballStemmer snowballStemmer1 = new SnowballStemmer();
      SnowballStemmer snowballStemmer2 = new SnowballStemmer();
      snowballStemmer2.stem("QQR ?d~~`");
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.setStemmer(snowballStemmer0);
      String string3 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string3);
      
      String string4 = naiveBayesMultinomialText1.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string4);
      
      String string5 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string5);
      
      naiveBayesMultinomialText1.getTokenizer();
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
  }

  /**
  //Test case number: 66
  /*Coverage entropy=2.557702411245053
  */
  @Test(timeout = 4000)
  public void test66()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string0);
      
      naiveBayesMultinomialText0.getDebug();
      String string1 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string1);
      
      naiveBayesMultinomialText0.normTipText();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("QQR ?d~~`");
      DenseInstance denseInstance0 = new DenseInstance(34);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(denseInstance0);
      String string2 = naiveBayesMultinomialText0.normTipText();
      assertEquals("The norm of the instances after normalization.", string2);
      
      SnowballStemmer snowballStemmer1 = new SnowballStemmer("0Fsa");
      SnowballStemmer snowballStemmer2 = new SnowballStemmer();
      SnowballStemmer snowballStemmer3 = new SnowballStemmer();
      SnowballStemmer snowballStemmer4 = new SnowballStemmer("Xgra8^C'");
      String string3 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string3);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.setStemmer(snowballStemmer0);
      String string4 = naiveBayesMultinomialText1.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string4);
      
      String string5 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string5);
      
      naiveBayesMultinomialText1.setStemmer(snowballStemmer0);
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
  }

  /**
  //Test case number: 67
  /*Coverage entropy=1.8529610277865571
  */
  @Test(timeout = 4000)
  public void test67()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("mG6L]KKDTstZ");
      SparseInstance sparseInstance0 = new SparseInstance(449);
      AbstractClassifier.runClassifier(naiveBayesMultinomialText0, (String[]) null);
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText2.normTipText();
      assertEquals("The norm of the instances after normalization.", string0);
      
      SnowballStemmer snowballStemmer1 = new SnowballStemmer("");
      SnowballStemmer snowballStemmer2 = new SnowballStemmer();
      SnowballStemmer snowballStemmer3 = new SnowballStemmer();
      snowballStemmer3.stemmerTipText();
      snowballStemmer0.stem("org.tartarus.snowball.ext");
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      int[] intArray0 = new int[6];
      intArray0[0] = 449;
      intArray0[1] = 449;
      intArray0[2] = (-595);
      intArray0[3] = 449;
      intArray0[4] = (-595);
      intArray0[5] = 15;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0.0, intArray0, 1706);
      String string1 = naiveBayesMultinomialText3.normalizeDocLengthTipText();
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string1);
      assertFalse(naiveBayesMultinomialText3.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText3.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
  }

  /**
  //Test case number: 68
  /*Coverage entropy=2.5323204850794725
  */
  @Test(timeout = 4000)
  public void test68()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[7];
      stringArray0[0] = "-M";
      Stopwords stopwords0 = new Stopwords();
      stopwords0.add("-M");
      naiveBayesMultinomialText0.m_stopwords = stopwords0;
      stringArray0[1] = "";
      double[] doubleArray0 = new double[4];
      doubleArray0[0] = (-3.0);
      doubleArray0[1] = (-697.5256209247241);
      doubleArray0[2] = 0.0;
      doubleArray0[3] = 0.0;
      naiveBayesMultinomialText0.m_probOfClass = doubleArray0;
      naiveBayesMultinomialText0.m_normalize = true;
      stringArray0[2] = "";
      stringArray0[3] = "xv(4~qpWc.ksB";
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      stringArray0[4] = "jZ1!tacbiBpk!7";
      stringArray0[5] = "";
      stringArray0[6] = "";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      naiveBayesMultinomialText0.listOptions();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 69
  /*Coverage entropy=1.5271837172395382
  */
  @Test(timeout = 4000)
  public void test69()  throws Throwable  {
      String[] stringArray0 = new String[9];
      stringArray0[0] = "^%u5Z/]G(+.Nh";
      stringArray0[1] = "This will normalize the class if it's numeric. This could help improve performance of the network, It normalizes the class to be between -1 and 1. Note that this is only internally, the output will be scaled back to the original range.";
      stringArray0[2] = "^%u5Z/]G(+.Nh";
      stringArray0[3] = "^%u5Z/]G(+.Nh";
      stringArray0[4] = "-stopwords";
      stringArray0[5] = "^%u5Z/]G(+.Nh";
      stringArray0[6] = "-stopwords";
      stringArray0[7] = "";
      stringArray0[8] = "~i=JaD.,o";
      NaiveBayesMultinomialText.main(stringArray0);
      assertEquals(9, stringArray0.length);
  }
}
