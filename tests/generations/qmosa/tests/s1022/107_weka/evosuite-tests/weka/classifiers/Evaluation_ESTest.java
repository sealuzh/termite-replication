/*
 * This file was automatically generated by EvoSuite
 * Tue Nov 19 04:37:09 GMT 2019
 */

package weka.classifiers;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.shaded.org.mockito.Mockito.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.awt.AWTEventMulticaster;
import java.awt.Dimension;
import java.awt.GridBagLayout;
import java.awt.HeadlessException;
import java.awt.IllegalComponentStateException;
import java.awt.Point;
import java.awt.Rectangle;
import java.awt.dnd.DropTarget;
import java.awt.event.MouseWheelListener;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.Enumeration;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.Locale;
import java.util.Properties;
import java.util.Random;
import java.util.stream.LongStream;
import javax.swing.Box;
import javax.swing.JFrame;
import javax.swing.JInternalFrame;
import javax.swing.JSlider;
import javax.swing.TransferHandler;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.System;
import org.evosuite.runtime.ViolatedAssumptionAnswer;
import org.evosuite.runtime.mock.java.util.MockRandom;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;
import weka.attributeSelection.InfoGainAttributeEval;
import weka.attributeSelection.ReliefFAttributeEval;
import weka.attributeSelection.WrapperSubsetEval;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.Classifier;
import weka.classifiers.CostMatrix;
import weka.classifiers.Evaluation;
import weka.classifiers.Sourcable;
import weka.classifiers.bayes.NaiveBayesMultinomialText;
import weka.classifiers.functions.GaussianProcesses;
import weka.classifiers.functions.LinearRegression;
import weka.classifiers.functions.MultilayerPerceptron;
import weka.classifiers.functions.SGDText;
import weka.classifiers.functions.SimpleLinearRegression;
import weka.classifiers.functions.SimpleLogistic;
import weka.classifiers.lazy.IBk;
import weka.classifiers.lazy.KStar;
import weka.classifiers.meta.AdaBoostM1;
import weka.classifiers.meta.AdditiveRegression;
import weka.classifiers.meta.CostSensitiveClassifier;
import weka.classifiers.meta.FilteredClassifier;
import weka.classifiers.meta.LogitBoost;
import weka.classifiers.meta.MultiClassClassifier;
import weka.classifiers.meta.MultiClassClassifierUpdateable;
import weka.classifiers.meta.RandomSubSpace;
import weka.classifiers.meta.RegressionByDiscretization;
import weka.classifiers.meta.Stacking;
import weka.classifiers.meta.Vote;
import weka.classifiers.misc.InputMappedClassifier;
import weka.classifiers.rules.M5Rules;
import weka.classifiers.rules.ZeroR;
import weka.classifiers.trees.J48;
import weka.classifiers.trees.LMT;
import weka.clusterers.FilteredClusterer;
import weka.clusterers.SimpleKMeans;
import weka.core.AbstractInstance;
import weka.core.Attribute;
import weka.core.BinarySparseInstance;
import weka.core.Capabilities;
import weka.core.CheckGOE;
import weka.core.DenseInstance;
import weka.core.FindWithCapabilities;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.ProtectedProperties;
import weka.core.SelectedTag;
import weka.core.SparseInstance;
import weka.core.TestInstances;
import weka.core.converters.ArffLoader;
import weka.core.converters.CSVLoader;
import weka.core.converters.ConverterUtils;
import weka.core.converters.DatabaseLoader;
import weka.core.converters.JSONLoader;
import weka.core.converters.LibSVMLoader;
import weka.core.converters.Loader;
import weka.core.converters.TextDirectoryLoader;
import weka.core.neighboursearch.CoverTree;
import weka.core.neighboursearch.KDTree;
import weka.core.neighboursearch.kdtrees.KDTreeNodeSplitter;
import weka.core.neighboursearch.kdtrees.MedianOfWidestDimension;
import weka.core.tokenizers.AlphabeticTokenizer;
import weka.core.tokenizers.NGramTokenizer;
import weka.core.tokenizers.Tokenizer;
import weka.core.tokenizers.WordTokenizer;
import weka.estimators.NormalEstimator;
import weka.filters.AllFilter;
import weka.filters.supervised.attribute.Discretize;
import weka.gui.LogPanel;
import weka.gui.WekaTaskMonitor;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class Evaluation_ESTest extends Evaluation_ESTest_scaffolding {

  /**
  //Test case number: 0
  /*Coverage entropy=2.1987926481569326
  */
  @Test(timeout = 4000)
  public void test000()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getDebug());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getDebug());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals(0, costSensitiveClassifier0.graphType());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier3);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      
      CostSensitiveClassifier costSensitiveClassifier4 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier4);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      
      CostMatrix costMatrix0 = costSensitiveClassifier2.getCostMatrix();
      assertNotNull(costMatrix0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.size());
      assertEquals(1, costMatrix0.numRows());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier4));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier3);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier4);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier0);
      
      MockRandom mockRandom0 = new MockRandom();
      assertNotNull(mockRandom0);
      
      LongStream longStream0 = mockRandom0.longs(0L);
      assertNotNull(longStream0);
      
      double double0 = Evaluation.MIN_SF_PROB;
      assertEquals(4.9E-324, double0, 0.01);
      
      String string0 = costMatrix0.toMatlab();
      assertEquals("[0.0]", string0);
      assertNotNull(string0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.size());
      assertEquals(1, costMatrix0.numRows());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier4));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier3);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier4);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier0);
      
      MockRandom mockRandom1 = new MockRandom();
      assertNotNull(mockRandom1);
      assertFalse(mockRandom1.equals((Object)mockRandom0));
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertNotNull(instances0);
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      
      double[] doubleArray0 = evaluation0.getClassPriors();
      assertNotNull(doubleArray0);
      assertEquals(0, doubleArray0.length);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      
      String string1 = evaluation0.toSummaryString(true);
      assertEquals("=== Summary ===\n\nTotal Number of Instances                0     \n", string1);
      assertNotNull(string1);
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(string1.equals((Object)string0));
  }

  /**
  //Test case number: 1
  /*Coverage entropy=1.794375227518644
  */
  @Test(timeout = 4000)
  public void test001()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals(1, lMT0.graphType());
      assertFalse(lMT0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      
      M5Rules m5Rules0 = new M5Rules();
      assertNotNull(m5Rules0);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getUseUnsmoothed());
      
      Capabilities capabilities0 = m5Rules0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      
      Instances instances0 = testInstances0.generate("getClass");
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      
      Capabilities capabilities1 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities1);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities1, capabilities0);
      
      capabilities0.assign(capabilities1);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      boolean boolean1 = FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      
      Iterator iterator0 = capabilities0.capabilities();
      assertNotNull(iterator0);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(capabilities0, capabilities1);
      
      evaluation0.setPriors(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(capabilities0, capabilities1);
      
      KDTree kDTree0 = new KDTree();
      assertNotNull(kDTree0);
      assertEquals(1, KDTree.MAX);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(0, KDTree.MIN);
      assertFalse(kDTree0.getMeasurePerformance());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree0.distanceFunctionTipText());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree0.nodeSplitterTipText());
      assertEquals(0.0, kDTree0.measureNumLeaves(), 0.01);
      assertEquals(0.0, kDTree0.measureMaxDepth(), 0.01);
      assertEquals("The max number of instances in a leaf.", kDTree0.maxInstInLeafTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree0.measurePerformanceTipText());
      assertEquals(0.01, kDTree0.getMinBoxRelWidth(), 0.01);
      assertEquals(40, kDTree0.getMaxInstInLeaf());
      assertTrue(kDTree0.getNormalizeNodeWidth());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree0.minBoxRelWidthTipText());
      assertEquals(0.0, kDTree0.measureTreeSize(), 0.01);
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree0.normalizeNodeWidthTipText());
      
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = (double) 0;
      doubleArray0[1] = (double) (-1);
      doubleArray0[2] = (-0.25);
      doubleArray0[3] = (double) (-1);
      doubleArray0[4] = (double) (-2);
      doubleArray0[5] = (double) (-1);
      doubleArray0[6] = (double) 2;
      Enumeration enumeration0 = instances0.enumerateInstances();
      assertNotNull(enumeration0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(capabilities0, capabilities1);
      
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-0.25), doubleArray0);
      assertNotNull(binarySparseInstance0);
      assertEquals(7, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {0.0, (-1.0), (-0.25), (-1.0), (-2.0), (-1.0), 2.0}, doubleArray0, 0.01);
      assertEquals(7, binarySparseInstance0.numAttributes());
      assertEquals((-0.25), binarySparseInstance0.weight(), 0.01);
      assertEquals(6, binarySparseInstance0.numValues());
      
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance0);
      assertNotNull(sparseInstance0);
      assertEquals(7, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {0.0, (-1.0), (-0.25), (-1.0), (-2.0), (-1.0), 2.0}, doubleArray0, 0.01);
      assertEquals(7, binarySparseInstance0.numAttributes());
      assertEquals((-0.25), binarySparseInstance0.weight(), 0.01);
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals(7, sparseInstance0.numAttributes());
      assertEquals(6, sparseInstance0.numValues());
      assertEquals((-0.25), sparseInstance0.weight(), 0.01);
      
      double[] doubleArray1 = new double[2];
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      
      doubleArray1[0] = (double) (-1);
      doubleArray1[1] = (double) 0;
      try { 
        evaluation0.toCumulativeMarginDistributionString();
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Class must be nominal for margin distributions
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 2
  /*Coverage entropy=2.2614525198568534
  */
  @Test(timeout = 4000)
  public void test002()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      
      Instances instances1 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader0);
      assertNotNull(instances1);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals(0, instances1.numInstances());
      assertEquals(2, instances1.numAttributes());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(0, instances1.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertEquals(1, instances1.classIndex());
      assertTrue(instances1.equals((Object)instances0));
      assertNotSame(instances1, instances0);
      
      Instances instances2 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances2);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertEquals(0, instances2.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(1, instances2.classIndex());
      assertTrue(instances2.equals((Object)instances0));
      assertNotSame(instances2, instances0);
      assertSame(instances2, instances1);
      
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      
      Instances instances3 = testInstances0.generate();
      assertNotNull(instances3);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20.0, instances3.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances3.relationName());
      assertEquals(2, instances3.numAttributes());
      assertFalse(instances3.checkForStringAttributes());
      assertEquals(2, instances3.numClasses());
      assertEquals(20, instances3.size());
      assertEquals(1, instances3.classIndex());
      assertEquals(20, instances3.numInstances());
      assertFalse(instances3.equals((Object)instances0));
      assertFalse(instances3.equals((Object)instances2));
      assertFalse(instances3.equals((Object)instances1));
      assertNotSame(instances3, instances0);
      assertNotSame(instances3, instances2);
      assertNotSame(instances3, instances1);
      
      KDTree kDTree0 = new KDTree();
      assertNotNull(kDTree0);
      assertEquals(1, KDTree.MAX);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(0, KDTree.MIN);
      assertEquals(40, kDTree0.getMaxInstInLeaf());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree0.minBoxRelWidthTipText());
      assertTrue(kDTree0.getNormalizeNodeWidth());
      assertEquals(0.0, kDTree0.measureTreeSize(), 0.01);
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree0.normalizeNodeWidthTipText());
      assertEquals(0.0, kDTree0.measureNumLeaves(), 0.01);
      assertFalse(kDTree0.getMeasurePerformance());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree0.distanceFunctionTipText());
      assertEquals(0.0, kDTree0.measureMaxDepth(), 0.01);
      assertEquals("The max number of instances in a leaf.", kDTree0.maxInstInLeafTipText());
      assertEquals(0.01, kDTree0.getMinBoxRelWidth(), 0.01);
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree0.nodeSplitterTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree0.measurePerformanceTipText());
      
      KDTree kDTree1 = new KDTree();
      assertNotNull(kDTree1);
      assertEquals(1, KDTree.MAX);
      assertEquals(0, KDTree.MIN);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(0.0, kDTree1.measureTreeSize(), 0.01);
      assertEquals(40, kDTree1.getMaxInstInLeaf());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree1.minBoxRelWidthTipText());
      assertTrue(kDTree1.getNormalizeNodeWidth());
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree1.normalizeNodeWidthTipText());
      assertEquals(0.0, kDTree1.measureNumLeaves(), 0.01);
      assertEquals(0.0, kDTree1.measureMaxDepth(), 0.01);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree1.distanceFunctionTipText());
      assertFalse(kDTree1.getMeasurePerformance());
      assertEquals("The max number of instances in a leaf.", kDTree1.maxInstInLeafTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree1.measurePerformanceTipText());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree1.nodeSplitterTipText());
      assertEquals(0.01, kDTree1.getMinBoxRelWidth(), 0.01);
      assertFalse(kDTree1.equals((Object)kDTree0));
      
      WekaTaskMonitor wekaTaskMonitor0 = new WekaTaskMonitor();
      assertNotNull(wekaTaskMonitor0);
      assertFalse(wekaTaskMonitor0.isFocusTraversalPolicySet());
      assertFalse(wekaTaskMonitor0.isFocusCycleRoot());
      assertFalse(wekaTaskMonitor0.isFocusTraversalPolicyProvider());
      assertFalse(wekaTaskMonitor0.getIgnoreRepaint());
      assertTrue(wekaTaskMonitor0.getFocusTraversalKeysEnabled());
      
      MouseWheelListener mouseWheelListener0 = AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener0);
      
      MouseWheelListener mouseWheelListener1 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener1);
      
      MouseWheelListener mouseWheelListener2 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener2);
      
      MouseWheelListener mouseWheelListener3 = AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener3);
      
      MouseWheelListener mouseWheelListener4 = AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener4);
      
      MouseWheelListener mouseWheelListener5 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener5);
      
      MouseWheelListener mouseWheelListener6 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener6);
      
      WekaTaskMonitor wekaTaskMonitor1 = new WekaTaskMonitor();
      assertNotNull(wekaTaskMonitor1);
      assertTrue(wekaTaskMonitor1.getFocusTraversalKeysEnabled());
      assertFalse(wekaTaskMonitor1.getIgnoreRepaint());
      assertFalse(wekaTaskMonitor1.isFocusTraversalPolicySet());
      assertFalse(wekaTaskMonitor1.isFocusTraversalPolicyProvider());
      assertFalse(wekaTaskMonitor1.isFocusCycleRoot());
      assertFalse(wekaTaskMonitor1.equals((Object)wekaTaskMonitor0));
      
      MouseWheelListener mouseWheelListener7 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener7);
      
      MouseWheelListener mouseWheelListener8 = AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener8);
      
      wekaTaskMonitor1.addMouseWheelListener((MouseWheelListener) null);
      assertTrue(wekaTaskMonitor1.getFocusTraversalKeysEnabled());
      assertFalse(wekaTaskMonitor1.getIgnoreRepaint());
      assertFalse(wekaTaskMonitor1.isFocusTraversalPolicySet());
      assertFalse(wekaTaskMonitor1.isFocusTraversalPolicyProvider());
      assertFalse(wekaTaskMonitor1.isFocusCycleRoot());
      assertFalse(wekaTaskMonitor1.equals((Object)wekaTaskMonitor0));
      assertNotSame(wekaTaskMonitor1, wekaTaskMonitor0);
      
      Evaluation evaluation0 = new Evaluation(instances2);
      assertNotNull(evaluation0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertEquals(0, instances2.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(1, instances2.classIndex());
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertTrue(instances2.equals((Object)instances0));
      assertFalse(instances2.equals((Object)instances3));
      
      SimpleLogistic simpleLogistic0 = new SimpleLogistic();
      assertNotNull(simpleLogistic0);
      assertEquals(50, simpleLogistic0.getHeuristicStop());
      assertEquals("Sets the maximum number of iterations for LogitBoost. Default value is 500, for very small/large datasets a lower/higher value might be preferable.", simpleLogistic0.maxBoostingIterationsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", simpleLogistic0.weightTrimBetaTipText());
      assertEquals(0.0, simpleLogistic0.getWeightTrimBeta(), 0.01);
      assertTrue(simpleLogistic0.getUseCrossValidation());
      assertFalse(simpleLogistic0.getErrorOnProbabilities());
      assertEquals("If set to true, classifier may output additional info to the console.", simpleLogistic0.debugTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations (instead of cross-validation or training error).", simpleLogistic0.useAICTipText());
      assertEquals("Use error on the probabilties as error measure when determining the best number of LogitBoost iterations. If set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error (either on the training set or in the cross-validation, depending on useCrossValidation).", simpleLogistic0.errorOnProbabilitiesTipText());
      assertFalse(simpleLogistic0.getDebug());
      assertEquals("If heuristicStop > 0, the heuristic for greedy stopping while cross-validating the number of LogitBoost iterations is enabled. This means LogitBoost is stopped if no new error minimum has been reached in the last heuristicStop iterations. It is recommended to use this heuristic, it gives a large speed-up especially on small datasets. The default value is 50.", simpleLogistic0.heuristicStopTipText());
      assertEquals(500, simpleLogistic0.getMaxBoostingIterations());
      assertEquals("Set fixed number of iterations for LogitBoost. If >= 0, this sets the number of LogitBoost iterations to perform. If < 0, the number is cross-validated or a stopping criterion on the training set is used (depending on the value of useCrossValidation).", simpleLogistic0.numBoostingIterationsTipText());
      assertEquals("Sets whether the number of LogitBoost iterations is to be cross-validated or the stopping criterion on the training set should be used. If not set (and no fixed number of iterations was given), the number of LogitBoost iterations is used that minimizes the error on the training set (misclassification error or error on probabilities depending on errorOnProbabilities).", simpleLogistic0.useCrossValidationTipText());
      assertFalse(simpleLogistic0.getUseAIC());
      assertEquals(0, simpleLogistic0.getNumBoostingIterations());
      
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props");
      boolean boolean0 = FileSystemHandling.setPermissions(evoSuiteFile0, false, false, true);
      assertFalse(boolean0);
      
      Evaluation evaluation1 = new Evaluation(instances2);
      assertNotNull(evaluation1);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertEquals(0, instances2.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(1, instances2.classIndex());
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertTrue(instances2.equals((Object)instances0));
      assertFalse(instances2.equals((Object)instances3));
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      double double0 = evaluation1.SFEntropyGain();
      assertEquals(0.0, double0, 0.01);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertEquals(0, instances2.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(1, instances2.classIndex());
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertTrue(instances2.equals((Object)instances0));
      assertFalse(instances2.equals((Object)instances3));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(instances2, instances0);
      assertSame(instances2, instances1);
      assertNotSame(instances2, instances3);
      assertNotSame(evaluation1, evaluation0);
      
      String string0 = Evaluation.makeOptionString(simpleLogistic0, true);
      assertNotNull(string0);
      assertEquals(50, simpleLogistic0.getHeuristicStop());
      assertEquals("Sets the maximum number of iterations for LogitBoost. Default value is 500, for very small/large datasets a lower/higher value might be preferable.", simpleLogistic0.maxBoostingIterationsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", simpleLogistic0.weightTrimBetaTipText());
      assertEquals(0.0, simpleLogistic0.getWeightTrimBeta(), 0.01);
      assertTrue(simpleLogistic0.getUseCrossValidation());
      assertFalse(simpleLogistic0.getErrorOnProbabilities());
      assertEquals("If set to true, classifier may output additional info to the console.", simpleLogistic0.debugTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations (instead of cross-validation or training error).", simpleLogistic0.useAICTipText());
      assertEquals("Use error on the probabilties as error measure when determining the best number of LogitBoost iterations. If set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error (either on the training set or in the cross-validation, depending on useCrossValidation).", simpleLogistic0.errorOnProbabilitiesTipText());
      assertFalse(simpleLogistic0.getDebug());
      assertEquals("If heuristicStop > 0, the heuristic for greedy stopping while cross-validating the number of LogitBoost iterations is enabled. This means LogitBoost is stopped if no new error minimum has been reached in the last heuristicStop iterations. It is recommended to use this heuristic, it gives a large speed-up especially on small datasets. The default value is 50.", simpleLogistic0.heuristicStopTipText());
      assertEquals(500, simpleLogistic0.getMaxBoostingIterations());
      assertEquals("Set fixed number of iterations for LogitBoost. If >= 0, this sets the number of LogitBoost iterations to perform. If < 0, the number is cross-validated or a stopping criterion on the training set is used (depending on the value of useCrossValidation).", simpleLogistic0.numBoostingIterationsTipText());
      assertEquals("Sets whether the number of LogitBoost iterations is to be cross-validated or the stopping criterion on the training set should be used. If not set (and no fixed number of iterations was given), the number of LogitBoost iterations is used that minimizes the error on the training set (misclassification error or error on probabilities depending on errorOnProbabilities).", simpleLogistic0.useCrossValidationTipText());
      assertFalse(simpleLogistic0.getUseAIC());
      assertEquals(0, simpleLogistic0.getNumBoostingIterations());
      
      boolean boolean1 = evaluation1.equals((Object) null);
      assertFalse(boolean1);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertEquals(0, instances2.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(1, instances2.classIndex());
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertTrue(instances2.equals((Object)instances0));
      assertFalse(instances2.equals((Object)instances3));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertTrue(boolean1 == boolean0);
      assertNotSame(instances2, instances0);
      assertSame(instances2, instances1);
      assertNotSame(instances2, instances3);
      assertNotSame(evaluation1, evaluation0);
  }

  /**
  //Test case number: 3
  /*Coverage entropy=3.381749380901425
  */
  @Test(timeout = 4000)
  public void test003()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertNotNull(instances0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      
      boolean boolean0 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "5D");
      assertFalse(boolean0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      
      boolean boolean1 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean1);
      assertFalse(boolean1 == boolean0);
      
      String string0 = evaluation0.toMatrixString();
      assertEquals("=== Confusion Matrix ===\n\n   <-- classified as\n", string0);
      assertNotNull(string0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      
      evaluation0.m_Correct = (-304.0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals((-304.0), evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      
      textDirectoryLoader0.setDebug(true);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertTrue(textDirectoryLoader0.getDebug());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      
      evaluation0.m_SumPriorEntropy = (-304.0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertTrue(textDirectoryLoader0.getDebug());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals((-304.0), evaluation0.SFEntropyGain(), 0.01);
      assertEquals((-304.0), evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals((-304.0), evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      
      Instances instances1 = evaluation0.getHeader();
      assertNotNull(instances1);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertTrue(textDirectoryLoader0.getDebug());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals((-304.0), evaluation0.SFEntropyGain(), 0.01);
      assertEquals((-304.0), evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals((-304.0), evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(0, instances1.size());
      assertEquals(1, instances1.classIndex());
      assertEquals(0, instances1.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertEquals(0, instances1.numClasses());
      assertEquals(2, instances1.numAttributes());
      assertTrue(instances1.equals((Object)instances0));
      assertNotSame(instances0, instances1);
      assertNotSame(instances1, instances0);
      
      textDirectoryLoader0.setCharSet("3Ixn>$8?ZJQJaYy/ [");
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("3Ixn>$8?ZJQJaYy/ [", textDirectoryLoader0.getCharSet());
      assertTrue(textDirectoryLoader0.getDebug());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      
      String string1 = evaluation0.toClassDetailsString(".arff");
      assertEquals(".arff\n                 TP Rate  FP Rate  Precision  Recall  F-Measure  MCC    ROC Area  PRC Area  Class\nWeighted Avg.  NaN      NaN      NaN        NaN     NaN        NaN    NaN       NaN    \n", string1);
      assertNotNull(string1);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("3Ixn>$8?ZJQJaYy/ [", textDirectoryLoader0.getCharSet());
      assertTrue(textDirectoryLoader0.getDebug());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals((-304.0), evaluation0.SFEntropyGain(), 0.01);
      assertEquals((-304.0), evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals((-304.0), evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertTrue(instances0.equals((Object)instances1));
      assertFalse(string1.equals((Object)string0));
      assertNotSame(instances0, instances1);
      
      SimpleLinearRegression simpleLinearRegression0 = new SimpleLinearRegression();
      assertNotNull(simpleLinearRegression0);
      assertEquals("Learns a simple linear regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes.", simpleLinearRegression0.globalInfo());
      assertFalse(simpleLinearRegression0.foundUsefulAttribute());
      assertEquals(0.0, simpleLinearRegression0.getIntercept(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", simpleLinearRegression0.debugTipText());
      assertEquals(0, simpleLinearRegression0.getAttributeIndex());
      assertEquals(0.0, simpleLinearRegression0.getSlope(), 0.01);
      assertFalse(simpleLinearRegression0.getDebug());
      
      String string2 = Evaluation.makeOptionString(simpleLinearRegression0, true);
      assertNotNull(string2);
      assertEquals("Learns a simple linear regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes.", simpleLinearRegression0.globalInfo());
      assertFalse(simpleLinearRegression0.foundUsefulAttribute());
      assertEquals(0.0, simpleLinearRegression0.getIntercept(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", simpleLinearRegression0.debugTipText());
      assertEquals(0, simpleLinearRegression0.getAttributeIndex());
      assertEquals(0.0, simpleLinearRegression0.getSlope(), 0.01);
      assertFalse(simpleLinearRegression0.getDebug());
      assertFalse(string2.equals((Object)string0));
      assertFalse(string2.equals((Object)string1));
      
      double double0 = evaluation0.correct();
      assertEquals((-304.0), double0, 0.01);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("3Ixn>$8?ZJQJaYy/ [", textDirectoryLoader0.getCharSet());
      assertTrue(textDirectoryLoader0.getDebug());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals((-304.0), evaluation0.SFEntropyGain(), 0.01);
      assertEquals((-304.0), evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals((-304.0), evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertTrue(instances0.equals((Object)instances1));
      assertNotSame(instances0, instances1);
      
      double double1 = evaluation0.pctCorrect();
      assertEquals(Double.NEGATIVE_INFINITY, double1, 0.01);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("3Ixn>$8?ZJQJaYy/ [", textDirectoryLoader0.getCharSet());
      assertTrue(textDirectoryLoader0.getDebug());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals((-304.0), evaluation0.SFEntropyGain(), 0.01);
      assertEquals((-304.0), evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals((-304.0), evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertTrue(instances0.equals((Object)instances1));
      assertNotEquals(double1, double0, 0.01);
      assertNotSame(instances0, instances1);
      
      double double2 = evaluation0.meanPriorAbsoluteError();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("3Ixn>$8?ZJQJaYy/ [", textDirectoryLoader0.getCharSet());
      assertTrue(textDirectoryLoader0.getDebug());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals((-304.0), evaluation0.SFEntropyGain(), 0.01);
      assertEquals((-304.0), evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals((-304.0), evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertNotEquals(double2, double1, 0.01);
      assertNotEquals(double2, double0, 0.01);
      assertTrue(instances0.equals((Object)instances1));
      assertNotSame(instances0, instances1);
      
      double double3 = evaluation0.KBInformation();
      assertEquals(0.0, double3, 0.01);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("3Ixn>$8?ZJQJaYy/ [", textDirectoryLoader0.getCharSet());
      assertTrue(textDirectoryLoader0.getDebug());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals((-304.0), evaluation0.SFEntropyGain(), 0.01);
      assertEquals((-304.0), evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals((-304.0), evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NEGATIVE_INFINITY, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertNotEquals(double3, double1, 0.01);
      assertNotEquals(double3, double2, 0.01);
      assertNotEquals(double3, double0, 0.01);
      assertTrue(instances0.equals((Object)instances1));
      assertNotSame(instances0, instances1);
      
      String string3 = Evaluation.makeOptionString(simpleLinearRegression0, true);
      assertNotNull(string3);
      assertEquals("Learns a simple linear regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes.", simpleLinearRegression0.globalInfo());
      assertFalse(simpleLinearRegression0.foundUsefulAttribute());
      assertEquals(0.0, simpleLinearRegression0.getIntercept(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", simpleLinearRegression0.debugTipText());
      assertEquals(0, simpleLinearRegression0.getAttributeIndex());
      assertEquals(0.0, simpleLinearRegression0.getSlope(), 0.01);
      assertFalse(simpleLinearRegression0.getDebug());
      assertTrue(string3.equals((Object)string2));
      assertFalse(string3.equals((Object)string1));
      assertFalse(string3.equals((Object)string0));
  }

  /**
  //Test case number: 4
  /*Coverage entropy=0.9004024235381879
  */
  @Test(timeout = 4000)
  public void test004()  throws Throwable  {
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      boolean boolean0 = FileSystemHandling.appendLineToFile(evoSuiteFile0, "f");
      assertFalse(boolean0);
      
      CSVLoader cSVLoader0 = new CSVLoader();
      assertNotNull(cSVLoader0);
      assertEquals("First row of data does not contain attribute names", cSVLoader0.noHeaderRowPresentTipText());
      assertEquals("The range of attributes to force to type STRING, example ranges: 'first-last', '1,4,7-14, 50-last'.", cSVLoader0.dateAttributesTipText());
      assertEquals("?", cSVLoader0.getMissingValue());
      assertEquals("The characters to use as enclosures for strings. E.g. \",'", cSVLoader0.enclosureCharactersTipText());
      assertEquals("The range of attributes to force to be of type NOMINAL, example ranges: 'first-last', '1,4,7-14,50-last'.", cSVLoader0.nominalAttributesTipText());
      assertEquals(".csv", cSVLoader0.getFileExtension());
      assertEquals("The format to use for parsing date values.", cSVLoader0.dateFormatTipText());
      assertEquals("Reads a source that is in comma separated format (the default). One can also change the column separator from comma to tab or another character. Assumes that the first row in the file determines the number of and names of the attributes.", cSVLoader0.globalInfo());
      assertEquals("", cSVLoader0.getDateFormat());
      assertEquals("Use relative rather than absolute paths", cSVLoader0.useRelativePathTipText());
      assertEquals("The character to use as separator for the columns/fields (use '\\t' for TAB).", cSVLoader0.fieldSeparatorTipText());
      assertFalse(cSVLoader0.getNoHeaderRowPresent());
      assertEquals("The range of attributes to force to be of type STRING, example ranges: 'first-last', '1,4,7-14,50-last'.", cSVLoader0.stringAttributesTipText());
      assertEquals("CSV data files", cSVLoader0.getFileDescription());
      assertEquals("The placeholder for missing values, default is '?'.", cSVLoader0.missingValueTipText());
      assertFalse(cSVLoader0.getUseRelativePath());
      assertEquals("\",'", cSVLoader0.getEnclosureCharacters());
      
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      assertNotNull(gridBagLayout0);
      
      Box box0 = Box.createVerticalBox();
      assertNotNull(box0);
      assertFalse(box0.getIgnoreRepaint());
      assertTrue(box0.getFocusTraversalKeysEnabled());
      assertFalse(box0.isFocusTraversalPolicySet());
      assertFalse(box0.isFocusCycleRoot());
      assertFalse(box0.isFocusTraversalPolicyProvider());
      
      Dimension dimension0 = gridBagLayout0.preferredLayoutSize(box0);
      assertNotNull(dimension0);
      assertEquals(0, dimension0.height);
      assertEquals(0, dimension0.width);
      assertFalse(box0.getIgnoreRepaint());
      assertTrue(box0.getFocusTraversalKeysEnabled());
      assertFalse(box0.isFocusTraversalPolicySet());
      assertFalse(box0.isFocusCycleRoot());
      assertFalse(box0.isFocusTraversalPolicyProvider());
      assertEquals(0.0, dimension0.getWidth(), 0.01);
      assertEquals(0.0, dimension0.getHeight(), 0.01);
      
      GridBagLayout gridBagLayout1 = new GridBagLayout();
      assertNotNull(gridBagLayout1);
      assertFalse(gridBagLayout1.equals((Object)gridBagLayout0));
      
      Point point0 = gridBagLayout0.getLayoutOrigin();
      assertNotNull(point0);
      assertEquals(0, point0.x);
      assertEquals(0, point0.y);
      assertEquals(0.0, point0.getY(), 0.01);
      assertEquals(0.0, point0.getX(), 0.01);
      assertFalse(gridBagLayout0.equals((Object)gridBagLayout1));
      assertNotSame(gridBagLayout0, gridBagLayout1);
      
      boolean boolean1 = FileSystemHandling.setPermissions(evoSuiteFile0, false, true, false);
      assertTrue(boolean1);
      assertFalse(boolean1 == boolean0);
      
      String string0 = Evaluation.makeOptionString((Classifier) null, true);
      assertNotNull(string0);
  }

  /**
  //Test case number: 5
  /*Coverage entropy=1.85930838197441
  */
  @Test(timeout = 4000)
  public void test005()  throws Throwable  {
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      
      M5Rules m5Rules0 = new M5Rules();
      assertNotNull(m5Rules0);
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      
      Capabilities capabilities0 = m5Rules0.getCapabilities();
      assertNotNull(capabilities0);
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      
      Instances instances0 = testInstances0.generate("getClass");
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      Capabilities capabilities1 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities1);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities1, capabilities0);
      
      capabilities0.assign(capabilities1);
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      boolean boolean1 = FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      
      Iterator iterator0 = capabilities0.capabilities();
      assertNotNull(iterator0);
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(capabilities0, capabilities1);
      
      evaluation0.setPriors(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertFalse(m5Rules0.getUnpruned());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(capabilities0, capabilities1);
      
      KDTree kDTree0 = new KDTree();
      assertNotNull(kDTree0);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(0, KDTree.MIN);
      assertEquals(1, KDTree.MAX);
      assertEquals("The max number of instances in a leaf.", kDTree0.maxInstInLeafTipText());
      assertEquals(0.0, kDTree0.measureNumLeaves(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree0.measurePerformanceTipText());
      assertEquals(0.0, kDTree0.measureTreeSize(), 0.01);
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree0.minBoxRelWidthTipText());
      assertTrue(kDTree0.getNormalizeNodeWidth());
      assertEquals(0.01, kDTree0.getMinBoxRelWidth(), 0.01);
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree0.normalizeNodeWidthTipText());
      assertEquals(40, kDTree0.getMaxInstInLeaf());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree0.distanceFunctionTipText());
      assertEquals(0.0, kDTree0.measureMaxDepth(), 0.01);
      assertFalse(kDTree0.getMeasurePerformance());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree0.nodeSplitterTipText());
      
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = (double) 0;
      doubleArray0[1] = (double) (-1);
      doubleArray0[2] = (-0.25);
      doubleArray0[3] = (double) (-1);
      doubleArray0[4] = (double) (-2);
      doubleArray0[5] = (double) (-1);
      doubleArray0[6] = (double) 2;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-0.25), doubleArray0);
      assertNotNull(binarySparseInstance0);
      assertEquals(7, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {0.0, (-1.0), (-0.25), (-1.0), (-2.0), (-1.0), 2.0}, doubleArray0, 0.01);
      assertEquals(7, binarySparseInstance0.numAttributes());
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals((-0.25), binarySparseInstance0.weight(), 0.01);
      
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance0);
      assertNotNull(sparseInstance0);
      assertEquals(7, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {0.0, (-1.0), (-0.25), (-1.0), (-2.0), (-1.0), 2.0}, doubleArray0, 0.01);
      assertEquals(7, binarySparseInstance0.numAttributes());
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals((-0.25), binarySparseInstance0.weight(), 0.01);
      assertEquals(6, sparseInstance0.numValues());
      assertEquals((-0.25), sparseInstance0.weight(), 0.01);
      assertEquals(7, sparseInstance0.numAttributes());
      
      double[] doubleArray1 = new double[2];
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      
      doubleArray1[0] = (double) (-1);
      doubleArray1[1] = (double) 0;
      try { 
        evaluation0.evaluationForSingleInstance(doubleArray1, binarySparseInstance0, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 6
  /*Coverage entropy=1.859190220565926
  */
  @Test(timeout = 4000)
  public void test006()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      
      M5Rules m5Rules0 = new M5Rules();
      assertNotNull(m5Rules0);
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      
      Capabilities capabilities0 = m5Rules0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      
      Instances instances0 = testInstances0.generate("getClass");
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      Capabilities capabilities1 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities1);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(capabilities1.hasDependencies());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities1, capabilities0);
      
      capabilities0.assign(capabilities1);
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(capabilities1.hasDependencies());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      boolean boolean1 = FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      
      evaluation0.setPriors(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(capabilities0, capabilities1);
      
      KDTree kDTree0 = new KDTree();
      assertNotNull(kDTree0);
      assertEquals(0, KDTree.MIN);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(1, KDTree.MAX);
      assertEquals(0.0, kDTree0.measureNumLeaves(), 0.01);
      assertFalse(kDTree0.getMeasurePerformance());
      assertEquals(0.0, kDTree0.measureMaxDepth(), 0.01);
      assertEquals("The max number of instances in a leaf.", kDTree0.maxInstInLeafTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree0.distanceFunctionTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree0.measurePerformanceTipText());
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree0.normalizeNodeWidthTipText());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree0.nodeSplitterTipText());
      assertEquals(0.01, kDTree0.getMinBoxRelWidth(), 0.01);
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree0.minBoxRelWidthTipText());
      assertEquals(40, kDTree0.getMaxInstInLeaf());
      assertEquals(0.0, kDTree0.measureTreeSize(), 0.01);
      assertTrue(kDTree0.getNormalizeNodeWidth());
      
      KDTree kDTree1 = new KDTree();
      assertNotNull(kDTree1);
      assertEquals(1, KDTree.MAX);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(0, KDTree.MIN);
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree1.measurePerformanceTipText());
      assertEquals(0.0, kDTree1.measureNumLeaves(), 0.01);
      assertEquals(40, kDTree1.getMaxInstInLeaf());
      assertEquals(0.0, kDTree1.measureTreeSize(), 0.01);
      assertTrue(kDTree1.getNormalizeNodeWidth());
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree1.normalizeNodeWidthTipText());
      assertEquals(0.01, kDTree1.getMinBoxRelWidth(), 0.01);
      assertFalse(kDTree1.getMeasurePerformance());
      assertEquals("The max number of instances in a leaf.", kDTree1.maxInstInLeafTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree1.distanceFunctionTipText());
      assertEquals(0.0, kDTree1.measureMaxDepth(), 0.01);
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree1.minBoxRelWidthTipText());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree1.nodeSplitterTipText());
      assertFalse(kDTree1.equals((Object)kDTree0));
      
      kDTree1.setMinBoxRelWidth(0);
      assertEquals(1, KDTree.MAX);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(0, KDTree.MIN);
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree1.measurePerformanceTipText());
      assertEquals(0.0, kDTree1.measureNumLeaves(), 0.01);
      assertEquals(40, kDTree1.getMaxInstInLeaf());
      assertEquals(0.0, kDTree1.measureTreeSize(), 0.01);
      assertEquals(0.0, kDTree1.getMinBoxRelWidth(), 0.01);
      assertTrue(kDTree1.getNormalizeNodeWidth());
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree1.normalizeNodeWidthTipText());
      assertFalse(kDTree1.getMeasurePerformance());
      assertEquals("The max number of instances in a leaf.", kDTree1.maxInstInLeafTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree1.distanceFunctionTipText());
      assertEquals(0.0, kDTree1.measureMaxDepth(), 0.01);
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree1.minBoxRelWidthTipText());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree1.nodeSplitterTipText());
      assertFalse(kDTree1.equals((Object)kDTree0));
      assertNotSame(kDTree1, kDTree0);
      
      CoverTree coverTree0 = new CoverTree();
      assertNotNull(coverTree0);
      assertEquals(1.3, coverTree0.getBase(), 0.01);
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertFalse(coverTree0.getMeasurePerformance());
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01);
      
      JSONLoader jSONLoader0 = new JSONLoader();
      assertNotNull(jSONLoader0);
      assertEquals("http://", jSONLoader0.retrieveURL());
      assertEquals("Use relative rather than absolute paths", jSONLoader0.useRelativePathTipText());
      assertEquals("JSON Instances files", jSONLoader0.getFileDescription());
      assertEquals(".json", jSONLoader0.getFileExtension());
      assertFalse(jSONLoader0.getUseRelativePath());
      
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      assertNotNull(gridBagLayout0);
      
      double[] doubleArray0 = evaluation0.makeDistribution(1);
      assertNotNull(doubleArray0);
      assertEquals(1, doubleArray0.length);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertArrayEquals(new double[] {1.0}, doubleArray0, 0.01);
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(capabilities0, capabilities1);
  }

  /**
  //Test case number: 7
  /*Coverage entropy=2.255850278960495
  */
  @Test(timeout = 4000)
  public void test007()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances0);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      
      DenseInstance denseInstance0 = new DenseInstance(25);
      assertNotNull(denseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(25, denseInstance0.numValues());
      assertEquals(25, denseInstance0.numAttributes());
      
      Instances instances1 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader0);
      assertNotNull(instances1);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0, instances1.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(1, instances1.classIndex());
      assertEquals(0, instances1.numInstances());
      assertEquals(2, instances1.numAttributes());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertTrue(instances1.equals((Object)instances0));
      assertNotSame(instances1, instances0);
      
      Instances instances2 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances2);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(1, instances2.classIndex());
      assertEquals(0, instances2.numClasses());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertTrue(instances2.equals((Object)instances0));
      assertNotSame(instances2, instances0);
      assertSame(instances2, instances1);
      
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      instances2.sort(comparator0);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(1, instances2.classIndex());
      assertEquals(0, instances2.numClasses());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertTrue(instances2.equals((Object)instances0));
      assertNotSame(instances2, instances0);
      assertSame(instances2, instances1);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertTrue(instances0.equals((Object)instances1));
      assertTrue(instances0.equals((Object)instances2));
      
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(16);
      assertNotNull(binarySparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(16, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(16, binarySparseInstance0.numAttributes());
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(16, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(16, binarySparseInstance0.numAttributes());
      assertEquals(16, binarySparseInstance1.numAttributes());
      assertEquals(16, binarySparseInstance1.numValues());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      binarySparseInstance1.deleteAttributeAt(2);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(16, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(16, binarySparseInstance0.numAttributes());
      assertEquals(15, binarySparseInstance1.numAttributes());
      assertEquals(15, binarySparseInstance1.numValues());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance1));
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      assertNotSame(binarySparseInstance0, binarySparseInstance1);
      assertNotSame(binarySparseInstance1, binarySparseInstance0);
      
      evaluation0.setPriors(instances0);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertTrue(instances0.equals((Object)instances1));
      assertTrue(instances0.equals((Object)instances2));
      assertNotSame(instances0, instances2);
      assertNotSame(instances0, instances1);
      
      KDTree kDTree0 = new KDTree();
      assertNotNull(kDTree0);
      assertEquals(1, KDTree.MAX);
      assertEquals(0, KDTree.MIN);
      assertEquals(2, KDTree.WIDTH);
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree0.normalizeNodeWidthTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree0.distanceFunctionTipText());
      assertEquals(0.0, kDTree0.measureMaxDepth(), 0.01);
      assertFalse(kDTree0.getMeasurePerformance());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree0.minBoxRelWidthTipText());
      assertTrue(kDTree0.getNormalizeNodeWidth());
      assertEquals("The max number of instances in a leaf.", kDTree0.maxInstInLeafTipText());
      assertEquals(0.0, kDTree0.measureTreeSize(), 0.01);
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree0.nodeSplitterTipText());
      assertEquals(40, kDTree0.getMaxInstInLeaf());
      assertEquals(0.0, kDTree0.measureNumLeaves(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree0.measurePerformanceTipText());
      assertEquals(0.01, kDTree0.getMinBoxRelWidth(), 0.01);
      
      MedianOfWidestDimension medianOfWidestDimension0 = new MedianOfWidestDimension();
      assertNotNull(medianOfWidestDimension0);
      assertEquals(1, KDTreeNodeSplitter.MAX);
      assertEquals(0, KDTreeNodeSplitter.MIN);
      assertEquals(2, KDTreeNodeSplitter.WIDTH);
      
      kDTree0.setNodeSplitter(medianOfWidestDimension0);
      assertEquals(1, KDTree.MAX);
      assertEquals(0, KDTree.MIN);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(1, KDTreeNodeSplitter.MAX);
      assertEquals(0, KDTreeNodeSplitter.MIN);
      assertEquals(2, KDTreeNodeSplitter.WIDTH);
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree0.normalizeNodeWidthTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree0.distanceFunctionTipText());
      assertEquals(0.0, kDTree0.measureMaxDepth(), 0.01);
      assertFalse(kDTree0.getMeasurePerformance());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree0.minBoxRelWidthTipText());
      assertTrue(kDTree0.getNormalizeNodeWidth());
      assertEquals("The max number of instances in a leaf.", kDTree0.maxInstInLeafTipText());
      assertEquals(0.0, kDTree0.measureTreeSize(), 0.01);
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree0.nodeSplitterTipText());
      assertEquals(40, kDTree0.getMaxInstInLeaf());
      assertEquals(0.0, kDTree0.measureNumLeaves(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree0.measurePerformanceTipText());
      assertEquals(0.01, kDTree0.getMinBoxRelWidth(), 0.01);
      
      KDTree kDTree1 = new KDTree();
      assertNotNull(kDTree1);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(0, KDTree.MIN);
      assertEquals(1, KDTree.MAX);
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree1.normalizeNodeWidthTipText());
      assertEquals(40, kDTree1.getMaxInstInLeaf());
      assertEquals(0.01, kDTree1.getMinBoxRelWidth(), 0.01);
      assertEquals(0.0, kDTree1.measureNumLeaves(), 0.01);
      assertEquals(0.0, kDTree1.measureTreeSize(), 0.01);
      assertTrue(kDTree1.getNormalizeNodeWidth());
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree1.measurePerformanceTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree1.distanceFunctionTipText());
      assertEquals(0.0, kDTree1.measureMaxDepth(), 0.01);
      assertEquals("The max number of instances in a leaf.", kDTree1.maxInstInLeafTipText());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree1.nodeSplitterTipText());
      assertFalse(kDTree1.getMeasurePerformance());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree1.minBoxRelWidthTipText());
      assertFalse(kDTree1.equals((Object)kDTree0));
      
      kDTree1.setMinBoxRelWidth(2);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(0, KDTree.MIN);
      assertEquals(1, KDTree.MAX);
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree1.normalizeNodeWidthTipText());
      assertEquals(40, kDTree1.getMaxInstInLeaf());
      assertEquals(0.0, kDTree1.measureNumLeaves(), 0.01);
      assertEquals(0.0, kDTree1.measureTreeSize(), 0.01);
      assertTrue(kDTree1.getNormalizeNodeWidth());
      assertEquals(2.0, kDTree1.getMinBoxRelWidth(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree1.measurePerformanceTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree1.distanceFunctionTipText());
      assertEquals(0.0, kDTree1.measureMaxDepth(), 0.01);
      assertEquals("The max number of instances in a leaf.", kDTree1.maxInstInLeafTipText());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree1.nodeSplitterTipText());
      assertFalse(kDTree1.getMeasurePerformance());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree1.minBoxRelWidthTipText());
      assertFalse(kDTree1.equals((Object)kDTree0));
      assertNotSame(kDTree1, kDTree0);
      
      CoverTree coverTree0 = new CoverTree();
      assertNotNull(coverTree0);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01);
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01);
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01);
      assertEquals(1.3, coverTree0.getBase(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertFalse(coverTree0.getMeasurePerformance());
      
      JSONLoader jSONLoader0 = new JSONLoader();
      assertNotNull(jSONLoader0);
      assertEquals(".json", jSONLoader0.getFileExtension());
      assertEquals("JSON Instances files", jSONLoader0.getFileDescription());
      assertEquals("http://", jSONLoader0.retrieveURL());
      assertFalse(jSONLoader0.getUseRelativePath());
      assertEquals("Use relative rather than absolute paths", jSONLoader0.useRelativePathTipText());
      
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      assertNotNull(gridBagLayout0);
      
      IBk iBk0 = new IBk();
      assertNotNull(iBk0);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals(1, iBk0.getKNN());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertFalse(iBk0.getDebug());
      assertEquals(0, iBk0.getWindowSize());
      
      String string0 = Evaluation.makeOptionString(iBk0, true);
      assertNotNull(string0);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals(1, iBk0.getKNN());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertFalse(iBk0.getDebug());
      assertEquals(0, iBk0.getWindowSize());
      
      try { 
        evaluation0.evaluateModelOnceAndRecordPrediction((double[]) null, (Instance) denseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Utils", e);
      }
  }

  /**
  //Test case number: 8
  /*Coverage entropy=2.337827080898473
  */
  @Test(timeout = 4000)
  public void test008()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances0);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      
      boolean boolean0 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "@data");
      assertFalse(boolean0);
      
      TextDirectoryLoader textDirectoryLoader1 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader1);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      
      Instances instances1 = textDirectoryLoader1.getDataSet();
      assertNotNull(instances1);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(1, instances1.classIndex());
      assertEquals(0, instances1.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertEquals(0, instances1.numInstances());
      assertEquals(2, instances1.numAttributes());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertTrue(instances1.equals((Object)instances0));
      assertNotSame(textDirectoryLoader1, textDirectoryLoader0);
      assertNotSame(instances1, instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertTrue(instances0.equals((Object)instances1));
      
      double double0 = evaluation0.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertTrue(instances0.equals((Object)instances1));
      assertNotSame(textDirectoryLoader0, textDirectoryLoader1);
      assertNotSame(instances0, instances1);
      
      double double1 = evaluation0.weightedTruePositiveRate();
      assertEquals(Double.NaN, double1, 0.01);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertTrue(instances0.equals((Object)instances1));
      assertEquals(double1, double0, 0.01);
      assertNotSame(textDirectoryLoader0, textDirectoryLoader1);
      assertNotSame(instances0, instances1);
      
      double double2 = evaluation0.sizeOfPredictedRegions();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertTrue(instances0.equals((Object)instances1));
      assertEquals(double2, double1, 0.01);
      assertEquals(double2, double0, 0.01);
      assertNotSame(textDirectoryLoader0, textDirectoryLoader1);
      assertNotSame(instances0, instances1);
      
      double[] doubleArray0 = evaluation0.makeDistribution(Double.NaN);
      assertNotNull(doubleArray0);
      assertEquals(0, doubleArray0.length);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertTrue(instances0.equals((Object)instances1));
      assertNotSame(textDirectoryLoader0, textDirectoryLoader1);
      assertNotSame(instances0, instances1);
  }

  /**
  //Test case number: 9
  /*Coverage entropy=2.568585947787186
  */
  @Test(timeout = 4000)
  public void test009()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertNotNull(instances0);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      
      String string0 = evaluation0.toCumulativeMarginDistributionString();
      assertEquals(" -1       0    \n", string0);
      assertNotNull(string0);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      
      String string1 = evaluation0.toMatrixString();
      assertEquals("=== Confusion Matrix ===\n\n   <-- classified as\n", string1);
      assertNotNull(string1);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertFalse(string1.equals((Object)string0));
      
      double double0 = evaluation0.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      
      CSVLoader cSVLoader0 = new CSVLoader();
      assertNotNull(cSVLoader0);
      assertEquals("Use relative rather than absolute paths", cSVLoader0.useRelativePathTipText());
      assertFalse(cSVLoader0.getUseRelativePath());
      assertEquals("CSV data files", cSVLoader0.getFileDescription());
      assertFalse(cSVLoader0.getNoHeaderRowPresent());
      assertEquals("The characters to use as enclosures for strings. E.g. \",'", cSVLoader0.enclosureCharactersTipText());
      assertEquals("First row of data does not contain attribute names", cSVLoader0.noHeaderRowPresentTipText());
      assertEquals("?", cSVLoader0.getMissingValue());
      assertEquals("The range of attributes to force to be of type NOMINAL, example ranges: 'first-last', '1,4,7-14,50-last'.", cSVLoader0.nominalAttributesTipText());
      assertEquals("The range of attributes to force to be of type STRING, example ranges: 'first-last', '1,4,7-14,50-last'.", cSVLoader0.stringAttributesTipText());
      assertEquals("The placeholder for missing values, default is '?'.", cSVLoader0.missingValueTipText());
      assertEquals("\",'", cSVLoader0.getEnclosureCharacters());
      assertEquals("The character to use as separator for the columns/fields (use '\\t' for TAB).", cSVLoader0.fieldSeparatorTipText());
      assertEquals("Reads a source that is in comma separated format (the default). One can also change the column separator from comma to tab or another character. Assumes that the first row in the file determines the number of and names of the attributes.", cSVLoader0.globalInfo());
      assertEquals("", cSVLoader0.getDateFormat());
      assertEquals("The format to use for parsing date values.", cSVLoader0.dateFormatTipText());
      assertEquals("The range of attributes to force to type STRING, example ranges: 'first-last', '1,4,7-14, 50-last'.", cSVLoader0.dateAttributesTipText());
      assertEquals(".csv", cSVLoader0.getFileExtension());
      
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      assertNotNull(gridBagLayout0);
      
      Box box0 = Box.createVerticalBox();
      assertNotNull(box0);
      assertTrue(box0.getFocusTraversalKeysEnabled());
      assertFalse(box0.isFocusCycleRoot());
      assertFalse(box0.isFocusTraversalPolicyProvider());
      assertFalse(box0.isFocusTraversalPolicySet());
      assertFalse(box0.getIgnoreRepaint());
      
      Dimension dimension0 = gridBagLayout0.preferredLayoutSize(box0);
      assertNotNull(dimension0);
      assertEquals(0, dimension0.height);
      assertEquals(0, dimension0.width);
      assertTrue(box0.getFocusTraversalKeysEnabled());
      assertFalse(box0.isFocusCycleRoot());
      assertFalse(box0.isFocusTraversalPolicyProvider());
      assertFalse(box0.isFocusTraversalPolicySet());
      assertFalse(box0.getIgnoreRepaint());
      assertEquals(0.0, dimension0.getWidth(), 0.01);
      assertEquals(0.0, dimension0.getHeight(), 0.01);
      
      GridBagLayout gridBagLayout1 = new GridBagLayout();
      assertNotNull(gridBagLayout1);
      assertFalse(gridBagLayout1.equals((Object)gridBagLayout0));
      
      Point point0 = gridBagLayout0.getLayoutOrigin();
      assertNotNull(point0);
      assertEquals(0, point0.y);
      assertEquals(0, point0.x);
      assertEquals(0.0, point0.getX(), 0.01);
      assertEquals(0.0, point0.getY(), 0.01);
      assertFalse(gridBagLayout0.equals((Object)gridBagLayout1));
      assertNotSame(gridBagLayout0, gridBagLayout1);
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      double double1 = evaluation0.falseNegativeRate(2711);
      assertEquals(0.0, double1, 0.01);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertNotEquals(double1, double0, 0.01);
      
      double double2 = evaluation0.totalCost();
      assertEquals(0.0, double2, 0.01);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(double2, double1, 0.01);
      assertNotEquals(double2, double0, 0.01);
      
      double double3 = evaluation0.meanAbsoluteError();
      assertEquals(Double.NaN, double3, 0.01);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(double3, double0, 0.01);
      assertNotEquals(double3, double1, 0.01);
      assertNotEquals(double3, double2, 0.01);
  }

  /**
  //Test case number: 10
  /*Coverage entropy=2.0608333657846836
  */
  @Test(timeout = 4000)
  public void test010()  throws Throwable  {
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      boolean boolean1 = FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier3);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      
      NormalEstimator normalEstimator0 = new NormalEstimator(2);
      assertNotNull(normalEstimator0);
      assertFalse(normalEstimator0.getDebug());
      assertEquals(0.0, normalEstimator0.getSumOfWeights(), 0.01);
      assertEquals(0.3333333333333333, normalEstimator0.getStdDev(), 0.01);
      assertEquals(2.0, normalEstimator0.getPrecision(), 0.01);
      assertEquals(0.0, normalEstimator0.getMean(), 0.01);
      assertEquals("If set to true, estimator may output additional info to the console.", normalEstimator0.debugTipText());
      
      Capabilities capabilities1 = normalEstimator0.getCapabilities();
      assertNotNull(capabilities1);
      assertFalse(normalEstimator0.getDebug());
      assertEquals(0.0, normalEstimator0.getSumOfWeights(), 0.01);
      assertEquals(0.3333333333333333, normalEstimator0.getStdDev(), 0.01);
      assertEquals(2.0, normalEstimator0.getPrecision(), 0.01);
      assertEquals(0.0, normalEstimator0.getMean(), 0.01);
      assertEquals("If set to true, estimator may output additional info to the console.", normalEstimator0.debugTipText());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities1, capabilities0);
      
      capabilities1.enableAll();
      assertFalse(normalEstimator0.getDebug());
      assertEquals(0.0, normalEstimator0.getSumOfWeights(), 0.01);
      assertEquals(0.3333333333333333, normalEstimator0.getStdDev(), 0.01);
      assertEquals(2.0, normalEstimator0.getPrecision(), 0.01);
      assertEquals(0.0, normalEstimator0.getMean(), 0.01);
      assertEquals("If set to true, estimator may output additional info to the console.", normalEstimator0.debugTipText());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertTrue(capabilities1.hasDependencies());
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities1, capabilities0);
      
      boolean boolean2 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean2);
      assertTrue(boolean2 == boolean1);
      assertTrue(boolean2 == boolean0);
      
      double[] doubleArray0 = new double[2];
      doubleArray0[0] = 2.0;
      doubleArray0[1] = (double) 1;
      int[] intArray0 = new int[3];
      intArray0[0] = 1;
      intArray0[1] = 1;
      intArray0[2] = 1;
      SparseInstance sparseInstance0 = new SparseInstance(4.8, doubleArray0, intArray0, 1);
      assertNotNull(sparseInstance0);
      assertEquals(3, intArray0.length);
      assertEquals(2, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new int[] {1, 1, 1}, intArray0);
      assertArrayEquals(new double[] {2.0, 1.0}, doubleArray0, 0.01);
      assertEquals(1, sparseInstance0.numAttributes());
      assertEquals(3, sparseInstance0.numValues());
      assertEquals(4.8, sparseInstance0.weight(), 0.01);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      
      double double0 = evaluation0.relativeAbsoluteError();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(capabilities0, capabilities1);
      
      double double1 = evaluation0.numFalsePositives(2);
      assertEquals(0.0, double1, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotEquals(double1, double0, 0.01);
      assertNotSame(capabilities0, capabilities1);
  }

  /**
  //Test case number: 11
  /*Coverage entropy=2.1995219101255112
  */
  @Test(timeout = 4000)
  public void test011()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      
      boolean boolean0 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "@data");
      assertFalse(boolean0);
      
      TextDirectoryLoader textDirectoryLoader1 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader1);
      assertFalse(textDirectoryLoader1.getDebug());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      
      Instances instances1 = textDirectoryLoader1.getDataSet();
      assertNotNull(instances1);
      assertFalse(textDirectoryLoader1.getDebug());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals(0, instances1.numClasses());
      assertEquals(1, instances1.classIndex());
      assertEquals(0, instances1.numInstances());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(0, instances1.size());
      assertEquals(2, instances1.numAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertTrue(instances1.equals((Object)instances0));
      assertNotSame(textDirectoryLoader1, textDirectoryLoader0);
      assertNotSame(instances1, instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertTrue(instances0.equals((Object)instances1));
      
      evaluation0.m_CoverageStatisticsAvailable = false;
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      
      double double0 = evaluation0.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertTrue(instances0.equals((Object)instances1));
      assertNotSame(textDirectoryLoader0, textDirectoryLoader1);
      assertNotSame(instances0, instances1);
      
      double double1 = evaluation0.weightedTruePositiveRate();
      assertEquals(Double.NaN, double1, 0.01);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertTrue(instances0.equals((Object)instances1));
      assertEquals(double1, double0, 0.01);
      assertNotSame(textDirectoryLoader0, textDirectoryLoader1);
      assertNotSame(instances0, instances1);
      
      double double2 = evaluation0.sizeOfPredictedRegions();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertTrue(instances0.equals((Object)instances1));
      assertEquals(double2, double1, 0.01);
      assertEquals(double2, double0, 0.01);
      assertNotSame(textDirectoryLoader0, textDirectoryLoader1);
      assertNotSame(instances0, instances1);
  }

  /**
  //Test case number: 12
  /*Coverage entropy=0.3333333333333333
  */
  @Test(timeout = 4000)
  public void test012()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals(1, lMT0.graphType());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals(1, lMT0.graphType());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      assertNotNull(adaBoostM1_0);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      
      Capabilities capabilities1 = adaBoostM1_0.getCapabilities();
      assertNotNull(capabilities1);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities1, capabilities0);
      
      RegressionByDiscretization regressionByDiscretization0 = new RegressionByDiscretization();
      assertNotNull(regressionByDiscretization0);
      assertEquals(0, RegressionByDiscretization.ESTIMATOR_HISTOGRAM);
      assertEquals(2, RegressionByDiscretization.ESTIMATOR_NORMAL);
      assertEquals(1, RegressionByDiscretization.ESTIMATOR_KERNEL);
      assertFalse(regressionByDiscretization0.getMinimizeAbsoluteError());
      assertFalse(regressionByDiscretization0.getDeleteEmptyBins());
      assertEquals("The density estimator to use.", regressionByDiscretization0.estimatorTypeTipText());
      assertFalse(regressionByDiscretization0.getUseEqualFrequency());
      assertEquals("Whether to delete empty bins after discretization.", regressionByDiscretization0.deleteEmptyBinsTipText());
      assertFalse(regressionByDiscretization0.getDebug());
      assertEquals("The base classifier to be used.", regressionByDiscretization0.classifierTipText());
      assertEquals("Number of bins for discretization.", regressionByDiscretization0.numBinsTipText());
      assertEquals(10, regressionByDiscretization0.getNumBins());
      assertEquals("If set to true, equal-frequency binning will be used instead of equal-width binning.", regressionByDiscretization0.useEqualFrequencyTipText());
      assertEquals("Whether to minimize absolute error.", regressionByDiscretization0.minimizeAbsoluteErrorTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", regressionByDiscretization0.debugTipText());
      
      Capabilities capabilities2 = regressionByDiscretization0.getCapabilities();
      assertNotNull(capabilities2);
      assertEquals(0, RegressionByDiscretization.ESTIMATOR_HISTOGRAM);
      assertEquals(2, RegressionByDiscretization.ESTIMATOR_NORMAL);
      assertEquals(1, RegressionByDiscretization.ESTIMATOR_KERNEL);
      assertFalse(regressionByDiscretization0.getMinimizeAbsoluteError());
      assertFalse(regressionByDiscretization0.getDeleteEmptyBins());
      assertEquals("The density estimator to use.", regressionByDiscretization0.estimatorTypeTipText());
      assertFalse(regressionByDiscretization0.getUseEqualFrequency());
      assertEquals("Whether to delete empty bins after discretization.", regressionByDiscretization0.deleteEmptyBinsTipText());
      assertFalse(regressionByDiscretization0.getDebug());
      assertEquals("The base classifier to be used.", regressionByDiscretization0.classifierTipText());
      assertEquals("Number of bins for discretization.", regressionByDiscretization0.numBinsTipText());
      assertEquals(10, regressionByDiscretization0.getNumBins());
      assertEquals("If set to true, equal-frequency binning will be used instead of equal-width binning.", regressionByDiscretization0.useEqualFrequencyTipText());
      assertEquals("Whether to minimize absolute error.", regressionByDiscretization0.minimizeAbsoluteErrorTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", regressionByDiscretization0.debugTipText());
      assertTrue(capabilities2.hasDependencies());
      assertEquals(2, capabilities2.getMinimumNumberInstances());
      assertFalse(capabilities2.equals((Object)capabilities0));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertNotSame(capabilities2, capabilities0);
      assertNotSame(capabilities2, capabilities1);
      
      Capabilities capabilities3 = capabilities0.getAttributeCapabilities();
      assertNotNull(capabilities3);
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals(1, lMT0.graphType());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities3.getMinimumNumberInstances());
      assertFalse(capabilities3.hasDependencies());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities3.equals((Object)capabilities1));
      assertFalse(capabilities3.equals((Object)capabilities0));
      assertFalse(capabilities3.equals((Object)capabilities2));
      assertNotSame(capabilities0, capabilities3);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities3, capabilities1);
      assertNotSame(capabilities3, capabilities0);
      assertNotSame(capabilities3, capabilities2);
      
      ReliefFAttributeEval reliefFAttributeEval0 = new ReliefFAttributeEval();
      assertNotNull(reliefFAttributeEval0);
      assertEquals("Number of nearest neighbours for attribute estimation.", reliefFAttributeEval0.numNeighboursTipText());
      assertEquals(2, reliefFAttributeEval0.getSigma());
      assertFalse(reliefFAttributeEval0.getWeightByDistance());
      assertEquals("Weight nearest neighbours by their distance.", reliefFAttributeEval0.weightByDistanceTipText());
      assertEquals("Set influence of nearest neighbours. Used in an exp function to control how quickly weights decrease for more distant instances. Use in conjunction with weightByDistance. Sensible values = 1/5 to 1/10 the number of nearest neighbours.", reliefFAttributeEval0.sigmaTipText());
      assertEquals("Number of instances to sample. Default (-1) indicates that all instances will be used for attribute estimation.", reliefFAttributeEval0.sampleSizeTipText());
      assertEquals(1, reliefFAttributeEval0.getSeed());
      assertEquals(10, reliefFAttributeEval0.getNumNeighbours());
      assertEquals((-1), reliefFAttributeEval0.getSampleSize());
      assertEquals("Random seed for sampling instances.", reliefFAttributeEval0.seedTipText());
      
      Capabilities capabilities4 = reliefFAttributeEval0.getCapabilities();
      assertNotNull(capabilities4);
      assertEquals("Number of nearest neighbours for attribute estimation.", reliefFAttributeEval0.numNeighboursTipText());
      assertEquals(2, reliefFAttributeEval0.getSigma());
      assertFalse(reliefFAttributeEval0.getWeightByDistance());
      assertEquals("Weight nearest neighbours by their distance.", reliefFAttributeEval0.weightByDistanceTipText());
      assertEquals("Set influence of nearest neighbours. Used in an exp function to control how quickly weights decrease for more distant instances. Use in conjunction with weightByDistance. Sensible values = 1/5 to 1/10 the number of nearest neighbours.", reliefFAttributeEval0.sigmaTipText());
      assertEquals("Number of instances to sample. Default (-1) indicates that all instances will be used for attribute estimation.", reliefFAttributeEval0.sampleSizeTipText());
      assertEquals(1, reliefFAttributeEval0.getSeed());
      assertEquals(10, reliefFAttributeEval0.getNumNeighbours());
      assertEquals((-1), reliefFAttributeEval0.getSampleSize());
      assertEquals("Random seed for sampling instances.", reliefFAttributeEval0.seedTipText());
      assertFalse(capabilities4.hasDependencies());
      assertEquals(1, capabilities4.getMinimumNumberInstances());
      assertFalse(capabilities4.equals((Object)capabilities3));
      assertFalse(capabilities4.equals((Object)capabilities2));
      assertFalse(capabilities4.equals((Object)capabilities1));
      assertFalse(capabilities4.equals((Object)capabilities0));
      assertNotSame(capabilities4, capabilities3);
      assertNotSame(capabilities4, capabilities2);
      assertNotSame(capabilities4, capabilities1);
      assertNotSame(capabilities4, capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities2);
      assertNotNull(testInstances0);
      assertEquals(0, RegressionByDiscretization.ESTIMATOR_HISTOGRAM);
      assertEquals(2, RegressionByDiscretization.ESTIMATOR_NORMAL);
      assertEquals(1, RegressionByDiscretization.ESTIMATOR_KERNEL);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(regressionByDiscretization0.getMinimizeAbsoluteError());
      assertFalse(regressionByDiscretization0.getDeleteEmptyBins());
      assertEquals("The density estimator to use.", regressionByDiscretization0.estimatorTypeTipText());
      assertFalse(regressionByDiscretization0.getUseEqualFrequency());
      assertEquals("Whether to delete empty bins after discretization.", regressionByDiscretization0.deleteEmptyBinsTipText());
      assertFalse(regressionByDiscretization0.getDebug());
      assertEquals("The base classifier to be used.", regressionByDiscretization0.classifierTipText());
      assertEquals("Number of bins for discretization.", regressionByDiscretization0.numBinsTipText());
      assertEquals(10, regressionByDiscretization0.getNumBins());
      assertEquals("If set to true, equal-frequency binning will be used instead of equal-width binning.", regressionByDiscretization0.useEqualFrequencyTipText());
      assertEquals("Whether to minimize absolute error.", regressionByDiscretization0.minimizeAbsoluteErrorTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", regressionByDiscretization0.debugTipText());
      assertTrue(capabilities2.hasDependencies());
      assertEquals(2, capabilities2.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(capabilities2.equals((Object)capabilities3));
      assertFalse(capabilities2.equals((Object)capabilities0));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertFalse(capabilities2.equals((Object)capabilities4));
      assertNotSame(capabilities2, capabilities3);
      assertNotSame(capabilities2, capabilities0);
      assertNotSame(capabilities2, capabilities1);
      assertNotSame(capabilities2, capabilities4);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals(0, RegressionByDiscretization.ESTIMATOR_HISTOGRAM);
      assertEquals(2, RegressionByDiscretization.ESTIMATOR_NORMAL);
      assertEquals(1, RegressionByDiscretization.ESTIMATOR_KERNEL);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(regressionByDiscretization0.getMinimizeAbsoluteError());
      assertFalse(regressionByDiscretization0.getDeleteEmptyBins());
      assertEquals("The density estimator to use.", regressionByDiscretization0.estimatorTypeTipText());
      assertFalse(regressionByDiscretization0.getUseEqualFrequency());
      assertEquals("Whether to delete empty bins after discretization.", regressionByDiscretization0.deleteEmptyBinsTipText());
      assertFalse(regressionByDiscretization0.getDebug());
      assertEquals("The base classifier to be used.", regressionByDiscretization0.classifierTipText());
      assertEquals("Number of bins for discretization.", regressionByDiscretization0.numBinsTipText());
      assertEquals(10, regressionByDiscretization0.getNumBins());
      assertEquals("If set to true, equal-frequency binning will be used instead of equal-width binning.", regressionByDiscretization0.useEqualFrequencyTipText());
      assertEquals("Whether to minimize absolute error.", regressionByDiscretization0.minimizeAbsoluteErrorTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", regressionByDiscretization0.debugTipText());
      assertTrue(capabilities2.hasDependencies());
      assertEquals(2, capabilities2.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numAttributes());
      assertFalse(capabilities2.equals((Object)capabilities3));
      assertFalse(capabilities2.equals((Object)capabilities0));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertFalse(capabilities2.equals((Object)capabilities4));
      assertNotSame(capabilities2, capabilities3);
      assertNotSame(capabilities2, capabilities0);
      assertNotSame(capabilities2, capabilities1);
      assertNotSame(capabilities2, capabilities4);
      
      AlphabeticTokenizer alphabeticTokenizer1 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer1);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer1.globalInfo());
      assertFalse(alphabeticTokenizer1.equals((Object)alphabeticTokenizer0));
      
      capabilities1.enableAll();
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.equals((Object)capabilities4));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(capabilities1.equals((Object)capabilities2));
      assertFalse(capabilities1.equals((Object)capabilities3));
      assertNotSame(capabilities1, capabilities4);
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(capabilities1, capabilities2);
      assertNotSame(capabilities1, capabilities3);
      
      boolean boolean1 = FileSystemHandling.setPermissions((EvoSuiteFile) null, true, false, true);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      
      CostMatrix costMatrix0 = costSensitiveClassifier0.getCostMatrix();
      assertNotNull(costMatrix0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertEquals(1, costMatrix0.numRows());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.size());
      
      Evaluation evaluation0 = null;
      try {
        evaluation0 = new Evaluation(instances0, costMatrix0);
        fail("Expecting exception: Exception");
      
      } catch(Throwable e) {
         //
         // Class has to be nominal if cost matrix given!
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 13
  /*Coverage entropy=1.8892845262264077
  */
  @Test(timeout = 4000)
  public void test013()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      
      M5Rules m5Rules0 = new M5Rules();
      assertNotNull(m5Rules0);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      
      Capabilities capabilities0 = m5Rules0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      MultiClassClassifierUpdateable multiClassClassifierUpdateable0 = new MultiClassClassifierUpdateable();
      assertNotNull(multiClassClassifierUpdateable0);
      assertEquals(0, MultiClassClassifier.METHOD_1_AGAINST_ALL);
      assertEquals(3, MultiClassClassifier.METHOD_1_AGAINST_1);
      assertEquals(1, MultiClassClassifier.METHOD_ERROR_RANDOM);
      assertEquals(2, MultiClassClassifier.METHOD_ERROR_EXHAUSTIVE);
      assertEquals("The random number seed to be used.", multiClassClassifierUpdateable0.seedTipText());
      assertEquals("Sets the method to use for transforming the multi-class problem into several 2-class ones.", multiClassClassifierUpdateable0.methodTipText());
      assertFalse(multiClassClassifierUpdateable0.getUsePairwiseCoupling());
      assertEquals(2.0, multiClassClassifierUpdateable0.getRandomWidthFactor(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", multiClassClassifierUpdateable0.debugTipText());
      assertEquals("A metaclassifier for handling multi-class datasets with 2-class classifiers. This classifier is also capable of applying error correcting output codes for increased accuracy. The base classifier must be an updateable classifier", multiClassClassifierUpdateable0.globalInfo());
      assertEquals(1, multiClassClassifierUpdateable0.getSeed());
      assertEquals("Use pairwise coupling (only has an effect for 1-against-1).", multiClassClassifierUpdateable0.usePairwiseCouplingTipText());
      assertEquals("Sets the width multiplier when using random codes. The number of codes generated will be thus number multiplied by the number of classes.", multiClassClassifierUpdateable0.randomWidthFactorTipText());
      assertFalse(multiClassClassifierUpdateable0.getDebug());
      assertEquals("The base classifier to be used.", multiClassClassifierUpdateable0.classifierTipText());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      
      Capabilities capabilities1 = lMT0.getCapabilities();
      assertNotNull(capabilities1);
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities1, capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(capabilities0, capabilities1);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(capabilities0, capabilities1);
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier3);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier4 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier4);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertFalse(m5Rules0.getDebug());
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      
      // Undeclared exception!
      try { 
        evaluation0.weightedTrueNegativeRate();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 14
  /*Coverage entropy=1.3368883075390159
  */
  @Test(timeout = 4000)
  public void test014()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      
      DenseInstance denseInstance0 = new DenseInstance(25);
      assertNotNull(denseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(25, denseInstance0.numValues());
      assertEquals(25, denseInstance0.numAttributes());
      
      double double0 = 5.019028201623571E-4;
      double[] doubleArray0 = new double[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(5.019028201623571E-4, doubleArray0);
      assertNotNull(binarySparseInstance0);
      assertEquals(0, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(5.019028201623571E-4, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      
      boolean boolean0 = instances0.add((Instance) binarySparseInstance0);
      assertTrue(boolean0);
      assertEquals(0, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(5.019028201623571E-4, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.size());
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(5.019028201623571E-4, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      
      SparseInstance sparseInstance0 = new SparseInstance(0.0022462733445193627, doubleArray0);
      assertNotNull(sparseInstance0);
      assertEquals(0, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(0, sparseInstance0.numAttributes());
      assertEquals(0.0022462733445193627, sparseInstance0.weight(), 0.01);
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(0, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(5.019028201623571E-4, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals(0, binarySparseInstance1.numAttributes());
      assertEquals(5.019028201623571E-4, binarySparseInstance1.weight(), 0.01);
      assertEquals(0, binarySparseInstance1.numValues());
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      Evaluation evaluation0 = null;
      try {
        evaluation0 = new Evaluation(instances0);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 0
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 15
  /*Coverage entropy=1.6976552253708785
  */
  @Test(timeout = 4000)
  public void test015()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      
      testInstances0.setNumDate(1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      
      CostSensitiveClassifier costSensitiveClassifier3 = (CostSensitiveClassifier)AbstractClassifier.makeCopy(costSensitiveClassifier1);
      assertNotNull(costSensitiveClassifier3);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertNotSame(costSensitiveClassifier1, costSensitiveClassifier2);
      assertNotSame(costSensitiveClassifier1, costSensitiveClassifier3);
      assertNotSame(costSensitiveClassifier1, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier2);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier1);
      
      CostSensitiveClassifier costSensitiveClassifier4 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier4);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      
      CostMatrix costMatrix0 = costSensitiveClassifier4.getCostMatrix();
      assertNotNull(costMatrix0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.size());
      assertEquals(1, costMatrix0.numRows());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier2);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier3);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      
      MockRandom mockRandom0 = new MockRandom();
      assertNotNull(mockRandom0);
      
      double double0 = mockRandom0.nextGaussian();
      assertEquals(0.7, double0, 0.01);
      
      String string0 = costMatrix0.toMatlab();
      assertEquals("[0.0]", string0);
      assertNotNull(string0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.size());
      assertEquals(1, costMatrix0.numRows());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier2);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier3);
      
      MockRandom mockRandom1 = new MockRandom();
      assertNotNull(mockRandom1);
      assertFalse(mockRandom1.equals((Object)mockRandom0));
      
      costSensitiveClassifier2.setCostMatrix(costMatrix0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.size());
      assertEquals(1, costMatrix0.numRows());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier4));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier3);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier4);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier2);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier3);
      
      Evaluation evaluation1 = new Evaluation(instances0);
      assertNotNull(evaluation1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      double double1 = evaluation1.numFalseNegatives(2);
      assertEquals(0.0, double1, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotEquals(double1, double0, 0.01);
      assertNotSame(evaluation1, evaluation0);
      
      Evaluation evaluation2 = null;
      try {
        evaluation2 = new Evaluation(instances0, costMatrix0);
        fail("Expecting exception: Exception");
      
      } catch(Throwable e) {
         //
         // Cost matrix not compatible with data!
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 16
  /*Coverage entropy=1.94843118075652
  */
  @Test(timeout = 4000)
  public void test016()  throws Throwable  {
      Stacking stacking0 = new Stacking();
      assertNotNull(stacking0);
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertEquals(1, stacking0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertFalse(stacking0.getDebug());
      assertEquals(10, stacking0.getNumFolds());
      
      ZeroR zeroR0 = (ZeroR)stacking0.getMetaClassifier();
      assertNotNull(zeroR0);
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertEquals(1, stacking0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertFalse(stacking0.getDebug());
      assertEquals(10, stacking0.getNumFolds());
      assertFalse(zeroR0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", zeroR0.debugTipText());
      assertEquals("Class for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).", zeroR0.globalInfo());
      
      String[] stringArray0 = zeroR0.getOptions();
      assertNotNull(stringArray0);
      assertEquals(0, stringArray0.length);
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertEquals(1, stacking0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertFalse(stacking0.getDebug());
      assertEquals(10, stacking0.getNumFolds());
      assertFalse(zeroR0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", zeroR0.debugTipText());
      assertEquals("Class for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).", zeroR0.globalInfo());
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      WrapperSubsetEval wrapperSubsetEval0 = new WrapperSubsetEval();
      assertNotNull(wrapperSubsetEval0);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      
      SelectedTag selectedTag0 = wrapperSubsetEval0.getEvaluationMeasure();
      assertNotNull(selectedTag0);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      assertEquals("1", selectedTag0.toString());
      
      MultiClassClassifierUpdateable multiClassClassifierUpdateable0 = new MultiClassClassifierUpdateable();
      assertNotNull(multiClassClassifierUpdateable0);
      assertEquals(0, MultiClassClassifier.METHOD_1_AGAINST_ALL);
      assertEquals(2, MultiClassClassifier.METHOD_ERROR_EXHAUSTIVE);
      assertEquals(1, MultiClassClassifier.METHOD_ERROR_RANDOM);
      assertEquals(3, MultiClassClassifier.METHOD_1_AGAINST_1);
      assertEquals("Sets the width multiplier when using random codes. The number of codes generated will be thus number multiplied by the number of classes.", multiClassClassifierUpdateable0.randomWidthFactorTipText());
      assertEquals("The random number seed to be used.", multiClassClassifierUpdateable0.seedTipText());
      assertEquals("Sets the method to use for transforming the multi-class problem into several 2-class ones.", multiClassClassifierUpdateable0.methodTipText());
      assertFalse(multiClassClassifierUpdateable0.getDebug());
      assertEquals("Use pairwise coupling (only has an effect for 1-against-1).", multiClassClassifierUpdateable0.usePairwiseCouplingTipText());
      assertEquals(1, multiClassClassifierUpdateable0.getSeed());
      assertEquals("A metaclassifier for handling multi-class datasets with 2-class classifiers. This classifier is also capable of applying error correcting output codes for increased accuracy. The base classifier must be an updateable classifier", multiClassClassifierUpdateable0.globalInfo());
      assertEquals("The base classifier to be used.", multiClassClassifierUpdateable0.classifierTipText());
      assertEquals(2.0, multiClassClassifierUpdateable0.getRandomWidthFactor(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", multiClassClassifierUpdateable0.debugTipText());
      assertFalse(multiClassClassifierUpdateable0.getUsePairwiseCoupling());
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      
      Capabilities capabilities0 = costSensitiveClassifier2.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier1);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(6, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier1);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(6, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(5, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals(6, instances0.numAttributes());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier1);
      
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier3);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier4 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier4);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      
      CostSensitiveClassifier costSensitiveClassifier5 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier5);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, costSensitiveClassifier5.getSeed());
      assertFalse(costSensitiveClassifier5.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier5.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier5.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier5.seedTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier5.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier5.classifierTipText());
      assertEquals(0, costSensitiveClassifier5.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier5.costMatrixTipText());
      assertFalse(costSensitiveClassifier5.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier5.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier4));
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier6 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier6);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier6.getDebug());
      assertEquals("The random number seed to be used.", costSensitiveClassifier6.seedTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier6.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier6.debugTipText());
      assertFalse(costSensitiveClassifier6.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier6.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier6.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier6.costMatrixTipText());
      assertEquals(0, costSensitiveClassifier6.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier6.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier6.classifierTipText());
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier5));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier4));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier2));
      
      CoverTree coverTree0 = new CoverTree();
      assertNotNull(coverTree0);
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertFalse(coverTree0.getMeasurePerformance());
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01);
      assertEquals(1.3, coverTree0.getBase(), 0.01);
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      
      TextDirectoryLoader textDirectoryLoader1 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader1);
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      
      TextDirectoryLoader textDirectoryLoader2 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader2);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader2.charSetTipText());
      assertEquals("", textDirectoryLoader2.getCharSet());
      assertEquals("Directories", textDirectoryLoader2.getFileDescription());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader2.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader2.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader2.globalInfo());
      assertFalse(textDirectoryLoader2.getOutputFilename());
      assertFalse(textDirectoryLoader2.getDebug());
      assertFalse(textDirectoryLoader2.equals((Object)textDirectoryLoader1));
      assertFalse(textDirectoryLoader2.equals((Object)textDirectoryLoader0));
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      Instances instances1 = textDirectoryLoader2.getDataSet();
      assertNotNull(instances1);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader2.charSetTipText());
      assertEquals("", textDirectoryLoader2.getCharSet());
      assertEquals("Directories", textDirectoryLoader2.getFileDescription());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader2.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader2.outputFilenameTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader2.globalInfo());
      assertFalse(textDirectoryLoader2.getOutputFilename());
      assertFalse(textDirectoryLoader2.getDebug());
      assertEquals(0, instances1.numInstances());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertEquals(2, instances1.numAttributes());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertEquals(1, instances1.classIndex());
      assertEquals(0, instances1.numClasses());
      assertFalse(textDirectoryLoader2.equals((Object)textDirectoryLoader1));
      assertFalse(textDirectoryLoader2.equals((Object)textDirectoryLoader0));
      assertFalse(instances1.equals((Object)instances0));
      assertNotSame(textDirectoryLoader2, textDirectoryLoader1);
      assertNotSame(textDirectoryLoader2, textDirectoryLoader0);
      assertNotSame(instances1, instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(6, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(5, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals(6, instances0.numAttributes());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier5));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier4));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier6));
      assertFalse(instances0.equals((Object)instances1));
      
      double double0 = evaluation0.numFalseNegatives(5);
      assertEquals(0.0, double0, 0.01);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(6, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(5, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals(6, instances0.numAttributes());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier5));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier4));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier6));
      assertFalse(instances0.equals((Object)instances1));
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier5);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier3);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier4);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier6);
      assertNotSame(instances0, instances1);
      
      double double1 = evaluation0.falseNegativeRate(108);
      assertEquals(0.0, double1, 0.01);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(6, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(5, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals(6, instances0.numAttributes());
      assertEquals(double1, double0, 0.01);
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier5));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier4));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier6));
      assertFalse(instances0.equals((Object)instances1));
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier5);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier3);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier4);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier6);
      assertNotSame(instances0, instances1);
  }

  /**
  //Test case number: 17
  /*Coverage entropy=1.7896656513804965
  */
  @Test(timeout = 4000)
  public void test017()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      
      M5Rules m5Rules0 = new M5Rules();
      assertNotNull(m5Rules0);
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertFalse(m5Rules0.getUnpruned());
      
      Capabilities capabilities0 = m5Rules0.getCapabilities();
      assertNotNull(capabilities0);
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      assertNotNull(findWithCapabilities0);
      assertEquals("", findWithCapabilities0.getFilename());
      
      Capabilities capabilities1 = findWithCapabilities0.getNotCapabilities();
      assertNotNull(capabilities1);
      assertEquals("", findWithCapabilities0.getFilename());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities1, capabilities0);
      
      FilteredClusterer filteredClusterer0 = new FilteredClusterer();
      assertNotNull(filteredClusterer0);
      assertEquals("The filter to be used.", filteredClusterer0.filterTipText());
      assertEquals("The base clusterer to be used.", filteredClusterer0.clustererTipText());
      assertEquals("Class for running an arbitrary clusterer on data that has been passed through an arbitrary filter. Like the clusterer, the structure of the filter is based exclusively on the training data and test instances will be processed by the filter without changing their structure.", filteredClusterer0.globalInfo());
      
      AllFilter allFilter0 = (AllFilter)filteredClusterer0.getFilter();
      assertNotNull(allFilter0);
      assertEquals("The filter to be used.", filteredClusterer0.filterTipText());
      assertEquals("The base clusterer to be used.", filteredClusterer0.clustererTipText());
      assertEquals("Class for running an arbitrary clusterer on data that has been passed through an arbitrary filter. Like the clusterer, the structure of the filter is based exclusively on the training data and test instances will be processed by the filter without changing their structure.", filteredClusterer0.globalInfo());
      assertFalse(allFilter0.isOutputFormatDefined());
      assertFalse(allFilter0.isFirstBatchDone());
      assertFalse(allFilter0.mayRemoveInstanceAfterFirstBatchDone());
      assertTrue(allFilter0.isNewBatch());
      assertEquals("An instance filter that passes all instances through unmodified. Primarily for testing purposes.", allFilter0.globalInfo());
      
      FilteredClassifier filteredClassifier0 = new FilteredClassifier();
      assertNotNull(filteredClassifier0);
      assertEquals("If set to true, classifier may output additional info to the console.", filteredClassifier0.debugTipText());
      assertEquals("The filter to be used.", filteredClassifier0.filterTipText());
      assertEquals(1, filteredClassifier0.graphType());
      assertEquals("Class for running an arbitrary classifier on data that has been passed through an arbitrary filter. Like the classifier, the structure of the filter is based exclusively on the training data and test instances will be processed by the filter without changing their structure.", filteredClassifier0.globalInfo());
      assertFalse(filteredClassifier0.getDebug());
      assertEquals("The base classifier to be used.", filteredClassifier0.classifierTipText());
      
      Discretize discretize0 = (Discretize)filteredClassifier0.getFilter();
      assertNotNull(discretize0);
      assertEquals("If set to true, classifier may output additional info to the console.", filteredClassifier0.debugTipText());
      assertEquals("The filter to be used.", filteredClassifier0.filterTipText());
      assertEquals(1, filteredClassifier0.graphType());
      assertEquals("Class for running an arbitrary classifier on data that has been passed through an arbitrary filter. Like the classifier, the structure of the filter is based exclusively on the training data and test instances will be processed by the filter without changing their structure.", filteredClassifier0.globalInfo());
      assertFalse(filteredClassifier0.getDebug());
      assertEquals("The base classifier to be used.", filteredClassifier0.classifierTipText());
      assertFalse(discretize0.getUseBinNumbers());
      assertFalse(discretize0.isOutputFormatDefined());
      assertFalse(discretize0.getUseKononenko());
      assertFalse(discretize0.getUseBetterEncoding());
      assertFalse(discretize0.isFirstBatchDone());
      assertEquals("Make resulting attributes binary.", discretize0.makeBinaryTipText());
      assertFalse(discretize0.getMakeBinary());
      assertEquals("Set attribute selection mode. If false, only selected (numeric) attributes in the range will be discretized; if true, only non-selected attributes will be discretized.", discretize0.invertSelectionTipText());
      assertFalse(discretize0.mayRemoveInstanceAfterFirstBatchDone());
      assertTrue(discretize0.isNewBatch());
      assertEquals("Use bin numbers (eg BXofY) rather than ranges for for discretized attributes", discretize0.useBinNumbersTipText());
      assertEquals("Use Kononenko's MDL criterion. If set to false uses the Fayyad & Irani criterion.", discretize0.useKononenkoTipText());
      assertEquals("Uses a more efficient split point encoding.", discretize0.useBetterEncodingTipText());
      assertEquals("Specify range of attributes to act on. This is a comma separated list of attribute indices, with \"first\" and \"last\" valid values. Specify an inclusive range with \"-\". E.g: \"first-3,5,6-10,last\".", discretize0.attributeIndicesTipText());
      
      Capabilities capabilities2 = discretize0.getCapabilities();
      assertNotNull(capabilities2);
      assertEquals("If set to true, classifier may output additional info to the console.", filteredClassifier0.debugTipText());
      assertEquals("The filter to be used.", filteredClassifier0.filterTipText());
      assertEquals(1, filteredClassifier0.graphType());
      assertEquals("Class for running an arbitrary classifier on data that has been passed through an arbitrary filter. Like the classifier, the structure of the filter is based exclusively on the training data and test instances will be processed by the filter without changing their structure.", filteredClassifier0.globalInfo());
      assertFalse(filteredClassifier0.getDebug());
      assertEquals("The base classifier to be used.", filteredClassifier0.classifierTipText());
      assertFalse(discretize0.getUseBinNumbers());
      assertFalse(discretize0.isOutputFormatDefined());
      assertFalse(discretize0.getUseKononenko());
      assertFalse(discretize0.getUseBetterEncoding());
      assertFalse(discretize0.isFirstBatchDone());
      assertEquals("Make resulting attributes binary.", discretize0.makeBinaryTipText());
      assertFalse(discretize0.getMakeBinary());
      assertEquals("Set attribute selection mode. If false, only selected (numeric) attributes in the range will be discretized; if true, only non-selected attributes will be discretized.", discretize0.invertSelectionTipText());
      assertFalse(discretize0.mayRemoveInstanceAfterFirstBatchDone());
      assertTrue(discretize0.isNewBatch());
      assertEquals("Use bin numbers (eg BXofY) rather than ranges for for discretized attributes", discretize0.useBinNumbersTipText());
      assertEquals("Use Kononenko's MDL criterion. If set to false uses the Fayyad & Irani criterion.", discretize0.useKononenkoTipText());
      assertEquals("Uses a more efficient split point encoding.", discretize0.useBetterEncodingTipText());
      assertEquals("Specify range of attributes to act on. This is a comma separated list of attribute indices, with \"first\" and \"last\" valid values. Specify an inclusive range with \"-\". E.g: \"first-3,5,6-10,last\".", discretize0.attributeIndicesTipText());
      assertEquals(0, capabilities2.getMinimumNumberInstances());
      assertFalse(capabilities2.hasDependencies());
      assertFalse(capabilities2.equals((Object)capabilities0));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertNotSame(capabilities2, capabilities0);
      assertNotSame(capabilities2, capabilities1);
      
      TestInstances testInstances1 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(0, testInstances1.getClassType());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertFalse(testInstances1.getNoClass());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(0, testInstances1.getNumString());
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances1, testInstances0);
      
      TestInstances testInstances2 = new TestInstances();
      assertNotNull(testInstances2);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances2.getNumInstances());
      assertEquals(2, testInstances2.getNumRelationalNominalValues());
      assertEquals(0, testInstances2.getNumNumeric());
      assertEquals(0, testInstances2.getNumRelational());
      assertEquals(0, testInstances2.getNumRelationalDate());
      assertEquals(2, testInstances2.getNumNominalValues());
      assertEquals(2, testInstances2.getNumAttributes());
      assertEquals(2, testInstances2.getNumClasses());
      assertFalse(testInstances2.getNoClass());
      assertEquals((-1), testInstances2.getClassIndex());
      assertEquals(1, testInstances2.getClassType());
      assertEquals(1, testInstances2.getNumRelationalNominal());
      assertEquals(1, testInstances2.getSeed());
      assertEquals(" ", testInstances2.getWordSeparators());
      assertEquals(0, testInstances2.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances2.getRelation());
      assertEquals(0, testInstances2.getNumDate());
      assertEquals(1, testInstances2.getNumNominal());
      assertEquals(0, testInstances2.getNumString());
      assertEquals(0, testInstances2.getNumRelationalString());
      assertEquals(10, testInstances2.getNumInstancesRelational());
      assertFalse(testInstances2.getMultiInstance());
      assertFalse(testInstances2.equals((Object)testInstances1));
      assertFalse(testInstances2.equals((Object)testInstances0));
      
      testInstances1.setNumRelationalString(1196);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(0, testInstances1.getClassType());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(1196, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertFalse(testInstances1.getNoClass());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(0, testInstances1.getNumString());
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances1.equals((Object)testInstances2));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances1, testInstances2);
      assertNotSame(testInstances1, testInstances0);
      
      testInstances0.setNumNominalValues(1196);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1196, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertFalse(testInstances0.equals((Object)testInstances2));
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances0, testInstances1);
      assertNotSame(testInstances0, testInstances2);
      
      TestInstances testInstances3 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances3);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances3.getNumRelationalNominal());
      assertEquals(0, testInstances3.getNumString());
      assertEquals((-1), testInstances3.getClassIndex());
      assertEquals(1, testInstances3.getSeed());
      assertEquals(1, testInstances3.getNumRelationalNumeric());
      assertEquals(1, testInstances3.getNumNumeric());
      assertEquals(0, testInstances3.getNumRelationalString());
      assertEquals(2, testInstances3.getNumRelationalNominalValues());
      assertEquals(20, testInstances3.getNumInstances());
      assertFalse(testInstances3.getNoClass());
      assertFalse(testInstances3.getMultiInstance());
      assertEquals(10, testInstances3.getNumInstancesRelational());
      assertEquals(2, testInstances3.getNumClasses());
      assertEquals(0, testInstances3.getNumRelational());
      assertEquals(" ", testInstances3.getWordSeparators());
      assertEquals("Testdata", testInstances3.getRelation());
      assertEquals(1, testInstances3.getNumRelationalDate());
      assertEquals(1, testInstances3.getNumDate());
      assertEquals(2, testInstances3.getNumNominalValues());
      assertEquals(4, testInstances3.getNumAttributes());
      assertEquals(1, testInstances3.getNumNominal());
      assertEquals(0, testInstances3.getClassType());
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertFalse(testInstances3.equals((Object)testInstances2));
      assertFalse(testInstances3.equals((Object)testInstances0));
      assertFalse(testInstances3.equals((Object)testInstances1));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(testInstances3, testInstances2);
      assertNotSame(testInstances3, testInstances0);
      assertNotSame(testInstances3, testInstances1);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      
      TestInstances testInstances4 = new TestInstances();
      assertNotNull(testInstances4);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances4.getNumAttributes());
      assertEquals(2, testInstances4.getNumClasses());
      assertEquals(0, testInstances4.getNumNumeric());
      assertEquals(0, testInstances4.getNumRelational());
      assertEquals(0, testInstances4.getNumRelationalDate());
      assertEquals(0, testInstances4.getNumString());
      assertEquals(1, testInstances4.getClassType());
      assertFalse(testInstances4.getNoClass());
      assertEquals((-1), testInstances4.getClassIndex());
      assertEquals(0, testInstances4.getNumRelationalString());
      assertEquals(10, testInstances4.getNumInstancesRelational());
      assertEquals(20, testInstances4.getNumInstances());
      assertEquals(2, testInstances4.getNumRelationalNominalValues());
      assertEquals(" ", testInstances4.getWordSeparators());
      assertEquals(0, testInstances4.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances4.getRelation());
      assertEquals(0, testInstances4.getNumDate());
      assertEquals(1, testInstances4.getNumNominal());
      assertEquals(2, testInstances4.getNumNominalValues());
      assertEquals(1, testInstances4.getNumRelationalNominal());
      assertEquals(1, testInstances4.getSeed());
      assertFalse(testInstances4.getMultiInstance());
      assertFalse(testInstances4.equals((Object)testInstances2));
      assertFalse(testInstances4.equals((Object)testInstances3));
      assertFalse(testInstances4.equals((Object)testInstances1));
      assertFalse(testInstances4.equals((Object)testInstances0));
      
      Instances instances0 = testInstances3.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances3.getNumRelationalNominal());
      assertEquals(0, testInstances3.getNumString());
      assertEquals((-1), testInstances3.getClassIndex());
      assertEquals(1, testInstances3.getSeed());
      assertEquals(1, testInstances3.getNumRelationalNumeric());
      assertEquals(1, testInstances3.getNumNumeric());
      assertEquals(0, testInstances3.getNumRelationalString());
      assertEquals(2, testInstances3.getNumRelationalNominalValues());
      assertEquals(20, testInstances3.getNumInstances());
      assertFalse(testInstances3.getNoClass());
      assertFalse(testInstances3.getMultiInstance());
      assertEquals(10, testInstances3.getNumInstancesRelational());
      assertEquals(2, testInstances3.getNumClasses());
      assertEquals(0, testInstances3.getNumRelational());
      assertEquals(" ", testInstances3.getWordSeparators());
      assertEquals("Testdata", testInstances3.getRelation());
      assertEquals(1, testInstances3.getNumRelationalDate());
      assertEquals(1, testInstances3.getNumDate());
      assertEquals(2, testInstances3.getNumNominalValues());
      assertEquals(4, testInstances3.getNumAttributes());
      assertEquals(1, testInstances3.getNumNominal());
      assertEquals(0, testInstances3.getClassType());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertFalse(testInstances3.equals((Object)testInstances2));
      assertFalse(testInstances3.equals((Object)testInstances0));
      assertFalse(testInstances3.equals((Object)testInstances4));
      assertFalse(testInstances3.equals((Object)testInstances1));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(testInstances3, testInstances2);
      assertNotSame(testInstances3, testInstances0);
      assertNotSame(testInstances3, testInstances4);
      assertNotSame(testInstances3, testInstances1);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      
      String[] stringArray0 = testInstances0.getOptions();
      assertNotNull(stringArray0);
      assertEquals(40, stringArray0.length);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1196, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances0.equals((Object)testInstances4));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertFalse(testInstances0.equals((Object)testInstances2));
      assertFalse(testInstances0.equals((Object)testInstances3));
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances0, testInstances4);
      assertNotSame(testInstances0, testInstances1);
      assertNotSame(testInstances0, testInstances2);
      assertNotSame(testInstances0, testInstances3);
      
      TestInstances testInstances5 = TestInstances.forCapabilities(capabilities1);
      assertNotNull(testInstances5);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances5.getNumRelationalNominalValues());
      assertFalse(testInstances5.getMultiInstance());
      assertEquals(20, testInstances5.getNumInstances());
      assertFalse(testInstances5.getNoClass());
      assertEquals(2, testInstances5.getNumNominalValues());
      assertEquals(10, testInstances5.getNumInstancesRelational());
      assertEquals(0, testInstances5.getNumRelationalString());
      assertEquals(0, testInstances5.getNumNumeric());
      assertEquals(0, testInstances5.getNumRelationalNumeric());
      assertEquals(0, testInstances5.getNumRelationalNominal());
      assertEquals(1, testInstances5.getSeed());
      assertEquals(0, testInstances5.getNumString());
      assertEquals((-1), testInstances5.getClassIndex());
      assertEquals(0, testInstances5.getNumNominal());
      assertEquals(0, testInstances5.getNumRelational());
      assertEquals(0, testInstances5.getNumDate());
      assertEquals(1, testInstances5.getNumAttributes());
      assertEquals(" ", testInstances5.getWordSeparators());
      assertEquals("Testdata", testInstances5.getRelation());
      assertEquals(0, testInstances5.getNumRelationalDate());
      assertEquals(1, testInstances5.getClassType());
      assertEquals(2, testInstances5.getNumClasses());
      assertEquals("", findWithCapabilities0.getFilename());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(testInstances5.equals((Object)testInstances2));
      assertFalse(testInstances5.equals((Object)testInstances3));
      assertFalse(testInstances5.equals((Object)testInstances1));
      assertFalse(testInstances5.equals((Object)testInstances4));
      assertFalse(testInstances5.equals((Object)testInstances0));
      assertFalse(capabilities1.equals((Object)capabilities2));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(testInstances5, testInstances2);
      assertNotSame(testInstances5, testInstances3);
      assertNotSame(testInstances5, testInstances1);
      assertNotSame(testInstances5, testInstances4);
      assertNotSame(testInstances5, testInstances0);
      assertNotSame(capabilities1, capabilities2);
      assertNotSame(capabilities1, capabilities0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances3.getNumRelationalNominal());
      assertEquals(0, testInstances3.getNumString());
      assertEquals((-1), testInstances3.getClassIndex());
      assertEquals(1, testInstances3.getSeed());
      assertEquals(1, testInstances3.getNumRelationalNumeric());
      assertEquals(1, testInstances3.getNumNumeric());
      assertEquals(0, testInstances3.getNumRelationalString());
      assertEquals(2, testInstances3.getNumRelationalNominalValues());
      assertEquals(20, testInstances3.getNumInstances());
      assertFalse(testInstances3.getNoClass());
      assertFalse(testInstances3.getMultiInstance());
      assertEquals(10, testInstances3.getNumInstancesRelational());
      assertEquals(2, testInstances3.getNumClasses());
      assertEquals(0, testInstances3.getNumRelational());
      assertEquals(" ", testInstances3.getWordSeparators());
      assertEquals("Testdata", testInstances3.getRelation());
      assertEquals(1, testInstances3.getNumRelationalDate());
      assertEquals(1, testInstances3.getNumDate());
      assertEquals(2, testInstances3.getNumNominalValues());
      assertEquals(4, testInstances3.getNumAttributes());
      assertEquals(1, testInstances3.getNumNominal());
      assertEquals(0, testInstances3.getClassType());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertFalse(m5Rules0.getDebug());
      assertFalse(m5Rules0.getUseUnsmoothed());
      assertEquals("Whether to use unsmoothed predictions.", m5Rules0.useUnsmoothedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", m5Rules0.debugTipText());
      assertFalse(m5Rules0.getBuildRegressionTree());
      assertEquals("Whether unpruned tree/rules are to be generated.", m5Rules0.unprunedTipText());
      assertEquals("Whether to generate a regression tree/rule instead of a model tree/rule.", m5Rules0.buildRegressionTreeTipText());
      assertEquals(4.0, m5Rules0.getMinNumInstances(), 0.01);
      assertEquals("Whether to generate rules (decision list) rather than a tree.", m5Rules0.generateRulesTipText());
      assertEquals("The minimum number of instances to allow at a leaf node.", m5Rules0.minNumInstancesTipText());
      assertFalse(m5Rules0.getUnpruned());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(testInstances3.equals((Object)testInstances5));
      assertFalse(testInstances3.equals((Object)testInstances2));
      assertFalse(testInstances3.equals((Object)testInstances0));
      assertFalse(testInstances3.equals((Object)testInstances4));
      assertFalse(testInstances3.equals((Object)testInstances1));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      
      try { 
        evaluation0.priorEntropy();
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Can't compute entropy of class prior: class numeric!
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 18
  /*Coverage entropy=2.5479759122939765
  */
  @Test(timeout = 4000)
  public void test018()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances0);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      
      DenseInstance denseInstance0 = new DenseInstance(25);
      assertNotNull(denseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(25, denseInstance0.numValues());
      assertEquals(25, denseInstance0.numAttributes());
      
      double[] doubleArray0 = new double[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(5.019028201623571E-4, doubleArray0);
      assertNotNull(binarySparseInstance0);
      assertEquals(0, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals(5.019028201623571E-4, binarySparseInstance0.weight(), 0.01);
      
      SparseInstance sparseInstance0 = new SparseInstance(0.0022462733445193627, doubleArray0);
      assertNotNull(sparseInstance0);
      assertEquals(0, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(0.0022462733445193627, sparseInstance0.weight(), 0.01);
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(0, sparseInstance0.numAttributes());
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(0, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals(5.019028201623571E-4, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance1.numValues());
      assertEquals(0, binarySparseInstance1.numAttributes());
      assertEquals(5.019028201623571E-4, binarySparseInstance1.weight(), 0.01);
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      assertNotNull(adaBoostM1_0);
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      
      String[] stringArray0 = new String[5];
      stringArray0[0] = "@data";
      stringArray0[1] = ".bsi";
      Evaluation evaluation1 = new Evaluation(instances0);
      assertNotNull(evaluation1);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      SimpleLogistic simpleLogistic0 = new SimpleLogistic();
      assertNotNull(simpleLogistic0);
      assertEquals("Sets whether the number of LogitBoost iterations is to be cross-validated or the stopping criterion on the training set should be used. If not set (and no fixed number of iterations was given), the number of LogitBoost iterations is used that minimizes the error on the training set (misclassification error or error on probabilities depending on errorOnProbabilities).", simpleLogistic0.useCrossValidationTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", simpleLogistic0.weightTrimBetaTipText());
      assertEquals(500, simpleLogistic0.getMaxBoostingIterations());
      assertEquals("If heuristicStop > 0, the heuristic for greedy stopping while cross-validating the number of LogitBoost iterations is enabled. This means LogitBoost is stopped if no new error minimum has been reached in the last heuristicStop iterations. It is recommended to use this heuristic, it gives a large speed-up especially on small datasets. The default value is 50.", simpleLogistic0.heuristicStopTipText());
      assertTrue(simpleLogistic0.getUseCrossValidation());
      assertFalse(simpleLogistic0.getErrorOnProbabilities());
      assertEquals(0.0, simpleLogistic0.getWeightTrimBeta(), 0.01);
      assertEquals("Set fixed number of iterations for LogitBoost. If >= 0, this sets the number of LogitBoost iterations to perform. If < 0, the number is cross-validated or a stopping criterion on the training set is used (depending on the value of useCrossValidation).", simpleLogistic0.numBoostingIterationsTipText());
      assertFalse(simpleLogistic0.getDebug());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations (instead of cross-validation or training error).", simpleLogistic0.useAICTipText());
      assertEquals(0, simpleLogistic0.getNumBoostingIterations());
      assertEquals(50, simpleLogistic0.getHeuristicStop());
      assertEquals("If set to true, classifier may output additional info to the console.", simpleLogistic0.debugTipText());
      assertEquals("Use error on the probabilties as error measure when determining the best number of LogitBoost iterations. If set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error (either on the training set or in the cross-validation, depending on useCrossValidation).", simpleLogistic0.errorOnProbabilitiesTipText());
      assertFalse(simpleLogistic0.getUseAIC());
      assertEquals("Sets the maximum number of iterations for LogitBoost. Default value is 500, for very small/large datasets a lower/higher value might be preferable.", simpleLogistic0.maxBoostingIterationsTipText());
      
      Vote vote0 = new Vote();
      assertNotNull(vote0);
      assertEquals(3, Vote.MAJORITY_VOTING_RULE);
      assertEquals(5, Vote.MAX_RULE);
      assertEquals(6, Vote.MEDIAN_RULE);
      assertEquals(2, Vote.PRODUCT_RULE);
      assertEquals(4, Vote.MIN_RULE);
      assertEquals(1, Vote.AVERAGE_RULE);
      assertEquals("The random number seed to be used.", vote0.seedTipText());
      assertFalse(vote0.getDebug());
      assertEquals(1, vote0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", vote0.debugTipText());
      assertEquals("The combination rule used.", vote0.combinationRuleTipText());
      assertEquals("The base classifiers to be used.", vote0.classifiersTipText());
      assertEquals("The pre-built serialized classifiers to include. Multiple serialized classifiers can be included alongside those that are built from scratch when this classifier runs. Note that it does not make sense to include pre-built classifiers in a cross-validation since they are static and their models do not change from fold to fold.", vote0.preBuiltClassifiersTipText());
      
      String string0 = Evaluation.getGlobalInfo(vote0);
      assertEquals("\nSynopsis for weka.classifiers.meta.Vote:\n\nClass for combining classifiers. Different combinations of probability estimates for classification are available.\n\nFor more information see:\n\nLudmila I. Kuncheva (2004). Combining Pattern Classifiers: Methods and Algorithms. John Wiley and Sons, Inc..\n\nJ. Kittler, M. Hatef, Robert P.W. Duin, J. Matas (1998). On combining classifiers. IEEE Transactions on Pattern Analysis and Machine Intelligence. 20(3):226-239.", string0);
      assertNotNull(string0);
      assertEquals(3, Vote.MAJORITY_VOTING_RULE);
      assertEquals(5, Vote.MAX_RULE);
      assertEquals(6, Vote.MEDIAN_RULE);
      assertEquals(2, Vote.PRODUCT_RULE);
      assertEquals(4, Vote.MIN_RULE);
      assertEquals(1, Vote.AVERAGE_RULE);
      assertEquals("The random number seed to be used.", vote0.seedTipText());
      assertFalse(vote0.getDebug());
      assertEquals(1, vote0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", vote0.debugTipText());
      assertEquals("The combination rule used.", vote0.combinationRuleTipText());
      assertEquals("The base classifiers to be used.", vote0.classifiersTipText());
      assertEquals("The pre-built serialized classifiers to include. Multiple serialized classifiers can be included alongside those that are built from scratch when this classifier runs. Note that it does not make sense to include pre-built classifiers in a cross-validation since they are static and their models do not change from fold to fold.", vote0.preBuiltClassifiersTipText());
      
      double double0 = evaluation1.rootRelativeSquaredError();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(evaluation1, evaluation0);
      
      ArffLoader arffLoader0 = new ArffLoader();
      assertNotNull(arffLoader0);
      assertEquals(".arff", arffLoader0.getFileExtension());
      assertEquals("Use relative rather than absolute paths", arffLoader0.useRelativePathTipText());
      assertEquals("Arff data files", arffLoader0.getFileDescription());
      assertEquals("http://", arffLoader0.retrieveURL());
      assertEquals("Reads a source that is in arff (attribute relation file format) format. ", arffLoader0.globalInfo());
      assertFalse(arffLoader0.getUseRelativePath());
      
      Instance instance0 = arffLoader0.getNextInstance(instances0);
      assertNull(instance0);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(".arff", arffLoader0.getFileExtension());
      assertEquals("Use relative rather than absolute paths", arffLoader0.useRelativePathTipText());
      assertEquals("Arff data files", arffLoader0.getFileDescription());
      assertEquals("http://", arffLoader0.retrieveURL());
      assertEquals("Reads a source that is in arff (attribute relation file format) format. ", arffLoader0.globalInfo());
      assertFalse(arffLoader0.getUseRelativePath());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      
      try { 
        evaluation0.evaluateModelOnce(doubleArray0, (Instance) binarySparseInstance0);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 0
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 19
  /*Coverage entropy=2.8409573360339078
  */
  @Test(timeout = 4000)
  public void test019()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      boolean boolean0 = FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertFalse(costSensitiveClassifier0.getDebug());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier3);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      
      CostMatrix costMatrix0 = costSensitiveClassifier3.getCostMatrix();
      assertNotNull(costMatrix0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals(1, costMatrix0.numRows());
      assertEquals(1, costMatrix0.size());
      assertEquals(1, costMatrix0.numColumns());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier2);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      
      double double0 = evaluation0.rootMeanSquaredError();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      
      double double1 = evaluation0.unweightedMacroFmeasure();
      assertEquals(Double.NaN, double1, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(double1, double0, 0.01);
      
      DenseInstance denseInstance0 = new DenseInstance(1);
      assertNotNull(denseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(1, denseInstance0.numAttributes());
      assertEquals(1, denseInstance0.numValues());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      
      double double2 = evaluation0.m_MissingClass;
      assertEquals(0.0, double2, 0.01);
      assertNotEquals(double2, double0, 0.01);
      assertNotEquals(double2, double1, 0.01);
      
      double double3 = evaluation0.trueNegativeRate(2);
      assertEquals(0.0, double3, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertNotEquals(double3, double1, 0.01);
      assertNotEquals(double3, double0, 0.01);
      assertEquals(double3, double2, 0.01);
      
      double double4 = evaluation0.falseNegativeRate(106);
      assertEquals(0.0, double4, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(double4, double3, 0.01);
      assertNotEquals(double4, double1, 0.01);
      assertEquals(double4, double2, 0.01);
      assertNotEquals(double4, double0, 0.01);
      
      double double5 = evaluation0.recall(2);
      assertEquals(0.0, double5, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(double5, double4, 0.01);
      assertEquals(double5, double2, 0.01);
      assertEquals(double5, double3, 0.01);
      assertNotEquals(double5, double1, 0.01);
      assertNotEquals(double5, double0, 0.01);
      
      double[][] doubleArray0 = evaluation0.confusionMatrix();
      assertNotNull(doubleArray0);
      assertEquals(4, doubleArray0.length);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      
      try { 
        evaluation0.evaluationForSingleInstance((Classifier) costSensitiveClassifier3, (Instance) denseInstance0, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 20
  /*Coverage entropy=2.592965132436166
  */
  @Test(timeout = 4000)
  public void test020()  throws Throwable  {
      KStar kStar0 = new KStar();
      assertNotNull(kStar0);
      assertEquals("If set to true, classifier may output additional info to the console.", kStar0.debugTipText());
      assertEquals(20, kStar0.getGlobalBlend());
      assertEquals("Determines how missing attribute values are treated.", kStar0.missingModeTipText());
      assertFalse(kStar0.getDebug());
      assertFalse(kStar0.getEntropicAutoBlend());
      assertEquals("The parameter for global blending. Values are restricted to [0,100].", kStar0.globalBlendTipText());
      assertEquals("Whether entropy-based blending is to be used.", kStar0.entropicAutoBlendTipText());
      
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      assertNotNull(adaBoostM1_0);
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getDebug());
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      
      Instances instances0 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader0);
      assertNotNull(instances0);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      
      Capabilities capabilities0 = adaBoostM1_0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      instances0.sort(comparator0);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(153);
      assertNotNull(binarySparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(153, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(153, binarySparseInstance0.numAttributes());
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(153, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(153, binarySparseInstance0.numAttributes());
      assertEquals(153, binarySparseInstance1.numValues());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertEquals(153, binarySparseInstance1.numAttributes());
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      binarySparseInstance1.deleteAttributeAt(2);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(153, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(153, binarySparseInstance0.numAttributes());
      assertEquals(152, binarySparseInstance1.numValues());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertEquals(152, binarySparseInstance1.numAttributes());
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance1));
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      assertNotSame(binarySparseInstance0, binarySparseInstance1);
      assertNotSame(binarySparseInstance1, binarySparseInstance0);
      
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance0);
      assertNotNull(sparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(153, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(153, binarySparseInstance0.numAttributes());
      assertEquals(153, sparseInstance0.numValues());
      assertEquals(153, sparseInstance0.numAttributes());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance1));
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      double double0 = evaluation0.m_MissingClass;
      assertEquals(0.0, double0, 0.01);
      
      double double1 = evaluation0.trueNegativeRate(23);
      assertEquals(0.0, double1, 0.01);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(double1, double0, 0.01);
      
      double double2 = evaluation0.falseNegativeRate((-846));
      assertEquals(0.0, double2, 0.01);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(double2, double1, 0.01);
      assertEquals(double2, double0, 0.01);
      
      double double3 = evaluation0.recall(2);
      assertEquals(0.0, double3, 0.01);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(double3, double0, 0.01);
      assertEquals(double3, double1, 0.01);
      assertEquals(double3, double2, 0.01);
      
      double[][] doubleArray0 = evaluation0.confusionMatrix();
      assertNotNull(doubleArray0);
      assertEquals(0, doubleArray0.length);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      
      try { 
        evaluation0.evaluationForSingleInstance((Classifier) adaBoostM1_0, (Instance) binarySparseInstance1, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 21
  /*Coverage entropy=2.5256694216226987
  */
  @Test(timeout = 4000)
  public void test021()  throws Throwable  {
      Stacking stacking0 = new Stacking();
      assertNotNull(stacking0);
      assertFalse(stacking0.getDebug());
      assertEquals(10, stacking0.getNumFolds());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      assertEquals(1, stacking0.getSeed());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      
      boolean boolean0 = FileSystemHandling.setPermissions((EvoSuiteFile) null, false, false, false);
      assertFalse(boolean0);
      
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      assertNotNull(adaBoostM1_0);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      
      Capabilities capabilities0 = adaBoostM1_0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      assertNotNull(capabilities1);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      instances0.deleteStringAttributes();
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      boolean boolean1 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean1);
      assertFalse(boolean1 == boolean0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      
      double double0 = evaluation0.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      double double1 = evaluation0.m_WithClass;
      assertEquals(0.0, double1, 0.01);
      assertNotEquals(double1, double0, 0.01);
      
      double double2 = evaluation0.weightedTrueNegativeRate();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertEquals(double2, double0, 0.01);
      assertNotEquals(double2, double1, 0.01);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      assertNotNull(findWithCapabilities0);
      assertEquals("", findWithCapabilities0.getFilename());
      
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      doReturn(0, 0, 0, 0, 0).when(comparator0).compare(any() , any());
      instances0.sort(comparator0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      Evaluation evaluation1 = new Evaluation(instances0);
      assertNotNull(evaluation1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      
      evaluation1.setDiscardPredictions(true);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertTrue(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(evaluation1, evaluation0);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      GaussianProcesses gaussianProcesses0 = new GaussianProcesses();
      assertNotNull(gaussianProcesses0);
      assertEquals(1, GaussianProcesses.FILTER_STANDARDIZE);
      assertEquals(0, GaussianProcesses.FILTER_NORMALIZE);
      assertEquals(2, GaussianProcesses.FILTER_NONE);
      assertFalse(gaussianProcesses0.getDebug());
      assertEquals(1.0, gaussianProcesses0.getNoise(), 0.01);
      assertEquals("The kernel to use.", gaussianProcesses0.kernelTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", gaussianProcesses0.debugTipText());
      assertEquals(" Implements Gaussian processes for regression without hyperparameter-tuning. To make choosing an appropriate noise level easier, this implementation applies normalization/standardization to the target attribute as well as the other attributes (if  normalization/standardizaton is turned on). Missing values are replaced by the global mean/mode. Nominal attributes are converted to binary ones. Note that kernel caching is turned off if the kernel used implements CachedKernel.", gaussianProcesses0.globalInfo());
      assertEquals("The level of Gaussian Noise (added to the diagonal of the Covariance Matrix), after the target has been normalized/standardized/left unchanged).", gaussianProcesses0.noiseTipText());
      assertEquals("Determines how/if the data will be transformed.", gaussianProcesses0.filterTypeTipText());
      
      double double3 = evaluation0.rootMeanPriorSquaredError();
      assertEquals(Double.NaN, double3, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(double3, double2, 0.01);
      assertEquals(double3, double0, 0.01);
      assertNotEquals(double3, double1, 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(evaluation0, evaluation1);
      
      double[] doubleArray0 = new double[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(3702.7653683924, doubleArray0);
      assertNotNull(binarySparseInstance0);
      assertEquals(0, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals(3702.7653683924, binarySparseInstance0.weight(), 0.01);
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(0, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals(3702.7653683924, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance1.numValues());
      assertEquals(0, binarySparseInstance1.numAttributes());
      assertEquals(3702.7653683924, binarySparseInstance1.weight(), 0.01);
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      Evaluation evaluation2 = new Evaluation(instances0);
      assertNotNull(evaluation2);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      
      char[] charArray0 = new char[3];
      charArray0[0] = 'v';
      charArray0[1] = 'E';
      charArray0[2] = '-';
      String string0 = evaluation2.num2ShortID(1, charArray0, 1436);
      assertNotNull(string0);
      assertEquals(3, charArray0.length);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertArrayEquals(new char[] {'v', 'E', '-'}, charArray0);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(evaluation2, evaluation0);
      assertNotSame(evaluation2, evaluation1);
      
      CoverTree coverTree0 = new CoverTree();
      assertNotNull(coverTree0);
      assertFalse(coverTree0.getMeasurePerformance());
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertEquals(1.3, coverTree0.getBase(), 0.01);
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01);
      
      try { 
        coverTree0.nearestNeighbour(binarySparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 22
  /*Coverage entropy=2.7546114082706614
  */
  @Test(timeout = 4000)
  public void test022()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(1, lMT0.graphType());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(1, lMT0.graphType());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(1, lMT0.graphType());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(1, lMT0.graphType());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier3);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      
      costSensitiveClassifier3.setDebug(false);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier2);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier0);
      
      CostSensitiveClassifier costSensitiveClassifier4 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier4);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      
      CostMatrix costMatrix0 = costSensitiveClassifier4.getCostMatrix();
      assertNotNull(costMatrix0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertEquals(1, costMatrix0.size());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.numRows());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier3);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier2);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(1, lMT0.graphType());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      
      MockRandom mockRandom0 = new MockRandom();
      assertNotNull(mockRandom0);
      
      double double0 = mockRandom0.nextGaussian();
      assertEquals(0.7, double0, 0.01);
      
      String string0 = costMatrix0.toMatlab();
      assertEquals("[0.0]", string0);
      assertNotNull(string0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertEquals(1, costMatrix0.size());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.numRows());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier3);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier2);
      
      MockRandom mockRandom1 = new MockRandom();
      assertNotNull(mockRandom1);
      assertFalse(mockRandom1.equals((Object)mockRandom0));
      
      double double1 = evaluation0.kappa();
      assertEquals(1.0, double1, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(1, lMT0.graphType());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertNotEquals(double1, double0, 0.01);
      
      lMT0.buildClassifier(instances0);
  }

  /**
  //Test case number: 23
  /*Coverage entropy=2.2065052252090784
  */
  @Test(timeout = 4000)
  public void test023()  throws Throwable  {
      MultilayerPerceptron multilayerPerceptron0 = new MultilayerPerceptron();
      assertNotNull(multilayerPerceptron0);
      assertEquals("This will cause the learning rate to decrease. This will divide the starting learning rate by the epoch number, to determine what the current learning rate should be. This may help to stop the network from diverging from the target output, as well as improve general performance. Note that the decaying learning rate will not be shown in the gui, only the original learning rate. If the learning rate is changed in the gui, this is treated as the starting learning rate.", multilayerPerceptron0.decayTipText());
      assertEquals("The percentage size of the validation set.(The training will continue until it is observed that the error on the validation set has been consistently getting worse, or if the training time is reached).\nIf This is set to zero no validation set will be used and instead the network will train for the specified number of epochs.", multilayerPerceptron0.validationSetSizeTipText());
      assertFalse(multilayerPerceptron0.getDecay());
      assertEquals("This will preprocess the instances with the filter. This could help improve performance if there are nominal attributes in the data.", multilayerPerceptron0.nominalToBinaryFilterTipText());
      assertEquals("Used to terminate validation testing.The value here dictates how many times in a row the validation set error can get worse before training is terminated.", multilayerPerceptron0.validationThresholdTipText());
      assertFalse(multilayerPerceptron0.getGUI());
      assertEquals("This will normalize the attributes. This could help improve performance of the network. This is not reliant on the class being numeric. This will also normalize nominal attributes as well (after they have been run through the nominal to binary filter if that is in use) so that the nominal values are between -1 and 1", multilayerPerceptron0.normalizeAttributesTipText());
      assertEquals("The amount the weights are updated.", multilayerPerceptron0.learningRateTipText());
      assertTrue(multilayerPerceptron0.getAutoBuild());
      assertEquals("A Classifier that uses backpropagation to classify instances.\nThis network can be built by hand, created by an algorithm or both. The network can also be monitored and modified during training time. The nodes in this network are all sigmoid (except for when the class is numeric in which case the the output nodes become unthresholded linear units).", multilayerPerceptron0.globalInfo());
      assertEquals(0, multilayerPerceptron0.getSeed());
      assertEquals("Seed used to initialise the random number generator.Random numbers are used for setting the initial weights of the connections betweem nodes, and also for shuffling the training data.", multilayerPerceptron0.seedTipText());
      assertEquals("This defines the hidden layers of the neural network. This is a list of positive whole numbers. 1 for each hidden layer. Comma seperated. To have no hidden layers put a single 0 here. This will only be used if autobuild is set. There are also wildcard values 'a' = (attribs + classes) / 2, 'i' = attribs, 'o' = classes , 't' = attribs + classes.", multilayerPerceptron0.hiddenLayersTipText());
      assertEquals("a", multilayerPerceptron0.getHiddenLayers());
      assertTrue(multilayerPerceptron0.getReset());
      assertTrue(multilayerPerceptron0.getNominalToBinaryFilter());
      assertEquals(0.2, multilayerPerceptron0.getMomentum(), 0.01);
      assertEquals(0.3, multilayerPerceptron0.getLearningRate(), 0.01);
      assertEquals(0, multilayerPerceptron0.getValidationSetSize());
      assertEquals(20, multilayerPerceptron0.getValidationThreshold());
      assertEquals("Momentum applied to the weights during updating.", multilayerPerceptron0.momentumTipText());
      assertEquals(500, multilayerPerceptron0.getTrainingTime());
      assertTrue(multilayerPerceptron0.getNormalizeNumericClass());
      assertEquals("Adds and connects up hidden layers in the network.", multilayerPerceptron0.autoBuildTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", multilayerPerceptron0.debugTipText());
      assertFalse(multilayerPerceptron0.getDebug());
      assertEquals("This will normalize the class if it's numeric. This could help improve performance of the network, It normalizes the class to be between -1 and 1. Note that this is only internally, the output will be scaled back to the original range.", multilayerPerceptron0.normalizeNumericClassTipText());
      assertTrue(multilayerPerceptron0.getNormalizeAttributes());
      assertEquals("The number of epochs to train through. If the validation set is non-zero then it can terminate the network early", multilayerPerceptron0.trainingTimeTipText());
      assertEquals("This will allow the network to reset with a lower learning rate. If the network diverges from the answer this will automatically reset the network with a lower learning rate and begin training again. This option is only available if the gui is not set. Note that if the network diverges but isn't allowed to reset it will fail the training process and return an error message.", multilayerPerceptron0.resetTipText());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      LinearRegression linearRegression0 = new LinearRegression();
      assertNotNull(linearRegression0);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression0.attributeSelectionMethodTipText());
      assertEquals("Eliminate colinear attributes.", linearRegression0.eliminateColinearAttributesTipText());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression0.minimalTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression0.ridgeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression0.debugTipText());
      assertFalse(linearRegression0.getDebug());
      assertTrue(linearRegression0.getEliminateColinearAttributes());
      assertEquals(1.0E-8, linearRegression0.getRidge(), 0.01);
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression0.globalInfo());
      assertFalse(linearRegression0.getMinimal());
      
      LinearRegression linearRegression1 = new LinearRegression();
      assertNotNull(linearRegression1);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression1.minimalTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression1.ridgeTipText());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression1.attributeSelectionMethodTipText());
      assertEquals("Eliminate colinear attributes.", linearRegression1.eliminateColinearAttributesTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression1.debugTipText());
      assertFalse(linearRegression1.getMinimal());
      assertTrue(linearRegression1.getEliminateColinearAttributes());
      assertEquals(1.0E-8, linearRegression1.getRidge(), 0.01);
      assertFalse(linearRegression1.getDebug());
      assertFalse(linearRegression1.equals((Object)linearRegression0));
      
      boolean boolean0 = FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "=q|6xt>ckl[Y");
      assertFalse(boolean0);
      
      Enumeration enumeration0 = linearRegression1.listOptions();
      assertNotNull(enumeration0);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression1.minimalTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression1.ridgeTipText());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression1.attributeSelectionMethodTipText());
      assertEquals("Eliminate colinear attributes.", linearRegression1.eliminateColinearAttributesTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression1.debugTipText());
      assertFalse(linearRegression1.getMinimal());
      assertTrue(linearRegression1.getEliminateColinearAttributes());
      assertEquals(1.0E-8, linearRegression1.getRidge(), 0.01);
      assertFalse(linearRegression1.getDebug());
      assertFalse(linearRegression1.equals((Object)linearRegression0));
      assertNotSame(linearRegression1, linearRegression0);
      
      SelectedTag selectedTag0 = new SelectedTag(1, linearRegression0.TAGS_SELECTION);
      assertNotNull(selectedTag0);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression0.attributeSelectionMethodTipText());
      assertEquals("Eliminate colinear attributes.", linearRegression0.eliminateColinearAttributesTipText());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression0.minimalTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression0.ridgeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression0.debugTipText());
      assertFalse(linearRegression0.getDebug());
      assertTrue(linearRegression0.getEliminateColinearAttributes());
      assertEquals(1.0E-8, linearRegression0.getRidge(), 0.01);
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression0.globalInfo());
      assertFalse(linearRegression0.getMinimal());
      assertEquals("1", selectedTag0.toString());
      assertFalse(linearRegression0.equals((Object)linearRegression1));
      
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      
      double[] doubleArray0 = new double[8];
      KDTree kDTree0 = new KDTree();
      assertNotNull(kDTree0);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(1, KDTree.MAX);
      assertEquals(0, KDTree.MIN);
      assertEquals(0.01, kDTree0.getMinBoxRelWidth(), 0.01);
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree0.normalizeNodeWidthTipText());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree0.minBoxRelWidthTipText());
      assertTrue(kDTree0.getNormalizeNodeWidth());
      assertEquals(0.0, kDTree0.measureNumLeaves(), 0.01);
      assertEquals(40, kDTree0.getMaxInstInLeaf());
      assertEquals(0.0, kDTree0.measureTreeSize(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree0.measurePerformanceTipText());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree0.nodeSplitterTipText());
      assertFalse(kDTree0.getMeasurePerformance());
      assertEquals(0.0, kDTree0.measureMaxDepth(), 0.01);
      assertEquals("The max number of instances in a leaf.", kDTree0.maxInstInLeafTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree0.distanceFunctionTipText());
      
      KDTree kDTree1 = new KDTree();
      assertNotNull(kDTree1);
      assertEquals(1, KDTree.MAX);
      assertEquals(0, KDTree.MIN);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(0.0, kDTree1.measureNumLeaves(), 0.01);
      assertEquals(0.0, kDTree1.measureMaxDepth(), 0.01);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree1.distanceFunctionTipText());
      assertFalse(kDTree1.getMeasurePerformance());
      assertEquals("The max number of instances in a leaf.", kDTree1.maxInstInLeafTipText());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree1.nodeSplitterTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree1.measurePerformanceTipText());
      assertEquals(0.01, kDTree1.getMinBoxRelWidth(), 0.01);
      assertEquals(0.0, kDTree1.measureTreeSize(), 0.01);
      assertEquals(40, kDTree1.getMaxInstInLeaf());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree1.minBoxRelWidthTipText());
      assertTrue(kDTree1.getNormalizeNodeWidth());
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree1.normalizeNodeWidthTipText());
      assertFalse(kDTree1.equals((Object)kDTree0));
      
      WekaTaskMonitor wekaTaskMonitor0 = new WekaTaskMonitor();
      assertNotNull(wekaTaskMonitor0);
      assertFalse(wekaTaskMonitor0.getIgnoreRepaint());
      assertFalse(wekaTaskMonitor0.isFocusTraversalPolicyProvider());
      assertFalse(wekaTaskMonitor0.isFocusCycleRoot());
      assertFalse(wekaTaskMonitor0.isFocusTraversalPolicySet());
      assertTrue(wekaTaskMonitor0.getFocusTraversalKeysEnabled());
      
      MouseWheelListener mouseWheelListener0 = mock(MouseWheelListener.class, new ViolatedAssumptionAnswer());
      MouseWheelListener mouseWheelListener1 = AWTEventMulticaster.remove(mouseWheelListener0, mouseWheelListener0);
      assertNull(mouseWheelListener1);
      
      MouseWheelListener mouseWheelListener2 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener2);
      
      MouseWheelListener mouseWheelListener3 = AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener3);
      
      MouseWheelListener mouseWheelListener4 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener4);
      
      MouseWheelListener mouseWheelListener5 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener5);
      
      WekaTaskMonitor wekaTaskMonitor1 = new WekaTaskMonitor();
      assertNotNull(wekaTaskMonitor1);
      assertFalse(wekaTaskMonitor1.isFocusTraversalPolicyProvider());
      assertFalse(wekaTaskMonitor1.getIgnoreRepaint());
      assertFalse(wekaTaskMonitor1.isFocusTraversalPolicySet());
      assertTrue(wekaTaskMonitor1.getFocusTraversalKeysEnabled());
      assertFalse(wekaTaskMonitor1.isFocusCycleRoot());
      assertFalse(wekaTaskMonitor1.equals((Object)wekaTaskMonitor0));
      
      wekaTaskMonitor1.addMouseWheelListener((MouseWheelListener) null);
      assertFalse(wekaTaskMonitor1.isFocusTraversalPolicyProvider());
      assertFalse(wekaTaskMonitor1.getIgnoreRepaint());
      assertFalse(wekaTaskMonitor1.isFocusTraversalPolicySet());
      assertTrue(wekaTaskMonitor1.getFocusTraversalKeysEnabled());
      assertFalse(wekaTaskMonitor1.isFocusCycleRoot());
      assertFalse(wekaTaskMonitor1.equals((Object)wekaTaskMonitor0));
      assertNotSame(wekaTaskMonitor1, wekaTaskMonitor0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      
      String string0 = Evaluation.getGlobalInfo(linearRegression1);
      assertEquals("\nSynopsis for weka.classifiers.functions.LinearRegression:\n\nClass for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", string0);
      assertNotNull(string0);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression1.minimalTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression1.ridgeTipText());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression1.attributeSelectionMethodTipText());
      assertEquals("Eliminate colinear attributes.", linearRegression1.eliminateColinearAttributesTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression1.debugTipText());
      assertFalse(linearRegression1.getMinimal());
      assertTrue(linearRegression1.getEliminateColinearAttributes());
      assertEquals(1.0E-8, linearRegression1.getRidge(), 0.01);
      assertFalse(linearRegression1.getDebug());
      assertFalse(linearRegression1.equals((Object)linearRegression0));
      assertNotSame(linearRegression1, linearRegression0);
      
      double double0 = evaluation0.m_SumAbsErr;
      assertEquals(0.0, double0, 0.01);
      
      ArffLoader arffLoader0 = new ArffLoader();
      assertNotNull(arffLoader0);
      assertFalse(arffLoader0.getUseRelativePath());
      assertEquals("Arff data files", arffLoader0.getFileDescription());
      assertEquals("Use relative rather than absolute paths", arffLoader0.useRelativePathTipText());
      assertEquals(".arff", arffLoader0.getFileExtension());
      assertEquals("http://", arffLoader0.retrieveURL());
      assertEquals("Reads a source that is in arff (attribute relation file format) format. ", arffLoader0.globalInfo());
      
      Instance instance0 = arffLoader0.getNextInstance(instances0);
      assertNull(instance0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(arffLoader0.getUseRelativePath());
      assertEquals("Arff data files", arffLoader0.getFileDescription());
      assertEquals("Use relative rather than absolute paths", arffLoader0.useRelativePathTipText());
      assertEquals(".arff", arffLoader0.getFileExtension());
      assertEquals("http://", arffLoader0.retrieveURL());
      assertEquals("Reads a source that is in arff (attribute relation file format) format. ", arffLoader0.globalInfo());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      
      try { 
        evaluation0.evaluateModelOnce(doubleArray0, (Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 24
  /*Coverage entropy=2.2065052252090784
  */
  @Test(timeout = 4000)
  public void test024()  throws Throwable  {
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getNoClass());
      
      boolean boolean0 = FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "=q|6xt>ckl[Y");
      assertFalse(boolean0);
      
      LinearRegression linearRegression0 = new LinearRegression();
      assertNotNull(linearRegression0);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression0.minimalTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression0.globalInfo());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression0.attributeSelectionMethodTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression0.ridgeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression0.debugTipText());
      assertTrue(linearRegression0.getEliminateColinearAttributes());
      assertEquals(1.0E-8, linearRegression0.getRidge(), 0.01);
      assertFalse(linearRegression0.getDebug());
      assertFalse(linearRegression0.getMinimal());
      assertEquals("Eliminate colinear attributes.", linearRegression0.eliminateColinearAttributesTipText());
      
      Enumeration enumeration0 = linearRegression0.listOptions();
      assertNotNull(enumeration0);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression0.minimalTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression0.globalInfo());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression0.attributeSelectionMethodTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression0.ridgeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression0.debugTipText());
      assertTrue(linearRegression0.getEliminateColinearAttributes());
      assertEquals(1.0E-8, linearRegression0.getRidge(), 0.01);
      assertFalse(linearRegression0.getDebug());
      assertFalse(linearRegression0.getMinimal());
      assertEquals("Eliminate colinear attributes.", linearRegression0.eliminateColinearAttributesTipText());
      
      SelectedTag selectedTag0 = linearRegression0.getAttributeSelectionMethod();
      assertNotNull(selectedTag0);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression0.minimalTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression0.globalInfo());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression0.attributeSelectionMethodTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression0.ridgeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression0.debugTipText());
      assertTrue(linearRegression0.getEliminateColinearAttributes());
      assertEquals(1.0E-8, linearRegression0.getRidge(), 0.01);
      assertFalse(linearRegression0.getDebug());
      assertFalse(linearRegression0.getMinimal());
      assertEquals("Eliminate colinear attributes.", linearRegression0.eliminateColinearAttributesTipText());
      assertEquals("0", selectedTag0.toString());
      
      TestInstances testInstances1 = new TestInstances();
      assertNotNull(testInstances1);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertFalse(testInstances1.getNoClass());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(0, testInstances1.getNumRelationalDate());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(0, testInstances1.getNumDate());
      assertEquals(2, testInstances1.getNumAttributes());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(0, testInstances1.getNumRelationalNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(0, testInstances1.getNumString());
      assertEquals(0, testInstances1.getNumNumeric());
      assertFalse(testInstances1.equals((Object)testInstances0));
      
      LogitBoost logitBoost0 = new LogitBoost();
      assertNotNull(logitBoost0);
      assertEquals("The base classifier to be used.", logitBoost0.classifierTipText());
      assertEquals("The random number seed to be used.", logitBoost0.seedTipText());
      assertEquals("Number of folds for internal cross-validation (default 0 means no cross-validation is performed).", logitBoost0.numFoldsTipText());
      assertEquals(1, logitBoost0.getNumRuns());
      assertEquals("Number of runs for internal cross-validation.", logitBoost0.numRunsTipText());
      assertEquals("Shrinkage parameter (use small value like 0.1 to reduce overfitting).", logitBoost0.shrinkageTipText());
      assertEquals("Weight threshold for weight pruning (reduce to 90 for speeding up learning process).", logitBoost0.weightThresholdTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", logitBoost0.debugTipText());
      assertEquals(100, logitBoost0.getWeightThreshold());
      assertEquals(1.0, logitBoost0.getShrinkage(), 0.01);
      assertEquals("Threshold on improvement in likelihood.", logitBoost0.likelihoodThresholdTipText());
      assertEquals("Whether resampling is used instead of reweighting.", logitBoost0.useResamplingTipText());
      assertEquals("The number of iterations to be performed.", logitBoost0.numIterationsTipText());
      assertFalse(logitBoost0.getUseResampling());
      assertEquals((-1.7976931348623157E308), logitBoost0.getLikelihoodThreshold(), 0.01);
      assertEquals(1, logitBoost0.getSeed());
      assertFalse(logitBoost0.getDebug());
      assertEquals(10, logitBoost0.getNumIterations());
      assertEquals(0, logitBoost0.getNumFolds());
      
      Capabilities capabilities1 = logitBoost0.getCapabilities();
      assertNotNull(capabilities1);
      assertEquals("The base classifier to be used.", logitBoost0.classifierTipText());
      assertEquals("The random number seed to be used.", logitBoost0.seedTipText());
      assertEquals("Number of folds for internal cross-validation (default 0 means no cross-validation is performed).", logitBoost0.numFoldsTipText());
      assertEquals(1, logitBoost0.getNumRuns());
      assertEquals("Number of runs for internal cross-validation.", logitBoost0.numRunsTipText());
      assertEquals("Shrinkage parameter (use small value like 0.1 to reduce overfitting).", logitBoost0.shrinkageTipText());
      assertEquals("Weight threshold for weight pruning (reduce to 90 for speeding up learning process).", logitBoost0.weightThresholdTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", logitBoost0.debugTipText());
      assertEquals(100, logitBoost0.getWeightThreshold());
      assertEquals(1.0, logitBoost0.getShrinkage(), 0.01);
      assertEquals("Threshold on improvement in likelihood.", logitBoost0.likelihoodThresholdTipText());
      assertEquals("Whether resampling is used instead of reweighting.", logitBoost0.useResamplingTipText());
      assertEquals("The number of iterations to be performed.", logitBoost0.numIterationsTipText());
      assertFalse(logitBoost0.getUseResampling());
      assertEquals((-1.7976931348623157E308), logitBoost0.getLikelihoodThreshold(), 0.01);
      assertEquals(1, logitBoost0.getSeed());
      assertFalse(logitBoost0.getDebug());
      assertEquals(10, logitBoost0.getNumIterations());
      assertEquals(0, logitBoost0.getNumFolds());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertTrue(capabilities1.hasDependencies());
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities1, capabilities0);
      
      capabilities0.assign(capabilities1);
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals("The base classifier to be used.", logitBoost0.classifierTipText());
      assertEquals("The random number seed to be used.", logitBoost0.seedTipText());
      assertEquals("Number of folds for internal cross-validation (default 0 means no cross-validation is performed).", logitBoost0.numFoldsTipText());
      assertEquals(1, logitBoost0.getNumRuns());
      assertEquals("Number of runs for internal cross-validation.", logitBoost0.numRunsTipText());
      assertEquals("Shrinkage parameter (use small value like 0.1 to reduce overfitting).", logitBoost0.shrinkageTipText());
      assertEquals("Weight threshold for weight pruning (reduce to 90 for speeding up learning process).", logitBoost0.weightThresholdTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", logitBoost0.debugTipText());
      assertEquals(100, logitBoost0.getWeightThreshold());
      assertEquals(1.0, logitBoost0.getShrinkage(), 0.01);
      assertEquals("Threshold on improvement in likelihood.", logitBoost0.likelihoodThresholdTipText());
      assertEquals("Whether resampling is used instead of reweighting.", logitBoost0.useResamplingTipText());
      assertEquals("The number of iterations to be performed.", logitBoost0.numIterationsTipText());
      assertFalse(logitBoost0.getUseResampling());
      assertEquals((-1.7976931348623157E308), logitBoost0.getLikelihoodThreshold(), 0.01);
      assertEquals(1, logitBoost0.getSeed());
      assertFalse(logitBoost0.getDebug());
      assertEquals(10, logitBoost0.getNumIterations());
      assertEquals(0, logitBoost0.getNumFolds());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertTrue(capabilities1.hasDependencies());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getNoClass());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.classIndex());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances0, testInstances1);
      
      KDTree kDTree0 = new KDTree();
      assertNotNull(kDTree0);
      assertEquals(0, KDTree.MIN);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(1, KDTree.MAX);
      assertEquals(40, kDTree0.getMaxInstInLeaf());
      assertTrue(kDTree0.getNormalizeNodeWidth());
      assertEquals(0.0, kDTree0.measureTreeSize(), 0.01);
      assertEquals(0.01, kDTree0.getMinBoxRelWidth(), 0.01);
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree0.normalizeNodeWidthTipText());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree0.nodeSplitterTipText());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree0.minBoxRelWidthTipText());
      assertFalse(kDTree0.getMeasurePerformance());
      assertEquals(0.0, kDTree0.measureNumLeaves(), 0.01);
      assertEquals(0.0, kDTree0.measureMaxDepth(), 0.01);
      assertEquals("The max number of instances in a leaf.", kDTree0.maxInstInLeafTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree0.distanceFunctionTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree0.measurePerformanceTipText());
      
      KDTree kDTree1 = new KDTree();
      assertNotNull(kDTree1);
      assertEquals(1, KDTree.MAX);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(0, KDTree.MIN);
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree1.normalizeNodeWidthTipText());
      assertEquals(0.0, kDTree1.measureMaxDepth(), 0.01);
      assertEquals(0.01, kDTree1.getMinBoxRelWidth(), 0.01);
      assertEquals(0.0, kDTree1.measureNumLeaves(), 0.01);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree1.distanceFunctionTipText());
      assertEquals(40, kDTree1.getMaxInstInLeaf());
      assertTrue(kDTree1.getNormalizeNodeWidth());
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree1.measurePerformanceTipText());
      assertEquals(0.0, kDTree1.measureTreeSize(), 0.01);
      assertFalse(kDTree1.getMeasurePerformance());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree1.minBoxRelWidthTipText());
      assertEquals("The max number of instances in a leaf.", kDTree1.maxInstInLeafTipText());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree1.nodeSplitterTipText());
      assertFalse(kDTree1.equals((Object)kDTree0));
      
      WekaTaskMonitor wekaTaskMonitor0 = new WekaTaskMonitor();
      assertNotNull(wekaTaskMonitor0);
      assertFalse(wekaTaskMonitor0.getIgnoreRepaint());
      assertFalse(wekaTaskMonitor0.isFocusCycleRoot());
      assertFalse(wekaTaskMonitor0.isFocusTraversalPolicyProvider());
      assertFalse(wekaTaskMonitor0.isFocusTraversalPolicySet());
      assertTrue(wekaTaskMonitor0.getFocusTraversalKeysEnabled());
      
      MouseWheelListener mouseWheelListener0 = mock(MouseWheelListener.class, new ViolatedAssumptionAnswer());
      MouseWheelListener mouseWheelListener1 = AWTEventMulticaster.add(mouseWheelListener0, mouseWheelListener0);
      assertNotNull(mouseWheelListener1);
      
      MouseWheelListener mouseWheelListener2 = AWTEventMulticaster.remove(mouseWheelListener1, mouseWheelListener1);
      assertNull(mouseWheelListener2);
      
      MouseWheelListener mouseWheelListener3 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener3);
      
      MouseWheelListener mouseWheelListener4 = AWTEventMulticaster.add((MouseWheelListener) null, mouseWheelListener1);
      assertNotNull(mouseWheelListener4);
      assertSame(mouseWheelListener1, mouseWheelListener4);
      assertSame(mouseWheelListener4, mouseWheelListener1);
      
      MouseWheelListener mouseWheelListener5 = AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener5);
      
      MouseWheelListener mouseWheelListener6 = AWTEventMulticaster.remove(mouseWheelListener4, (MouseWheelListener) null);
      assertNotNull(mouseWheelListener6);
      assertSame(mouseWheelListener1, mouseWheelListener4);
      assertSame(mouseWheelListener1, mouseWheelListener6);
      assertSame(mouseWheelListener4, mouseWheelListener6);
      assertSame(mouseWheelListener4, mouseWheelListener1);
      assertSame(mouseWheelListener6, mouseWheelListener4);
      assertSame(mouseWheelListener6, mouseWheelListener1);
      
      MouseWheelListener mouseWheelListener7 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener7);
      
      WekaTaskMonitor wekaTaskMonitor1 = new WekaTaskMonitor();
      assertNotNull(wekaTaskMonitor1);
      assertFalse(wekaTaskMonitor1.isFocusTraversalPolicyProvider());
      assertTrue(wekaTaskMonitor1.getFocusTraversalKeysEnabled());
      assertFalse(wekaTaskMonitor1.isFocusTraversalPolicySet());
      assertFalse(wekaTaskMonitor1.getIgnoreRepaint());
      assertFalse(wekaTaskMonitor1.isFocusCycleRoot());
      assertFalse(wekaTaskMonitor1.equals((Object)wekaTaskMonitor0));
      
      wekaTaskMonitor1.addMouseWheelListener(mouseWheelListener4);
      assertFalse(wekaTaskMonitor1.isFocusTraversalPolicyProvider());
      assertTrue(wekaTaskMonitor1.getFocusTraversalKeysEnabled());
      assertFalse(wekaTaskMonitor1.isFocusTraversalPolicySet());
      assertFalse(wekaTaskMonitor1.getIgnoreRepaint());
      assertFalse(wekaTaskMonitor1.isFocusCycleRoot());
      assertFalse(wekaTaskMonitor1.equals((Object)wekaTaskMonitor0));
      assertSame(mouseWheelListener1, mouseWheelListener4);
      assertSame(mouseWheelListener1, mouseWheelListener6);
      assertSame(mouseWheelListener4, mouseWheelListener6);
      assertSame(mouseWheelListener4, mouseWheelListener1);
      assertNotSame(wekaTaskMonitor1, wekaTaskMonitor0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getNoClass());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.classIndex());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances0.equals((Object)testInstances1));
      
      String string0 = Evaluation.getGlobalInfo(linearRegression0);
      assertEquals("\nSynopsis for weka.classifiers.functions.LinearRegression:\n\nClass for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", string0);
      assertNotNull(string0);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression0.minimalTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression0.globalInfo());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression0.attributeSelectionMethodTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression0.ridgeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression0.debugTipText());
      assertTrue(linearRegression0.getEliminateColinearAttributes());
      assertEquals(1.0E-8, linearRegression0.getRidge(), 0.01);
      assertFalse(linearRegression0.getDebug());
      assertFalse(linearRegression0.getMinimal());
      assertEquals("Eliminate colinear attributes.", linearRegression0.eliminateColinearAttributesTipText());
      
      double double0 = evaluation0.m_SumAbsErr;
      assertEquals(0.0, double0, 0.01);
      
      ArffLoader arffLoader0 = new ArffLoader();
      assertNotNull(arffLoader0);
      assertEquals(".arff", arffLoader0.getFileExtension());
      assertFalse(arffLoader0.getUseRelativePath());
      assertEquals("Use relative rather than absolute paths", arffLoader0.useRelativePathTipText());
      assertEquals("Arff data files", arffLoader0.getFileDescription());
      assertEquals("http://", arffLoader0.retrieveURL());
      assertEquals("Reads a source that is in arff (attribute relation file format) format. ", arffLoader0.globalInfo());
      
      SparseInstance sparseInstance0 = new SparseInstance(2);
      assertNotNull(sparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(2, sparseInstance0.numAttributes());
      assertEquals(2, sparseInstance0.numValues());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      
      double[] doubleArray0 = new double[2];
      boolean boolean1 = FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      doubleArray0[0] = 0.0011201687076767312;
      doubleArray0[1] = (double) 1;
      try { 
        evaluation0.evaluateModelOnce(doubleArray0, (Instance) sparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 25
  /*Coverage entropy=3.5192133936383834
  */
  @Test(timeout = 4000)
  public void test025()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      
      Double double0 = new Double((-1935.4593093));
      assertEquals((-1935.4593093), (double)double0, 0.01);
      assertNotNull(double0);
      
      Double double1 = new Double((-2.147483648E9));
      assertEquals((-2.147483648E9), (double)double1, 0.01);
      assertNotNull(double1);
      assertNotEquals((double)double1, (double)double0, 0.01);
      
      instances0.deleteStringAttributes();
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      
      String string0 = evaluation0.toClassDetailsString("main");
      assertEquals("main\n                 TP Rate  FP Rate  Precision  Recall  F-Measure  MCC    ROC Area  PRC Area  Class\n                 0        0        0          0       0          0     ?         ?         class1\n                 0        0        0          0       0          0     ?         ?         class2\nWeighted Avg.  NaN      NaN      NaN        NaN     NaN        NaN    NaN       NaN    \n", string0);
      assertNotNull(string0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      
      Evaluation evaluation1 = new Evaluation(instances0);
      assertNotNull(evaluation1);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      double double2 = evaluation1.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(evaluation1, evaluation0);
      
      evaluation0.m_WithClass = (-1935.4593093);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(-0.0, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(-0.0, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(-0.0, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(-0.0, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(-0.0, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(-0.0, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(-0.0, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(-0.0, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(-0.0, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(-0.0, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(-0.0, evaluation0.avgCost(), 0.01);
      assertEquals(-0.0, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(-0.0, evaluation0.pctUnclassified(), 0.01);
      assertEquals(-0.0, evaluation0.pctCorrect(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals((-1935.4593093), evaluation0.numInstances(), 0.01);
      
      double double3 = evaluation1.weightedTruePositiveRate();
      assertEquals(Double.NaN, double3, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertFalse(evaluation1.equals((Object)evaluation0));
      assertEquals(double3, double2, 0.01);
      assertNotSame(evaluation1, evaluation0);
      
      double double4 = evaluation0.sizeOfPredictedRegions();
      assertEquals(-0.0, double4, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(-0.0, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(-0.0, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(-0.0, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(-0.0, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(-0.0, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(-0.0, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(-0.0, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(-0.0, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(-0.0, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(-0.0, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(-0.0, evaluation0.avgCost(), 0.01);
      assertEquals(-0.0, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(-0.0, evaluation0.pctUnclassified(), 0.01);
      assertEquals(-0.0, evaluation0.pctCorrect(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals((-1935.4593093), evaluation0.numInstances(), 0.01);
      assertFalse(evaluation0.equals((Object)evaluation1));
      assertNotEquals(double4, double3, 0.01);
      assertNotEquals(double4, double2, 0.01);
      assertNotSame(evaluation0, evaluation1);
  }

  /**
  //Test case number: 26
  /*Coverage entropy=1.3609093299839157
  */
  @Test(timeout = 4000)
  public void test026()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      
      TextDirectoryLoader textDirectoryLoader1 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader1);
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      
      Instances instances1 = textDirectoryLoader1.getDataSet();
      assertNotNull(instances1);
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals(0, instances1.numInstances());
      assertEquals(2, instances1.numAttributes());
      assertEquals(1, instances1.classIndex());
      assertEquals(0, instances1.numClasses());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertFalse(instances1.equals((Object)instances0));
      assertNotSame(textDirectoryLoader1, textDirectoryLoader0);
      assertNotSame(instances1, instances0);
      
      evaluation0.setPriors(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertFalse(instances0.equals((Object)instances1));
      assertNotSame(instances0, instances1);
      
      KDTree kDTree0 = new KDTree();
      assertNotNull(kDTree0);
      assertEquals(0, KDTree.MIN);
      assertEquals(1, KDTree.MAX);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(0.0, kDTree0.measureMaxDepth(), 0.01);
      assertEquals(0.0, kDTree0.measureNumLeaves(), 0.01);
      assertEquals("The max number of instances in a leaf.", kDTree0.maxInstInLeafTipText());
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree0.normalizeNodeWidthTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree0.distanceFunctionTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree0.measurePerformanceTipText());
      assertEquals(0.01, kDTree0.getMinBoxRelWidth(), 0.01);
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree0.nodeSplitterTipText());
      assertFalse(kDTree0.getMeasurePerformance());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree0.minBoxRelWidthTipText());
      assertEquals(0.0, kDTree0.measureTreeSize(), 0.01);
      assertEquals(40, kDTree0.getMaxInstInLeaf());
      assertTrue(kDTree0.getNormalizeNodeWidth());
      
      KDTree kDTree1 = new KDTree();
      assertNotNull(kDTree1);
      assertEquals(0, KDTree.MIN);
      assertEquals(2, KDTree.WIDTH);
      assertEquals(1, KDTree.MAX);
      assertEquals(0.0, kDTree1.measureTreeSize(), 0.01);
      assertEquals(0.01, kDTree1.getMinBoxRelWidth(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree1.measurePerformanceTipText());
      assertEquals(40, kDTree1.getMaxInstInLeaf());
      assertTrue(kDTree1.getNormalizeNodeWidth());
      assertEquals(0.0, kDTree1.measureNumLeaves(), 0.01);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree1.distanceFunctionTipText());
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree1.nodeSplitterTipText());
      assertFalse(kDTree1.getMeasurePerformance());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree1.minBoxRelWidthTipText());
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree1.normalizeNodeWidthTipText());
      assertEquals(0.0, kDTree1.measureMaxDepth(), 0.01);
      assertEquals("The max number of instances in a leaf.", kDTree1.maxInstInLeafTipText());
      assertFalse(kDTree1.equals((Object)kDTree0));
      
      RandomSubSpace randomSubSpace0 = new RandomSubSpace();
      assertNotNull(randomSubSpace0);
      assertEquals("The number of iterations to be performed.", randomSubSpace0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", randomSubSpace0.debugTipText());
      assertEquals(1, randomSubSpace0.getNumExecutionSlots());
      assertEquals("Size of each subSpace: if less than 1 as a percentage of the number of attributes, otherwise the absolute number of attributes.", randomSubSpace0.subSpaceSizeTipText());
      assertEquals("The random number seed to be used.", randomSubSpace0.seedTipText());
      assertFalse(randomSubSpace0.getDebug());
      assertEquals("The base classifier to be used.", randomSubSpace0.classifierTipText());
      assertEquals(1, randomSubSpace0.getSeed());
      assertEquals(0.5, randomSubSpace0.getSubSpaceSize(), 0.01);
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", randomSubSpace0.numExecutionSlotsTipText());
      assertEquals(10, randomSubSpace0.getNumIterations());
      
      RandomSubSpace randomSubSpace1 = new RandomSubSpace();
      assertNotNull(randomSubSpace1);
      assertEquals("The number of iterations to be performed.", randomSubSpace1.numIterationsTipText());
      assertEquals(1, randomSubSpace1.getNumExecutionSlots());
      assertEquals("Size of each subSpace: if less than 1 as a percentage of the number of attributes, otherwise the absolute number of attributes.", randomSubSpace1.subSpaceSizeTipText());
      assertEquals("The base classifier to be used.", randomSubSpace1.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", randomSubSpace1.debugTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", randomSubSpace1.numExecutionSlotsTipText());
      assertEquals(10, randomSubSpace1.getNumIterations());
      assertEquals(0.5, randomSubSpace1.getSubSpaceSize(), 0.01);
      assertFalse(randomSubSpace1.getDebug());
      assertEquals(1, randomSubSpace1.getSeed());
      assertEquals("The random number seed to be used.", randomSubSpace1.seedTipText());
      assertFalse(randomSubSpace1.equals((Object)randomSubSpace0));
      
      randomSubSpace1.setDebug(false);
      assertEquals("The number of iterations to be performed.", randomSubSpace1.numIterationsTipText());
      assertEquals(1, randomSubSpace1.getNumExecutionSlots());
      assertEquals("Size of each subSpace: if less than 1 as a percentage of the number of attributes, otherwise the absolute number of attributes.", randomSubSpace1.subSpaceSizeTipText());
      assertEquals("The base classifier to be used.", randomSubSpace1.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", randomSubSpace1.debugTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", randomSubSpace1.numExecutionSlotsTipText());
      assertEquals(10, randomSubSpace1.getNumIterations());
      assertEquals(0.5, randomSubSpace1.getSubSpaceSize(), 0.01);
      assertFalse(randomSubSpace1.getDebug());
      assertEquals(1, randomSubSpace1.getSeed());
      assertEquals("The random number seed to be used.", randomSubSpace1.seedTipText());
      assertFalse(randomSubSpace1.equals((Object)randomSubSpace0));
      assertNotSame(randomSubSpace1, randomSubSpace0);
      
      RandomSubSpace[] randomSubSpaceArray0 = new RandomSubSpace[8];
      randomSubSpaceArray0[0] = randomSubSpace0;
      randomSubSpaceArray0[1] = randomSubSpace1;
      randomSubSpaceArray0[2] = randomSubSpace0;
      randomSubSpaceArray0[3] = randomSubSpace0;
      randomSubSpaceArray0[4] = randomSubSpace0;
      randomSubSpaceArray0[5] = randomSubSpace0;
      randomSubSpaceArray0[6] = randomSubSpace0;
      randomSubSpaceArray0[7] = randomSubSpace1;
      // Undeclared exception!
      try { 
        instances0.toArray(randomSubSpaceArray0);
        fail("Expecting exception: ArrayStoreException");
      
      } catch(ArrayStoreException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 27
  /*Coverage entropy=3.173784013015998
  */
  @Test(timeout = 4000)
  public void test027()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      
      Evaluation evaluation1 = new Evaluation(instances0);
      assertNotNull(evaluation1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      Evaluation evaluation2 = new Evaluation(instances0);
      assertNotNull(evaluation2);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      
      double double0 = evaluation2.SFMeanPriorEntropy();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertNotSame(evaluation2, evaluation0);
      assertNotSame(evaluation2, evaluation1);
      
      double double1 = evaluation2.incorrect();
      assertEquals(0.0, double1, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertNotEquals(double1, double0, 0.01);
      assertNotSame(evaluation2, evaluation0);
      assertNotSame(evaluation2, evaluation1);
      
      testInstances0.setNumNominal(1432);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1432, testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      
      instances0.deleteStringAttributes();
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1432, testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      assertNotNull(gridBagLayout0);
      
      GridBagLayout gridBagLayout1 = new GridBagLayout();
      assertNotNull(gridBagLayout1);
      assertFalse(gridBagLayout1.equals((Object)gridBagLayout0));
      
      GridBagLayout gridBagLayout2 = new GridBagLayout();
      assertNotNull(gridBagLayout2);
      assertFalse(gridBagLayout2.equals((Object)gridBagLayout1));
      assertFalse(gridBagLayout2.equals((Object)gridBagLayout0));
      
      Point point0 = gridBagLayout2.getLayoutOrigin();
      assertNotNull(point0);
      assertEquals(0, point0.x);
      assertEquals(0, point0.y);
      assertEquals(0.0, point0.getY(), 0.01);
      assertEquals(0.0, point0.getX(), 0.01);
      assertFalse(gridBagLayout2.equals((Object)gridBagLayout1));
      assertFalse(gridBagLayout2.equals((Object)gridBagLayout0));
      assertNotSame(gridBagLayout2, gridBagLayout1);
      assertNotSame(gridBagLayout2, gridBagLayout0);
      
      GridBagLayout gridBagLayout3 = new GridBagLayout();
      assertNotNull(gridBagLayout3);
      assertFalse(gridBagLayout3.equals((Object)gridBagLayout2));
      assertFalse(gridBagLayout3.equals((Object)gridBagLayout0));
      assertFalse(gridBagLayout3.equals((Object)gridBagLayout1));
      
      double double2 = evaluation1.falsePositiveRate(120);
      assertEquals(0.0, double2, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1432, testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertTrue(evaluation1.equals((Object)evaluation2));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertEquals(double2, double1, 0.01);
      assertNotEquals(double2, double0, 0.01);
      assertNotSame(evaluation1, evaluation0);
      assertNotSame(evaluation1, evaluation2);
      
      double double3 = evaluation0.weightedFalseNegativeRate();
      assertEquals(Double.NaN, double3, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1432, testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertTrue(evaluation0.equals((Object)evaluation2));
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertEquals(double3, double0, 0.01);
      assertNotEquals(double3, double2, 0.01);
      assertNotEquals(double3, double1, 0.01);
      assertNotSame(evaluation0, evaluation1);
      assertNotSame(evaluation0, evaluation2);
      
      double double4 = evaluation1.SFMeanSchemeEntropy();
      assertEquals(Double.NaN, double4, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1432, testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(double4, double0, 0.01);
      assertNotEquals(double4, double2, 0.01);
      assertEquals(double4, double3, 0.01);
      assertNotEquals(double4, double1, 0.01);
      assertTrue(evaluation1.equals((Object)evaluation2));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(evaluation1, evaluation0);
      assertNotSame(evaluation1, evaluation2);
      
      double double5 = evaluation1.weightedMatthewsCorrelation();
      assertEquals(Double.NaN, double5, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1432, testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(double5, double3, 0.01);
      assertNotEquals(double5, double2, 0.01);
      assertEquals(double5, double4, 0.01);
      assertNotEquals(double5, double1, 0.01);
      assertEquals(double5, double0, 0.01);
      assertTrue(evaluation1.equals((Object)evaluation2));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(evaluation1, evaluation0);
      assertNotSame(evaluation1, evaluation2);
      
      double double6 = evaluation0.weightedPrecision();
      assertEquals(Double.NaN, double6, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1432, testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(double6, double5, 0.01);
      assertEquals(double6, double0, 0.01);
      assertEquals(double6, double4, 0.01);
      assertNotEquals(double6, double1, 0.01);
      assertEquals(double6, double3, 0.01);
      assertNotEquals(double6, double2, 0.01);
      assertTrue(evaluation0.equals((Object)evaluation2));
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotSame(evaluation0, evaluation1);
      assertNotSame(evaluation0, evaluation2);
      
      double double7 = evaluation1.falseNegativeRate(1432);
      assertEquals(0.0, double7, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1432, testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertNotEquals(double7, double3, 0.01);
      assertNotEquals(double7, double4, 0.01);
      assertNotEquals(double7, double5, 0.01);
      assertEquals(double7, double1, 0.01);
      assertNotEquals(double7, double0, 0.01);
      assertEquals(double7, double2, 0.01);
      assertNotEquals(double7, double6, 0.01);
      assertTrue(evaluation1.equals((Object)evaluation2));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(evaluation1, evaluation0);
      assertNotSame(evaluation1, evaluation2);
      
      Instances instances1 = evaluation2.getHeader();
      assertNotNull(instances1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1432, testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances1.classIndex());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(0, instances1.numInstances());
      assertEquals(2, instances1.numAttributes());
      assertEquals(2, instances1.numClasses());
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertFalse(instances1.equals((Object)instances0));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertNotSame(instances0, instances1);
      assertNotSame(instances1, instances0);
      assertNotSame(evaluation2, evaluation0);
      assertNotSame(evaluation2, evaluation1);
  }

  /**
  //Test case number: 28
  /*Coverage entropy=2.3109290309357102
  */
  @Test(timeout = 4000)
  public void test028()  throws Throwable  {
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals(1, lMT0.graphType());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals(1, lMT0.graphType());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(1, lMT0.graphType());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(1, lMT0.graphType());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      
      WrapperSubsetEval wrapperSubsetEval0 = new WrapperSubsetEval();
      assertNotNull(wrapperSubsetEval0);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      
      SelectedTag selectedTag0 = wrapperSubsetEval0.getEvaluationMeasure();
      assertNotNull(selectedTag0);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      assertEquals("1", selectedTag0.toString());
      
      MultiClassClassifierUpdateable multiClassClassifierUpdateable0 = new MultiClassClassifierUpdateable();
      assertNotNull(multiClassClassifierUpdateable0);
      assertEquals(3, MultiClassClassifier.METHOD_1_AGAINST_1);
      assertEquals(2, MultiClassClassifier.METHOD_ERROR_EXHAUSTIVE);
      assertEquals(1, MultiClassClassifier.METHOD_ERROR_RANDOM);
      assertEquals(0, MultiClassClassifier.METHOD_1_AGAINST_ALL);
      assertEquals(1, multiClassClassifierUpdateable0.getSeed());
      assertEquals("A metaclassifier for handling multi-class datasets with 2-class classifiers. This classifier is also capable of applying error correcting output codes for increased accuracy. The base classifier must be an updateable classifier", multiClassClassifierUpdateable0.globalInfo());
      assertFalse(multiClassClassifierUpdateable0.getUsePairwiseCoupling());
      assertEquals("The base classifier to be used.", multiClassClassifierUpdateable0.classifierTipText());
      assertEquals("Use pairwise coupling (only has an effect for 1-against-1).", multiClassClassifierUpdateable0.usePairwiseCouplingTipText());
      assertFalse(multiClassClassifierUpdateable0.getDebug());
      assertEquals("Sets the width multiplier when using random codes. The number of codes generated will be thus number multiplied by the number of classes.", multiClassClassifierUpdateable0.randomWidthFactorTipText());
      assertEquals(2.0, multiClassClassifierUpdateable0.getRandomWidthFactor(), 0.01);
      assertEquals("Sets the method to use for transforming the multi-class problem into several 2-class ones.", multiClassClassifierUpdateable0.methodTipText());
      assertEquals("The random number seed to be used.", multiClassClassifierUpdateable0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", multiClassClassifierUpdateable0.debugTipText());
      
      Capabilities capabilities1 = costSensitiveClassifier0.getCapabilities();
      assertNotNull(capabilities1);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertTrue(capabilities1.hasDependencies());
      assertFalse(costSensitiveClassifier0.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier0.equals((Object)costSensitiveClassifier1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(costSensitiveClassifier0, costSensitiveClassifier2);
      assertNotSame(costSensitiveClassifier0, costSensitiveClassifier1);
      assertNotSame(capabilities1, capabilities0);
      
      Instances instances1 = testInstances0.generate();
      assertNotNull(instances1);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(1, lMT0.graphType());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(20.0, instances1.sumOfWeights(), 0.01);
      assertEquals(20, instances1.numInstances());
      assertEquals(4, instances1.numAttributes());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(20, instances1.size());
      assertEquals(3, instances1.classIndex());
      assertEquals(4, instances1.numClasses());
      assertEquals("Testdata", instances1.relationName());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(instances1.equals((Object)instances0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(instances1, instances0);
      
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier3);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      
      CostSensitiveClassifier costSensitiveClassifier4 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier4);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      
      CostSensitiveClassifier costSensitiveClassifier5 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier5);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier5.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier5.minimizeExpectedCostTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier5.seedTipText());
      assertEquals(0, costSensitiveClassifier5.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier5.costMatrixTipText());
      assertEquals(1, costSensitiveClassifier5.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier5.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier5.globalInfo());
      assertFalse(costSensitiveClassifier5.getDebug());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier5.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier5.classifierTipText());
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier4));
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier1));
      
      CostSensitiveClassifier costSensitiveClassifier6 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier6);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier6.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier6.getMinimizeExpectedCost());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier6.costMatrixTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier6.globalInfo());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier6.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier6.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier6.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier6.debugTipText());
      assertEquals(1, costSensitiveClassifier6.getSeed());
      assertEquals(0, costSensitiveClassifier6.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier6.seedTipText());
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier4));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier5));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier2));
      
      CoverTree coverTree0 = new CoverTree();
      assertNotNull(coverTree0);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertEquals(1.3, coverTree0.getBase(), 0.01);
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01);
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01);
      assertFalse(coverTree0.getMeasurePerformance());
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      
      Instances instances2 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances2);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(1, instances2.classIndex());
      assertEquals(0, instances2.numClasses());
      assertFalse(instances2.equals((Object)instances0));
      assertFalse(instances2.equals((Object)instances1));
      assertNotSame(instances2, instances0);
      assertNotSame(instances2, instances1);
      
      Evaluation evaluation0 = new Evaluation(instances1);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(1, lMT0.graphType());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(20.0, instances1.sumOfWeights(), 0.01);
      assertEquals(20, instances1.numInstances());
      assertEquals(4, instances1.numAttributes());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(20, instances1.size());
      assertEquals(3, instances1.classIndex());
      assertEquals(4, instances1.numClasses());
      assertEquals("Testdata", instances1.relationName());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(instances1.equals((Object)instances2));
      assertFalse(instances1.equals((Object)instances0));
      
      try { 
        evaluation0.evaluateModelOnce(0.0032427481349873426, (Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 29
  /*Coverage entropy=2.3109290309357102
  */
  @Test(timeout = 4000)
  public void test029()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier3);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      
      CostMatrix costMatrix0 = costSensitiveClassifier3.getCostMatrix();
      assertNotNull(costMatrix0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.numRows());
      assertEquals(1, costMatrix0.size());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier2);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      
      MockRandom mockRandom0 = new MockRandom();
      assertNotNull(mockRandom0);
      
      double double0 = mockRandom0.nextGaussian();
      assertEquals(0.7, double0, 0.01);
      
      String string0 = costMatrix0.toMatlab();
      assertEquals("[0.0]", string0);
      assertNotNull(string0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.numRows());
      assertEquals(1, costMatrix0.size());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier2);
      
      MockRandom mockRandom1 = new MockRandom();
      assertNotNull(mockRandom1);
      assertFalse(mockRandom1.equals((Object)mockRandom0));
      
      costSensitiveClassifier3.setCostMatrix(costMatrix0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.numRows());
      assertEquals(1, costMatrix0.size());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier2);
      
      KDTree kDTree0 = new KDTree(instances0);
      assertNotNull(kDTree0);
      assertEquals(1, KDTree.MAX);
      assertEquals(0, KDTree.MIN);
      assertEquals(2, KDTree.WIDTH);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The the splitting method to split the nodes of the KDTree.", kDTree0.nodeSplitterTipText());
      assertFalse(kDTree0.getMeasurePerformance());
      assertEquals("The max number of instances in a leaf.", kDTree0.maxInstInLeafTipText());
      assertEquals(0.0, kDTree0.measureMaxDepth(), 0.01);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", kDTree0.distanceFunctionTipText());
      assertEquals("Whether if the widths of the KDTree node should be normalized by the width of the universe or not. Where, width of the node is the range of the split attribute based on the instances in that node, and width of the universe is the range of the split attribute based on all the instances (default: false).", kDTree0.normalizeNodeWidthTipText());
      assertEquals("The minimum relative width of the box. A node is only made a leaf if the width of the split dimension of the instances in a node normalized over the width of the split dimension of all the instances is less than or equal to this minimum relative width.", kDTree0.minBoxRelWidthTipText());
      assertTrue(kDTree0.getNormalizeNodeWidth());
      assertEquals(40, kDTree0.getMaxInstInLeaf());
      assertEquals(0.0, kDTree0.measureNumLeaves(), 0.01);
      assertEquals(0.0, kDTree0.measureTreeSize(), 0.01);
      assertEquals(0.01, kDTree0.getMinBoxRelWidth(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", kDTree0.measurePerformanceTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      
      int[] intArray0 = new int[5];
      intArray0[0] = 2;
      intArray0[1] = 1;
      intArray0[2] = (-1);
      intArray0[3] = 0;
      intArray0[4] = 1;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1, intArray0, 2296);
      assertNotNull(binarySparseInstance0);
      assertEquals(5, intArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new int[] {2, 1, (-1), 0, 1}, intArray0);
      assertEquals(2296, binarySparseInstance0.numAttributes());
      assertEquals(5, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(5, intArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new int[] {2, 1, (-1), 0, 1}, intArray0);
      assertEquals(2296, binarySparseInstance0.numAttributes());
      assertEquals(5, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(2296, binarySparseInstance1.numAttributes());
      assertEquals(5, binarySparseInstance1.numValues());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      SparseInstance sparseInstance0 = new SparseInstance((Instance) binarySparseInstance1);
      assertNotNull(sparseInstance0);
      assertEquals(5, intArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new int[] {2, 1, (-1), 0, 1}, intArray0);
      assertEquals(2296, binarySparseInstance0.numAttributes());
      assertEquals(5, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(2296, binarySparseInstance1.numAttributes());
      assertEquals(5, binarySparseInstance1.numValues());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertEquals(2296, sparseInstance0.numAttributes());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(5, sparseInstance0.numValues());
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance1));
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      SparseInstance sparseInstance1 = new SparseInstance(sparseInstance0);
      assertNotNull(sparseInstance1);
      assertEquals(5, intArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new int[] {2, 1, (-1), 0, 1}, intArray0);
      assertEquals(2296, binarySparseInstance0.numAttributes());
      assertEquals(5, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(2296, binarySparseInstance1.numAttributes());
      assertEquals(5, binarySparseInstance1.numValues());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertEquals(2296, sparseInstance0.numAttributes());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(5, sparseInstance0.numValues());
      assertEquals(2296, sparseInstance1.numAttributes());
      assertEquals(1.0, sparseInstance1.weight(), 0.01);
      assertEquals(5, sparseInstance1.numValues());
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance1));
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      assertFalse(sparseInstance1.equals((Object)sparseInstance0));
      
      try { 
        evaluation0.evaluateModelOnce((double) 1, (Instance) sparseInstance1);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 30
  /*Coverage entropy=2.380728999769376
  */
  @Test(timeout = 4000)
  public void test030()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      assertNotNull(capabilities1);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(capabilities0, capabilities1);
      
      TestInstances testInstances1 = TestInstances.forCapabilities(capabilities1);
      assertNotNull(testInstances1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(0, testInstances1.getNumString());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertFalse(testInstances1.getNoClass());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getClassType());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(testInstances1, testInstances0);
      
      Instances instances0 = testInstances1.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(0, testInstances1.getNumString());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertFalse(testInstances1.getNoClass());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numAttributes());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(testInstances1, testInstances0);
      
      instances0.deleteStringAttributes();
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(0, testInstances1.getNumString());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertFalse(testInstances1.getNoClass());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numAttributes());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(testInstances1, testInstances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(0, testInstances1.getNumString());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertFalse(testInstances1.getNoClass());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(testInstances1.equals((Object)testInstances0));
      
      double double0 = evaluation0.m_WithClass;
      assertEquals(0.0, double0, 0.01);
      
      double double1 = evaluation0.weightedTrueNegativeRate();
      assertEquals(Double.NaN, double1, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(0, testInstances1.getNumString());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertFalse(testInstances1.getNoClass());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertNotEquals(double1, double0, 0.01);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(testInstances1, testInstances0);
      
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      assertNotNull(findWithCapabilities0);
      assertEquals("", findWithCapabilities0.getFilename());
      
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      assertNotNull(findWithCapabilities1);
      assertEquals("", findWithCapabilities1.getFilename());
      assertFalse(findWithCapabilities1.equals((Object)findWithCapabilities0));
      
      Capabilities capabilities2 = findWithCapabilities0.getNotCapabilities();
      assertNotNull(capabilities2);
      assertEquals("", findWithCapabilities0.getFilename());
      assertFalse(capabilities2.hasDependencies());
      assertEquals(1, capabilities2.getMinimumNumberInstances());
      assertFalse(findWithCapabilities0.equals((Object)findWithCapabilities1));
      assertFalse(capabilities2.equals((Object)capabilities0));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertNotSame(findWithCapabilities0, findWithCapabilities1);
      assertNotSame(capabilities2, capabilities0);
      assertNotSame(capabilities2, capabilities1);
      
      FilteredClusterer filteredClusterer0 = new FilteredClusterer();
      assertNotNull(filteredClusterer0);
      assertEquals("The filter to be used.", filteredClusterer0.filterTipText());
      assertEquals("Class for running an arbitrary clusterer on data that has been passed through an arbitrary filter. Like the clusterer, the structure of the filter is based exclusively on the training data and test instances will be processed by the filter without changing their structure.", filteredClusterer0.globalInfo());
      assertEquals("The base clusterer to be used.", filteredClusterer0.clustererTipText());
      
      TestInstances testInstances2 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances2);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances2.getNumRelationalNominal());
      assertEquals(1, testInstances2.getSeed());
      assertEquals(0, testInstances2.getNumString());
      assertEquals((-1), testInstances2.getClassIndex());
      assertEquals(4, testInstances2.getNumClasses());
      assertEquals(0, testInstances2.getNumRelational());
      assertEquals(1, testInstances2.getNumDate());
      assertEquals(" ", testInstances2.getWordSeparators());
      assertEquals("Testdata", testInstances2.getRelation());
      assertEquals(1, testInstances2.getNumRelationalDate());
      assertEquals(2, testInstances2.getNumNominalValues());
      assertEquals(1, testInstances2.getNumNominal());
      assertEquals(4, testInstances2.getNumAttributes());
      assertEquals(1, testInstances2.getClassType());
      assertEquals(2, testInstances2.getNumRelationalNominalValues());
      assertEquals(20, testInstances2.getNumInstances());
      assertFalse(testInstances2.getNoClass());
      assertFalse(testInstances2.getMultiInstance());
      assertEquals(10, testInstances2.getNumInstancesRelational());
      assertEquals(1, testInstances2.getNumNumeric());
      assertEquals(1, testInstances2.getNumRelationalNumeric());
      assertEquals(0, testInstances2.getNumRelationalString());
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances2.equals((Object)testInstances0));
      assertFalse(testInstances2.equals((Object)testInstances1));
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances2, testInstances0);
      assertNotSame(testInstances2, testInstances1);
      
      TestInstances testInstances3 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances3);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances3.getNumInstances());
      assertFalse(testInstances3.getNoClass());
      assertFalse(testInstances3.getMultiInstance());
      assertEquals(1, testInstances3.getNumNumeric());
      assertEquals(0, testInstances3.getNumRelationalString());
      assertEquals(1, testInstances3.getNumRelationalNumeric());
      assertEquals(0, testInstances3.getNumString());
      assertEquals(2, testInstances3.getNumRelationalNominalValues());
      assertEquals((-1), testInstances3.getClassIndex());
      assertEquals(1, testInstances3.getClassType());
      assertEquals(1, testInstances3.getSeed());
      assertEquals(1, testInstances3.getNumRelationalNominal());
      assertEquals(4, testInstances3.getNumAttributes());
      assertEquals(4, testInstances3.getNumClasses());
      assertEquals(1, testInstances3.getNumDate());
      assertEquals(" ", testInstances3.getWordSeparators());
      assertEquals("Testdata", testInstances3.getRelation());
      assertEquals(1, testInstances3.getNumRelationalDate());
      assertEquals(0, testInstances3.getNumRelational());
      assertEquals(1, testInstances3.getNumNominal());
      assertEquals(2, testInstances3.getNumNominalValues());
      assertEquals(10, testInstances3.getNumInstancesRelational());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(testInstances3.equals((Object)testInstances2));
      assertFalse(testInstances3.equals((Object)testInstances0));
      assertFalse(testInstances3.equals((Object)testInstances1));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(testInstances3, testInstances2);
      assertNotSame(testInstances3, testInstances0);
      assertNotSame(testInstances3, testInstances1);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      
      testInstances2.setNumRelationalString((-2812));
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances2.getNumRelationalNominal());
      assertEquals(1, testInstances2.getSeed());
      assertEquals(0, testInstances2.getNumString());
      assertEquals((-1), testInstances2.getClassIndex());
      assertEquals(4, testInstances2.getNumClasses());
      assertEquals(0, testInstances2.getNumRelational());
      assertEquals(1, testInstances2.getNumDate());
      assertEquals(" ", testInstances2.getWordSeparators());
      assertEquals("Testdata", testInstances2.getRelation());
      assertEquals(1, testInstances2.getNumRelationalDate());
      assertEquals(2, testInstances2.getNumNominalValues());
      assertEquals(1, testInstances2.getNumNominal());
      assertEquals(4, testInstances2.getNumAttributes());
      assertEquals(1, testInstances2.getClassType());
      assertEquals(2, testInstances2.getNumRelationalNominalValues());
      assertEquals(20, testInstances2.getNumInstances());
      assertFalse(testInstances2.getNoClass());
      assertFalse(testInstances2.getMultiInstance());
      assertEquals(10, testInstances2.getNumInstancesRelational());
      assertEquals(1, testInstances2.getNumNumeric());
      assertEquals(1, testInstances2.getNumRelationalNumeric());
      assertEquals((-2812), testInstances2.getNumRelationalString());
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances2.equals((Object)testInstances0));
      assertFalse(testInstances2.equals((Object)testInstances3));
      assertFalse(testInstances2.equals((Object)testInstances1));
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances2, testInstances0);
      assertNotSame(testInstances2, testInstances3);
      assertNotSame(testInstances2, testInstances1);
      
      TestInstances testInstances4 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances4);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals(1, testInstances4.getClassType());
      assertEquals((-1), testInstances4.getClassIndex());
      assertEquals(1, testInstances4.getNumNominal());
      assertEquals(" ", testInstances4.getWordSeparators());
      assertEquals("Testdata", testInstances4.getRelation());
      assertEquals(1, testInstances4.getSeed());
      assertEquals(0, testInstances4.getNumString());
      assertEquals(1, testInstances4.getNumRelationalNumeric());
      assertEquals(0, testInstances4.getNumRelationalString());
      assertFalse(testInstances4.getMultiInstance());
      assertEquals(1, testInstances4.getNumRelationalNominal());
      assertEquals(1, testInstances4.getNumRelationalDate());
      assertEquals(20, testInstances4.getNumInstances());
      assertEquals(10, testInstances4.getNumInstancesRelational());
      assertEquals(1, testInstances4.getNumNumeric());
      assertEquals(2, testInstances4.getNumRelationalNominalValues());
      assertEquals(1, testInstances4.getNumDate());
      assertEquals(2, testInstances4.getNumNominalValues());
      assertEquals(0, testInstances4.getNumRelational());
      assertFalse(testInstances4.getNoClass());
      assertEquals(4, testInstances4.getNumAttributes());
      assertEquals(4, testInstances4.getNumClasses());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(testInstances4.equals((Object)testInstances2));
      assertFalse(testInstances4.equals((Object)testInstances0));
      assertFalse(testInstances4.equals((Object)testInstances1));
      assertFalse(testInstances4.equals((Object)testInstances3));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(testInstances4, testInstances2);
      assertNotSame(testInstances4, testInstances0);
      assertNotSame(testInstances4, testInstances1);
      assertNotSame(testInstances4, testInstances3);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      
      testInstances4.setNumNominalValues(509);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals(1, testInstances4.getClassType());
      assertEquals((-1), testInstances4.getClassIndex());
      assertEquals(1, testInstances4.getNumNominal());
      assertEquals(" ", testInstances4.getWordSeparators());
      assertEquals("Testdata", testInstances4.getRelation());
      assertEquals(1, testInstances4.getSeed());
      assertEquals(509, testInstances4.getNumNominalValues());
      assertEquals(0, testInstances4.getNumString());
      assertEquals(1, testInstances4.getNumRelationalNumeric());
      assertEquals(0, testInstances4.getNumRelationalString());
      assertFalse(testInstances4.getMultiInstance());
      assertEquals(1, testInstances4.getNumRelationalNominal());
      assertEquals(1, testInstances4.getNumRelationalDate());
      assertEquals(20, testInstances4.getNumInstances());
      assertEquals(10, testInstances4.getNumInstancesRelational());
      assertEquals(1, testInstances4.getNumNumeric());
      assertEquals(2, testInstances4.getNumRelationalNominalValues());
      assertEquals(1, testInstances4.getNumDate());
      assertEquals(0, testInstances4.getNumRelational());
      assertFalse(testInstances4.getNoClass());
      assertEquals(4, testInstances4.getNumAttributes());
      assertEquals(4, testInstances4.getNumClasses());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(testInstances4.equals((Object)testInstances2));
      assertFalse(testInstances4.equals((Object)testInstances0));
      assertFalse(testInstances4.equals((Object)testInstances1));
      assertFalse(testInstances4.equals((Object)testInstances3));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(testInstances4, testInstances2);
      assertNotSame(testInstances4, testInstances0);
      assertNotSame(testInstances4, testInstances1);
      assertNotSame(testInstances4, testInstances3);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      
      TestInstances testInstances5 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances5);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances5.getNumRelationalNominal());
      assertEquals(4, testInstances5.getNumClasses());
      assertEquals((-1), testInstances5.getClassIndex());
      assertEquals(1, testInstances5.getSeed());
      assertEquals(4, testInstances5.getNumAttributes());
      assertEquals(" ", testInstances5.getWordSeparators());
      assertEquals(1, testInstances5.getNumNominal());
      assertEquals("Testdata", testInstances5.getRelation());
      assertEquals(1, testInstances5.getNumRelationalDate());
      assertEquals(0, testInstances5.getNumRelational());
      assertEquals(1, testInstances5.getClassType());
      assertEquals(2, testInstances5.getNumNominalValues());
      assertEquals(1, testInstances5.getNumDate());
      assertEquals(10, testInstances5.getNumInstancesRelational());
      assertFalse(testInstances5.getMultiInstance());
      assertEquals(20, testInstances5.getNumInstances());
      assertFalse(testInstances5.getNoClass());
      assertEquals(0, testInstances5.getNumRelationalString());
      assertEquals(1, testInstances5.getNumRelationalNumeric());
      assertEquals(2, testInstances5.getNumRelationalNominalValues());
      assertEquals(1, testInstances5.getNumNumeric());
      assertEquals(0, testInstances5.getNumString());
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances5.equals((Object)testInstances3));
      assertFalse(testInstances5.equals((Object)testInstances2));
      assertFalse(testInstances5.equals((Object)testInstances4));
      assertFalse(testInstances5.equals((Object)testInstances0));
      assertFalse(testInstances5.equals((Object)testInstances1));
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances5, testInstances3);
      assertNotSame(testInstances5, testInstances2);
      assertNotSame(testInstances5, testInstances4);
      assertNotSame(testInstances5, testInstances0);
      assertNotSame(testInstances5, testInstances1);
      
      TestInstances testInstances6 = new TestInstances();
      assertNotNull(testInstances6);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances6.getNumRelationalNominal());
      assertEquals(1, testInstances6.getSeed());
      assertEquals(0, testInstances6.getNumString());
      assertEquals((-1), testInstances6.getClassIndex());
      assertEquals(0, testInstances6.getNumRelationalString());
      assertEquals(0, testInstances6.getNumNumeric());
      assertEquals(0, testInstances6.getNumRelationalNumeric());
      assertEquals(2, testInstances6.getNumRelationalNominalValues());
      assertEquals(20, testInstances6.getNumInstances());
      assertFalse(testInstances6.getNoClass());
      assertFalse(testInstances6.getMultiInstance());
      assertEquals(10, testInstances6.getNumInstancesRelational());
      assertEquals(0, testInstances6.getNumRelational());
      assertEquals(0, testInstances6.getNumDate());
      assertEquals(" ", testInstances6.getWordSeparators());
      assertEquals("Testdata", testInstances6.getRelation());
      assertEquals(2, testInstances6.getNumNominalValues());
      assertEquals(1, testInstances6.getNumNominal());
      assertEquals(0, testInstances6.getNumRelationalDate());
      assertEquals(1, testInstances6.getClassType());
      assertEquals(2, testInstances6.getNumAttributes());
      assertEquals(2, testInstances6.getNumClasses());
      assertFalse(testInstances6.equals((Object)testInstances2));
      assertFalse(testInstances6.equals((Object)testInstances0));
      assertFalse(testInstances6.equals((Object)testInstances4));
      assertFalse(testInstances6.equals((Object)testInstances3));
      assertFalse(testInstances6.equals((Object)testInstances1));
      assertFalse(testInstances6.equals((Object)testInstances5));
      
      Instances instances1 = testInstances4.generate();
      assertNotNull(instances1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals(1, testInstances4.getClassType());
      assertEquals((-1), testInstances4.getClassIndex());
      assertEquals(1, testInstances4.getNumNominal());
      assertEquals(" ", testInstances4.getWordSeparators());
      assertEquals("Testdata", testInstances4.getRelation());
      assertEquals(1, testInstances4.getSeed());
      assertEquals(509, testInstances4.getNumNominalValues());
      assertEquals(0, testInstances4.getNumString());
      assertEquals(1, testInstances4.getNumRelationalNumeric());
      assertEquals(0, testInstances4.getNumRelationalString());
      assertFalse(testInstances4.getMultiInstance());
      assertEquals(1, testInstances4.getNumRelationalNominal());
      assertEquals(1, testInstances4.getNumRelationalDate());
      assertEquals(20, testInstances4.getNumInstances());
      assertEquals(10, testInstances4.getNumInstancesRelational());
      assertEquals(1, testInstances4.getNumNumeric());
      assertEquals(2, testInstances4.getNumRelationalNominalValues());
      assertEquals(1, testInstances4.getNumDate());
      assertEquals(0, testInstances4.getNumRelational());
      assertFalse(testInstances4.getNoClass());
      assertEquals(4, testInstances4.getNumAttributes());
      assertEquals(4, testInstances4.getNumClasses());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(20, instances1.size());
      assertEquals(3, instances1.classIndex());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(20, instances1.numInstances());
      assertEquals(20.0, instances1.sumOfWeights(), 0.01);
      assertEquals(4, instances1.numAttributes());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(4, instances1.numClasses());
      assertFalse(testInstances4.equals((Object)testInstances6));
      assertFalse(testInstances4.equals((Object)testInstances2));
      assertFalse(testInstances4.equals((Object)testInstances0));
      assertFalse(testInstances4.equals((Object)testInstances1));
      assertFalse(testInstances4.equals((Object)testInstances3));
      assertFalse(testInstances4.equals((Object)testInstances5));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(instances1.equals((Object)instances0));
      assertNotSame(testInstances4, testInstances6);
      assertNotSame(testInstances4, testInstances2);
      assertNotSame(testInstances4, testInstances0);
      assertNotSame(testInstances4, testInstances1);
      assertNotSame(testInstances4, testInstances3);
      assertNotSame(testInstances4, testInstances5);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(instances1, instances0);
      
      String[] stringArray0 = testInstances4.getOptions();
      assertNotNull(stringArray0);
      assertEquals(40, stringArray0.length);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals(1, testInstances4.getClassType());
      assertEquals((-1), testInstances4.getClassIndex());
      assertEquals(1, testInstances4.getNumNominal());
      assertEquals(" ", testInstances4.getWordSeparators());
      assertEquals("Testdata", testInstances4.getRelation());
      assertEquals(1, testInstances4.getSeed());
      assertEquals(509, testInstances4.getNumNominalValues());
      assertEquals(0, testInstances4.getNumString());
      assertEquals(1, testInstances4.getNumRelationalNumeric());
      assertEquals(0, testInstances4.getNumRelationalString());
      assertFalse(testInstances4.getMultiInstance());
      assertEquals(1, testInstances4.getNumRelationalNominal());
      assertEquals(1, testInstances4.getNumRelationalDate());
      assertEquals(20, testInstances4.getNumInstances());
      assertEquals(10, testInstances4.getNumInstancesRelational());
      assertEquals(1, testInstances4.getNumNumeric());
      assertEquals(2, testInstances4.getNumRelationalNominalValues());
      assertEquals(1, testInstances4.getNumDate());
      assertEquals(0, testInstances4.getNumRelational());
      assertFalse(testInstances4.getNoClass());
      assertEquals(4, testInstances4.getNumAttributes());
      assertEquals(4, testInstances4.getNumClasses());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(testInstances4.equals((Object)testInstances6));
      assertFalse(testInstances4.equals((Object)testInstances2));
      assertFalse(testInstances4.equals((Object)testInstances0));
      assertFalse(testInstances4.equals((Object)testInstances1));
      assertFalse(testInstances4.equals((Object)testInstances3));
      assertFalse(testInstances4.equals((Object)testInstances5));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(testInstances4, testInstances6);
      assertNotSame(testInstances4, testInstances2);
      assertNotSame(testInstances4, testInstances0);
      assertNotSame(testInstances4, testInstances1);
      assertNotSame(testInstances4, testInstances3);
      assertNotSame(testInstances4, testInstances5);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      
      Evaluation evaluation1 = new Evaluation(instances0);
      assertNotNull(evaluation1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(0, testInstances1.getNumString());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertFalse(testInstances1.getNoClass());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(capabilities1.equals((Object)capabilities2));
      assertFalse(testInstances1.equals((Object)testInstances6));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertFalse(testInstances1.equals((Object)testInstances3));
      assertFalse(testInstances1.equals((Object)testInstances2));
      assertFalse(testInstances1.equals((Object)testInstances5));
      assertFalse(testInstances1.equals((Object)testInstances4));
      assertFalse(instances0.equals((Object)instances1));
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      String string0 = evaluation0.toSummaryString("weka/core/Capabilities.props", false);
      assertEquals("weka/core/Capabilities.props\nTotal Number of Instances                0     \n", string0);
      assertNotNull(string0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(0, testInstances1.getNumString());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertFalse(testInstances1.getNoClass());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(capabilities1.equals((Object)capabilities2));
      assertFalse(testInstances1.equals((Object)testInstances6));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertFalse(testInstances1.equals((Object)testInstances3));
      assertFalse(testInstances1.equals((Object)testInstances2));
      assertFalse(testInstances1.equals((Object)testInstances5));
      assertFalse(testInstances1.equals((Object)testInstances4));
      assertFalse(instances0.equals((Object)instances1));
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(capabilities1, capabilities2);
      assertNotSame(testInstances1, testInstances6);
      assertNotSame(testInstances1, testInstances0);
      assertNotSame(testInstances1, testInstances3);
      assertNotSame(testInstances1, testInstances2);
      assertNotSame(testInstances1, testInstances5);
      assertNotSame(testInstances1, testInstances4);
      assertNotSame(instances0, instances1);
      assertNotSame(evaluation0, evaluation1);
      
      double double2 = evaluation0.falseNegativeRate(1068540723);
      assertEquals(0.0, double2, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(0, testInstances1.getNumString());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertFalse(testInstances1.getNoClass());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(capabilities1.equals((Object)capabilities2));
      assertFalse(testInstances1.equals((Object)testInstances6));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertFalse(testInstances1.equals((Object)testInstances3));
      assertFalse(testInstances1.equals((Object)testInstances2));
      assertFalse(testInstances1.equals((Object)testInstances5));
      assertFalse(testInstances1.equals((Object)testInstances4));
      assertFalse(instances0.equals((Object)instances1));
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotEquals(double2, double1, 0.01);
      assertEquals(double2, double0, 0.01);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(capabilities1, capabilities2);
      assertNotSame(testInstances1, testInstances6);
      assertNotSame(testInstances1, testInstances0);
      assertNotSame(testInstances1, testInstances3);
      assertNotSame(testInstances1, testInstances2);
      assertNotSame(testInstances1, testInstances5);
      assertNotSame(testInstances1, testInstances4);
      assertNotSame(instances0, instances1);
      assertNotSame(evaluation0, evaluation1);
  }

  /**
  //Test case number: 31
  /*Coverage entropy=2.386959303054203
  */
  @Test(timeout = 4000)
  public void test031()  throws Throwable  {
      KStar kStar0 = new KStar();
      assertNotNull(kStar0);
      assertEquals(20, kStar0.getGlobalBlend());
      assertEquals("If set to true, classifier may output additional info to the console.", kStar0.debugTipText());
      assertEquals("Whether entropy-based blending is to be used.", kStar0.entropicAutoBlendTipText());
      assertEquals("Determines how missing attribute values are treated.", kStar0.missingModeTipText());
      assertFalse(kStar0.getDebug());
      assertFalse(kStar0.getEntropicAutoBlend());
      assertEquals("The parameter for global blending. Values are restricted to [0,100].", kStar0.globalBlendTipText());
      
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      assertNotNull(adaBoostM1_0);
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      
      Capabilities capabilities0 = adaBoostM1_0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props");
      boolean boolean0 = FileSystemHandling.appendStringToFile(evoSuiteFile0, "weka/core/Capabilities.props");
      assertTrue(boolean0);
      
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.RELATIONAL_CLASS;
      capabilities0.disable(capabilities_Capability0);
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      assertNotNull(wordTokenizer0);
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      capabilities0.enableAll();
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      double[] doubleArray0 = new double[4];
      doubleArray0[0] = (-99.0);
      doubleArray0[1] = (double) (-1219);
      doubleArray0[2] = (double) 2;
      doubleArray0[3] = 1.0;
      int[] intArray0 = new int[4];
      intArray0[0] = 2;
      intArray0[1] = (-2);
      intArray0[2] = (-1995138716);
      intArray0[3] = (-1);
      SparseInstance sparseInstance0 = new SparseInstance(1.0, doubleArray0, intArray0, (-1995138716));
      assertNotNull(sparseInstance0);
      assertEquals(4, intArray0.length);
      assertEquals(4, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new int[] {2, (-2), (-1995138716), (-1)}, intArray0);
      assertArrayEquals(new double[] {(-99.0), (-1219.0), 2.0, 1.0}, doubleArray0, 0.01);
      assertEquals((-1995138716), sparseInstance0.numAttributes());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(4, sparseInstance0.numValues());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      
      double double0 = evaluation0.m_SumPriorSqrErr;
      assertEquals(0.0, double0, 0.01);
      
      double double1 = evaluation0.weightedFalseNegativeRate();
      assertEquals(Double.NaN, double1, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertNotEquals(double1, double0, 0.01);
      
      System.setCurrentTimeMillis(1815L);
      double double2 = evaluation0.weightedRecall();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertNotEquals(double2, double0, 0.01);
      assertEquals(double2, double1, 0.01);
  }

  /**
  //Test case number: 32
  /*Coverage entropy=2.972849472996488
  */
  @Test(timeout = 4000)
  public void test032()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      
      Double double0 = new Double(0.0);
      assertEquals(0.0, (double)double0, 0.01);
      assertNotNull(double0);
      
      instances0.deleteStringAttributes();
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      CostMatrix costMatrix0 = new CostMatrix(2);
      assertNotNull(costMatrix0);
      assertEquals(2, costMatrix0.size());
      assertEquals(2, costMatrix0.numRows());
      assertEquals(2, costMatrix0.numColumns());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostMatrix costMatrix1 = costSensitiveClassifier0.getCostMatrix();
      assertNotNull(costMatrix1);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals(1, costMatrix1.size());
      assertEquals(1, costMatrix1.numColumns());
      assertEquals(1, costMatrix1.numRows());
      assertFalse(costSensitiveClassifier0.equals((Object)costSensitiveClassifier1));
      assertFalse(costMatrix1.equals((Object)costMatrix0));
      assertNotSame(costSensitiveClassifier0, costSensitiveClassifier1);
      assertNotSame(costMatrix1, costMatrix0);
      
      instances0.deleteStringAttributes();
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      
      Double double1 = new Double((-2));
      assertEquals((-2.0), (double)double1, 0.01);
      assertNotNull(double1);
      assertNotEquals((double)double1, (double)double0, 0.01);
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      
      costSensitiveClassifier2.setCostMatrix(costMatrix0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(2, costMatrix0.size());
      assertEquals(2, costMatrix0.numRows());
      assertEquals(2, costMatrix0.numColumns());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertFalse(costMatrix0.equals((Object)costMatrix1));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertNotSame(costMatrix0, costMatrix1);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier1);
      
      CostMatrix costMatrix2 = costSensitiveClassifier0.getCostMatrix();
      assertNotNull(costMatrix2);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals(1, costMatrix2.numColumns());
      assertEquals(1, costMatrix2.numRows());
      assertEquals(1, costMatrix2.size());
      assertFalse(costSensitiveClassifier0.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier0.equals((Object)costSensitiveClassifier1));
      assertFalse(costMatrix2.equals((Object)costMatrix0));
      assertNotSame(costSensitiveClassifier0, costSensitiveClassifier2);
      assertNotSame(costSensitiveClassifier0, costSensitiveClassifier1);
      assertSame(costMatrix2, costMatrix1);
      assertNotSame(costMatrix2, costMatrix0);
      
      Evaluation evaluation1 = new Evaluation(instances0, costMatrix0);
      assertNotNull(evaluation1);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, costMatrix0.size());
      assertEquals(2, costMatrix0.numRows());
      assertEquals(2, costMatrix0.numColumns());
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertFalse(costMatrix0.equals((Object)costMatrix2));
      assertFalse(costMatrix0.equals((Object)costMatrix1));
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      evaluation1.addNumericTrainClass(1, (-1853.8104163393));
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, costMatrix0.size());
      assertEquals(2, costMatrix0.numRows());
      assertEquals(2, costMatrix0.numColumns());
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertFalse(costMatrix0.equals((Object)costMatrix2));
      assertFalse(costMatrix0.equals((Object)costMatrix1));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(costMatrix0, costMatrix2);
      assertNotSame(costMatrix0, costMatrix1);
      assertNotSame(evaluation1, evaluation0);
      
      Enumeration enumeration0 = costSensitiveClassifier2.listOptions();
      assertNotNull(enumeration0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier1);
      
      Integer integer0 = new Integer(1);
      assertEquals(1, (int)integer0);
      assertNotNull(integer0);
      
      double double2 = evaluation0.precision(1);
      assertEquals(0.0, double2, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotSame(evaluation0, evaluation1);
      
      double double3 = evaluation1.areaUnderPRC(1);
      assertEquals(Double.NaN, double3, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, costMatrix0.size());
      assertEquals(2, costMatrix0.numRows());
      assertEquals(2, costMatrix0.numColumns());
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertNotEquals(double3, double2, 0.01);
      assertFalse(costMatrix0.equals((Object)costMatrix2));
      assertFalse(costMatrix0.equals((Object)costMatrix1));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(costMatrix0, costMatrix2);
      assertNotSame(costMatrix0, costMatrix1);
      assertNotSame(evaluation1, evaluation0);
      
      double double4 = evaluation0.falseNegativeRate(1);
      assertEquals(0.0, double4, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertEquals(double4, double2, 0.01);
      assertNotEquals(double4, double3, 0.01);
      assertNotSame(evaluation0, evaluation1);
      
      double double5 = evaluation1.matthewsCorrelationCoefficient(2);
      assertEquals(0.0, double5, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, costMatrix0.size());
      assertEquals(2, costMatrix0.numRows());
      assertEquals(2, costMatrix0.numColumns());
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertNotEquals(double5, double3, 0.01);
      assertEquals(double5, double2, 0.01);
      assertEquals(double5, double4, 0.01);
      assertFalse(costMatrix0.equals((Object)costMatrix2));
      assertFalse(costMatrix0.equals((Object)costMatrix1));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(costMatrix0, costMatrix2);
      assertNotSame(costMatrix0, costMatrix1);
      assertNotSame(evaluation1, evaluation0);
      
      double double6 = evaluation1.unclassified();
      assertEquals(0.0, double6, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, costMatrix0.size());
      assertEquals(2, costMatrix0.numRows());
      assertEquals(2, costMatrix0.numColumns());
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(double6, double4, 0.01);
      assertEquals(double6, double2, 0.01);
      assertEquals(double6, double5, 0.01);
      assertNotEquals(double6, double3, 0.01);
      assertFalse(costMatrix0.equals((Object)costMatrix2));
      assertFalse(costMatrix0.equals((Object)costMatrix1));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(costMatrix0, costMatrix2);
      assertNotSame(costMatrix0, costMatrix1);
      assertNotSame(evaluation1, evaluation0);
      
      String string0 = evaluation1.toSummaryString(true);
      assertEquals("=== Summary ===\n\nTotal Number of Instances                0     \n", string0);
      assertNotNull(string0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, costMatrix0.size());
      assertEquals(2, costMatrix0.numRows());
      assertEquals(2, costMatrix0.numColumns());
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertFalse(costMatrix0.equals((Object)costMatrix2));
      assertFalse(costMatrix0.equals((Object)costMatrix1));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(costMatrix0, costMatrix2);
      assertNotSame(costMatrix0, costMatrix1);
      assertNotSame(evaluation1, evaluation0);
      
      // Undeclared exception!
      try { 
        ConverterUtils.DataSource.read("weka.classifiers.Evaluation");
        fail("Expecting exception: NoClassDefFoundError");
      
      } catch(NoClassDefFoundError e) {
         //
         // Could not initialize class weka.core.converters.ConverterUtils
         //
         verifyException("weka.core.converters.ConverterUtils$DataSource", e);
      }
  }

  /**
  //Test case number: 33
  /*Coverage entropy=2.560452965490236
  */
  @Test(timeout = 4000)
  public void test033()  throws Throwable  {
      KStar kStar0 = new KStar();
      assertNotNull(kStar0);
      assertEquals("The parameter for global blending. Values are restricted to [0,100].", kStar0.globalBlendTipText());
      assertEquals("Whether entropy-based blending is to be used.", kStar0.entropicAutoBlendTipText());
      assertFalse(kStar0.getDebug());
      assertEquals(20, kStar0.getGlobalBlend());
      assertEquals("If set to true, classifier may output additional info to the console.", kStar0.debugTipText());
      assertEquals("Determines how missing attribute values are treated.", kStar0.missingModeTipText());
      assertFalse(kStar0.getEntropicAutoBlend());
      
      byte[] byteArray0 = new byte[4];
      byteArray0[0] = (byte)24;
      byteArray0[1] = (byte)83;
      byteArray0[2] = (byte)93;
      byteArray0[0] = (byte)52;
      boolean boolean0 = FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      assertFalse(boolean0);
      assertEquals(4, byteArray0.length);
      assertArrayEquals(new byte[] {(byte)52, (byte)83, (byte)93, (byte)0}, byteArray0);
      
      boolean boolean1 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "&#13;");
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      
      textDirectoryLoader0.setDebug(true);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertTrue(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertNotNull(instances0);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertTrue(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertTrue(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      
      textDirectoryLoader0.setDebug(false);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      
      evaluation0.m_SumPredicted = (-0.3495505191523909);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      
      double double0 = evaluation0.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      
      double double1 = evaluation0.weightedTruePositiveRate();
      assertEquals(Double.NaN, double1, 0.01);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(double1, double0, 0.01);
      
      double double2 = evaluation0.sizeOfPredictedRegions();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(double2, double1, 0.01);
      assertEquals(double2, double0, 0.01);
      
      String string0 = evaluation0.toSummaryString(false);
      assertEquals("=== Summary ===\n\nTotal Number of Instances                0     \n", string0);
      assertNotNull(string0);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
  }

  /**
  //Test case number: 34
  /*Coverage entropy=1.9593337382266454
  */
  @Test(timeout = 4000)
  public void test034()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numClasses());
      
      DenseInstance denseInstance0 = new DenseInstance(25);
      assertNotNull(denseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(25, denseInstance0.numAttributes());
      assertEquals(25, denseInstance0.numValues());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      
      double double0 = 5.019028201623571E-4;
      double[] doubleArray0 = new double[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(5.019028201623571E-4, doubleArray0);
      assertNotNull(binarySparseInstance0);
      assertEquals(0, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals(5.019028201623571E-4, binarySparseInstance0.weight(), 0.01);
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      SparseInstance sparseInstance0 = new SparseInstance(0.0022462733445193627, doubleArray0);
      assertNotNull(sparseInstance0);
      assertEquals(0, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(0, sparseInstance0.numAttributes());
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(0.0022462733445193627, sparseInstance0.weight(), 0.01);
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(0, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals(5.019028201623571E-4, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance1.numAttributes());
      assertEquals(0, binarySparseInstance1.numValues());
      assertEquals(5.019028201623571E-4, binarySparseInstance1.weight(), 0.01);
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals(1, instances0.classIndex());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      
      String string0 = "getCostMatrix";
      int int0 = (-942);
      String[] stringArray0 = new String[0];
      try { 
        evaluation0.evaluateModelOnceAndRecordPrediction(doubleArray0, (Instance) binarySparseInstance0);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 0
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 35
  /*Coverage entropy=2.559956513676124
  */
  @Test(timeout = 4000)
  public void test035()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      Stacking stacking0 = new Stacking();
      assertNotNull(stacking0);
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals(1, stacking0.getSeed());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      assertFalse(stacking0.getDebug());
      assertEquals(10, stacking0.getNumFolds());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      
      stacking0.setSeed(107);
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      assertFalse(stacking0.getDebug());
      assertEquals(10, stacking0.getNumFolds());
      assertEquals(107, stacking0.getSeed());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      
      ZeroR zeroR0 = new ZeroR();
      assertNotNull(zeroR0);
      assertEquals("Class for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).", zeroR0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", zeroR0.debugTipText());
      assertFalse(zeroR0.getDebug());
      
      String[] stringArray0 = zeroR0.getOptions();
      assertNotNull(stringArray0);
      assertEquals(0, stringArray0.length);
      assertEquals("Class for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).", zeroR0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", zeroR0.debugTipText());
      assertFalse(zeroR0.getDebug());
      
      ZeroR zeroR1 = new ZeroR();
      assertNotNull(zeroR1);
      assertEquals("Class for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).", zeroR1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", zeroR1.debugTipText());
      assertFalse(zeroR1.getDebug());
      assertFalse(zeroR1.equals((Object)zeroR0));
      
      zeroR1.setDebug(false);
      assertEquals("Class for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).", zeroR1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", zeroR1.debugTipText());
      assertFalse(zeroR1.getDebug());
      assertFalse(zeroR1.equals((Object)zeroR0));
      assertNotSame(zeroR1, zeroR0);
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      
      Instances instances0 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader0);
      assertNotNull(instances0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      instances0.sort(comparator0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      
      evaluation0.setDiscardPredictions(true);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertTrue(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      
      GaussianProcesses gaussianProcesses0 = new GaussianProcesses();
      assertNotNull(gaussianProcesses0);
      assertEquals(1, GaussianProcesses.FILTER_STANDARDIZE);
      assertEquals(0, GaussianProcesses.FILTER_NORMALIZE);
      assertEquals(2, GaussianProcesses.FILTER_NONE);
      assertFalse(gaussianProcesses0.getDebug());
      assertEquals(1.0, gaussianProcesses0.getNoise(), 0.01);
      assertEquals("The kernel to use.", gaussianProcesses0.kernelTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", gaussianProcesses0.debugTipText());
      assertEquals(" Implements Gaussian processes for regression without hyperparameter-tuning. To make choosing an appropriate noise level easier, this implementation applies normalization/standardization to the target attribute as well as the other attributes (if  normalization/standardizaton is turned on). Missing values are replaced by the global mean/mode. Nominal attributes are converted to binary ones. Note that kernel caching is turned off if the kernel used implements CachedKernel.", gaussianProcesses0.globalInfo());
      assertEquals("The level of Gaussian Noise (added to the diagonal of the Covariance Matrix), after the target has been normalized/standardized/left unchanged).", gaussianProcesses0.noiseTipText());
      assertEquals("Determines how/if the data will be transformed.", gaussianProcesses0.filterTypeTipText());
      
      double double0 = evaluation0.rootMeanPriorSquaredError();
      assertEquals(Double.NaN, double0, 0.01);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertTrue(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = 2715.398102;
      doubleArray0[1] = 922.3872;
      doubleArray0[2] = 0.002683355685264898;
      doubleArray0[3] = Double.NaN;
      MouseWheelListener mouseWheelListener0 = mock(MouseWheelListener.class, new ViolatedAssumptionAnswer());
      MouseWheelListener mouseWheelListener1 = AWTEventMulticaster.add(mouseWheelListener0, mouseWheelListener0);
      assertNotNull(mouseWheelListener1);
      
      MouseWheelListener mouseWheelListener2 = AWTEventMulticaster.remove(mouseWheelListener1, mouseWheelListener1);
      assertNull(mouseWheelListener2);
      
      MouseWheelListener mouseWheelListener3 = AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener3);
      
      MouseWheelListener mouseWheelListener4 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener4);
      
      WekaTaskMonitor wekaTaskMonitor0 = new WekaTaskMonitor();
      assertNotNull(wekaTaskMonitor0);
      assertFalse(wekaTaskMonitor0.getIgnoreRepaint());
      assertTrue(wekaTaskMonitor0.getFocusTraversalKeysEnabled());
      assertFalse(wekaTaskMonitor0.isFocusTraversalPolicySet());
      assertFalse(wekaTaskMonitor0.isFocusCycleRoot());
      assertFalse(wekaTaskMonitor0.isFocusTraversalPolicyProvider());
      
      wekaTaskMonitor0.addMouseWheelListener(mouseWheelListener1);
      assertFalse(wekaTaskMonitor0.getIgnoreRepaint());
      assertTrue(wekaTaskMonitor0.getFocusTraversalKeysEnabled());
      assertFalse(wekaTaskMonitor0.isFocusTraversalPolicySet());
      assertFalse(wekaTaskMonitor0.isFocusCycleRoot());
      assertFalse(wekaTaskMonitor0.isFocusTraversalPolicyProvider());
      
      double double1 = evaluation0.SFMeanPriorEntropy();
      assertEquals(Double.NaN, double1, 0.01);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertTrue(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(double1, double0, 0.01);
      
      double double2 = evaluation0.SFPriorEntropy();
      assertEquals(0.0, double2, 0.01);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertTrue(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertNotEquals(double2, double0, 0.01);
      assertNotEquals(double2, double1, 0.01);
      
      double double3 = evaluation0.SFPriorEntropy();
      assertEquals(0.0, double3, 0.01);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertTrue(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertNotEquals(double3, double0, 0.01);
      assertEquals(double3, double2, 0.01);
      assertNotEquals(double3, double1, 0.01);
      
      double double4 = evaluation0.numInstances();
      assertEquals(0.0, double4, 0.01);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertTrue(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertNotEquals(double4, double0, 0.01);
      assertNotEquals(double4, double1, 0.01);
      assertEquals(double4, double3, 0.01);
      assertEquals(double4, double2, 0.01);
      
      double double5 = evaluation0.numFalsePositives(0);
      assertEquals(0.0, double5, 0.01);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertTrue(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(double5, double2, 0.01);
      assertNotEquals(double5, double1, 0.01);
      assertEquals(double5, double3, 0.01);
      assertNotEquals(double5, double0, 0.01);
      assertEquals(double5, double4, 0.01);
  }

  /**
  //Test case number: 36
  /*Coverage entropy=3.005272664186951
  */
  @Test(timeout = 4000)
  public void test036()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      
      String string0 = instances0.toSummaryString();
      assertEquals("Relation Name:  Testdata\nNum Instances:  20\nNum Attributes: 2\n\n     Name                      Type  Nom  Int Real     Missing      Unique  Dist\n   1 Nominal1                   Nom 100%   0%   0%     0 /  0%     0 /  0%     2 \n   2 Class                      Nom 100%   0%   0%     0 /  0%     0 /  0%     2 \n", string0);
      assertNotNull(string0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      
      testInstances0.setNumInstancesRelational(2372);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2372, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      doReturn(0, 0, 0, 0, 0).when(comparator0).compare(any() , any());
      instances0.sort(comparator0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2372, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2372, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      
      double double0 = evaluation0.SFMeanPriorEntropy();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2372, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      
      Double double1 = new Double((-1));
      assertEquals((-1.0), (double)double1, 0.01);
      assertNotNull(double1);
      assertNotEquals((double)double1, (double)double0, 0.01);
      
      instances0.deleteStringAttributes();
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2372, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      double double2 = evaluation0.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2372, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(double2, double0, 0.01);
      
      double double3 = evaluation0.m_WithClass;
      assertEquals(0.0, double3, 0.01);
      assertNotEquals(double3, double0, 0.01);
      assertNotEquals(double3, double2, 0.01);
      
      double double4 = evaluation0.weightedTrueNegativeRate();
      assertEquals(Double.NaN, double4, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2372, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(double4, double2, 0.01);
      assertNotEquals(double4, double3, 0.01);
      assertEquals(double4, double0, 0.01);
      
      boolean boolean1 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean1);
      assertFalse(boolean1 == boolean0);
      
      double[] doubleArray0 = new double[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(2485);
      assertNotNull(binarySparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(2485, binarySparseInstance0.numAttributes());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(2485, binarySparseInstance0.numValues());
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(2485, binarySparseInstance0.numAttributes());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(2485, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertEquals(2485, binarySparseInstance1.numValues());
      assertEquals(2485, binarySparseInstance1.numAttributes());
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance1);
      assertNotNull(sparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(2485, binarySparseInstance0.numAttributes());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(2485, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertEquals(2485, binarySparseInstance1.numValues());
      assertEquals(2485, binarySparseInstance1.numAttributes());
      assertEquals(2485, sparseInstance0.numAttributes());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(2485, sparseInstance0.numValues());
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance1));
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      double double5 = evaluation0.weightedMatthewsCorrelation();
      assertEquals(Double.NaN, double5, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2372, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(double5, double4, 0.01);
      assertEquals(double5, double2, 0.01);
      assertNotEquals(double5, double3, 0.01);
      assertEquals(double5, double0, 0.01);
      
      try { 
        evaluation0.evaluationForSingleInstance(doubleArray0, sparseInstance0, true);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 0
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 37
  /*Coverage entropy=1.7221489918454203
  */
  @Test(timeout = 4000)
  public void test037()  throws Throwable  {
      KStar kStar0 = new KStar();
      assertNotNull(kStar0);
      assertEquals("Determines how missing attribute values are treated.", kStar0.missingModeTipText());
      assertFalse(kStar0.getEntropicAutoBlend());
      assertEquals("Whether entropy-based blending is to be used.", kStar0.entropicAutoBlendTipText());
      assertEquals(20, kStar0.getGlobalBlend());
      assertEquals("If set to true, classifier may output additional info to the console.", kStar0.debugTipText());
      assertEquals("The parameter for global blending. Values are restricted to [0,100].", kStar0.globalBlendTipText());
      assertFalse(kStar0.getDebug());
      
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      assertNotNull(adaBoostM1_0);
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      
      Instances instances0 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader0);
      assertNotNull(instances0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      TextDirectoryLoader textDirectoryLoader1 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader1);
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      
      DatabaseLoader databaseLoader0 = new DatabaseLoader();
      assertNotNull(databaseLoader0);
      assertEquals("The user name for the database", databaseLoader0.userTipText());
      assertEquals("", databaseLoader0.getUser());
      assertEquals("For incremental loading a unique identiefer has to be specified.\nIf the query includes all columns of a table (SELECT *...) a primary key\ncan be detected automatically depending on the JDBC driver. If that is not possible\nspecify the key columns here in a comma separated list.", databaseLoader0.keysTipText());
      assertEquals("The URL of the database", databaseLoader0.urlTipText());
      assertEquals("The custom properties that the user can use to override the default ones.", databaseLoader0.customPropsFileTipText());
      assertEquals("Select * from Results0", databaseLoader0.getQuery());
      assertEquals("jdbc:idb=experiments.prp", databaseLoader0.getUrl());
      assertEquals("The query that should load the instances.\n The query has to be of the form SELECT <column-list>|* FROM <table> [WHERE <conditions>]", databaseLoader0.queryTipText());
      assertFalse(databaseLoader0.getSparseData());
      assertEquals("Encode data as sparse instances.", databaseLoader0.sparseDataTipText());
      assertEquals("", databaseLoader0.getPassword());
      assertEquals("The database password", databaseLoader0.passwordTipText());
      
      Instances instances1 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader1);
      assertNotNull(instances1);
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals(0, instances1.numInstances());
      assertEquals(2, instances1.numAttributes());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertEquals(1, instances1.classIndex());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(0, instances1.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertTrue(instances1.equals((Object)instances0));
      assertNotSame(textDirectoryLoader1, textDirectoryLoader0);
      assertNotSame(instances1, instances0);
      
      LinkedList<String> linkedList0 = new LinkedList<String>();
      assertNotNull(linkedList0);
      assertEquals(0, linkedList0.size());
      
      Properties properties0 = new Properties();
      assertNotNull(properties0);
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      assertNotNull(protectedProperties0);
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      assertEquals(0, protectedProperties0.size());
      assertTrue(protectedProperties0.isEmpty());
      
      Attribute attribute0 = new Attribute(".bsi", linkedList0, protectedProperties0);
      assertNotNull(attribute0);
      assertEquals(0, Attribute.ORDERING_SYMBOLIC);
      assertEquals(0, Attribute.NUMERIC);
      assertEquals(1, Attribute.ORDERING_ORDERED);
      assertEquals(2, Attribute.STRING);
      assertEquals(2, Attribute.ORDERING_MODULO);
      assertEquals(1, Attribute.NOMINAL);
      assertEquals(4, Attribute.RELATIONAL);
      assertEquals(3, Attribute.DATE);
      assertEquals(0, linkedList0.size());
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      assertEquals(0, protectedProperties0.size());
      assertTrue(protectedProperties0.isEmpty());
      assertEquals(1, attribute0.type());
      assertFalse(attribute0.isRelationValued());
      assertFalse(attribute0.isString());
      assertFalse(attribute0.isAveragable());
      assertEquals(0, attribute0.ordering());
      assertEquals(0.0, attribute0.getLowerNumericBound(), 0.01);
      assertEquals(0.0, attribute0.getUpperNumericBound(), 0.01);
      assertEquals((-1), attribute0.index());
      assertFalse(attribute0.hasZeropoint());
      assertEquals("", attribute0.getDateFormat());
      assertEquals(1.0, attribute0.weight(), 0.01);
      assertEquals(".bsi", attribute0.name());
      assertFalse(attribute0.isNumeric());
      assertFalse(attribute0.upperNumericBoundIsOpen());
      assertFalse(attribute0.isRegular());
      assertTrue(attribute0.isNominal());
      assertEquals(0, attribute0.numValues());
      assertFalse(attribute0.isDate());
      assertFalse(attribute0.lowerNumericBoundIsOpen());
      assertFalse(linkedList0.contains(".bsi"));
      
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (double) 3;
      doubleArray0[1] = (double) 10;
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertTrue(instances0.equals((Object)instances1));
      
      Instances instances2 = textDirectoryLoader1.getDataSet();
      assertNotNull(instances2);
      assertEquals(1, instances2.classIndex());
      assertEquals(0, instances2.numClasses());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertTrue(instances2.equals((Object)instances0));
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertNotSame(instances2, instances0);
      assertSame(instances2, instances1);
      assertNotSame(textDirectoryLoader1, textDirectoryLoader0);
      
      Evaluation evaluation1 = new Evaluation(instances2);
      assertNotNull(evaluation1);
      assertEquals(1, instances2.classIndex());
      assertEquals(0, instances2.numClasses());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertTrue(instances2.equals((Object)instances0));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      
      try { 
        evaluation1.correlationCoefficient();
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Can't compute correlation coefficient: class is nominal!
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 38
  /*Coverage entropy=1.6866994899585297
  */
  @Test(timeout = 4000)
  public void test038()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      boolean boolean0 = FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(3, instances0.classIndex());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier3);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      
      CostMatrix costMatrix0 = costSensitiveClassifier3.getCostMatrix();
      assertNotNull(costMatrix0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertEquals(1, costMatrix0.size());
      assertEquals(1, costMatrix0.numRows());
      assertEquals(1, costMatrix0.numColumns());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier2);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(3, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      
      MockRandom mockRandom0 = new MockRandom();
      assertNotNull(mockRandom0);
      
      double double0 = mockRandom0.nextGaussian();
      assertEquals(0.7, double0, 0.01);
      
      String string0 = costMatrix0.toMatlab();
      assertEquals("[0.0]", string0);
      assertNotNull(string0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertEquals(1, costMatrix0.size());
      assertEquals(1, costMatrix0.numRows());
      assertEquals(1, costMatrix0.numColumns());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier2);
      
      MockRandom mockRandom1 = new MockRandom();
      assertNotNull(mockRandom1);
      assertFalse(mockRandom1.equals((Object)mockRandom0));
      
      costSensitiveClassifier3.setCostMatrix(costMatrix0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertEquals(1, costMatrix0.size());
      assertEquals(1, costMatrix0.numRows());
      assertEquals(1, costMatrix0.numColumns());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier2);
      
      Instances instances1 = ConverterUtils.DataSource.read(".arff");
      assertNull(instances1);
      
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = (double) 2;
      doubleArray0[1] = (double) 1;
      doubleArray0[2] = (double) 1;
      doubleArray0[3] = (double) 1;
      doubleArray0[4] = 0.7;
      doubleArray0[5] = (double) 1;
      doubleArray0[6] = (double) 2;
      doubleArray0[7] = (double) 2;
      evaluation0.updateMargins(doubleArray0, 2, 0.9875537330536115);
      assertEquals(8, doubleArray0.length);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertArrayEquals(new double[] {2.0, 1.0, 1.0, 1.0, 0.7, 1.0, 2.0, 2.0}, doubleArray0, 0.01);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(1, lMT0.graphType());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals(15, lMT0.getMinNumInstances());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertTrue(lMT0.getFastRegression());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getUseAIC());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(3, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
  }

  /**
  //Test case number: 39
  /*Coverage entropy=1.5935954614673427
  */
  @Test(timeout = 4000)
  public void test039()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      
      Evaluation evaluation1 = new Evaluation(instances0);
      assertNotNull(evaluation1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      KStar kStar0 = new KStar();
      assertNotNull(kStar0);
      assertEquals("Whether entropy-based blending is to be used.", kStar0.entropicAutoBlendTipText());
      assertFalse(kStar0.getEntropicAutoBlend());
      assertEquals("If set to true, classifier may output additional info to the console.", kStar0.debugTipText());
      assertEquals(20, kStar0.getGlobalBlend());
      assertEquals("The parameter for global blending. Values are restricted to [0,100].", kStar0.globalBlendTipText());
      assertEquals("Determines how missing attribute values are treated.", kStar0.missingModeTipText());
      assertFalse(kStar0.getDebug());
      
      SimpleKMeans simpleKMeans0 = new SimpleKMeans();
      assertNotNull(simpleKMeans0);
      assertEquals(2, simpleKMeans0.getNumClusters());
      assertEquals("set number of clusters", simpleKMeans0.numClustersTipText());
      assertFalse(simpleKMeans0.getPreserveInstancesOrder());
      assertEquals("Initialize cluster centers using the probabilistic  farthest first method of the k-means++ algorithm", simpleKMeans0.initializeUsingKMeansPlusPlusMethodTipText());
      assertEquals(2, simpleKMeans0.numberOfClusters());
      assertFalse(simpleKMeans0.getDontReplaceMissingValues());
      assertEquals(10, simpleKMeans0.getSeed());
      assertEquals("Display std deviations of numeric attributes and counts of nominal attributes.", simpleKMeans0.displayStdDevsTipText());
      assertEquals("Replace missing values globally with mean/mode.", simpleKMeans0.dontReplaceMissingValuesTipText());
      assertEquals("The distance function to use for instances comparison (default: weka.core.EuclideanDistance). ", simpleKMeans0.distanceFunctionTipText());
      assertFalse(simpleKMeans0.getFastDistanceCalc());
      assertFalse(simpleKMeans0.getInitializeUsingKMeansPlusPlusMethod());
      assertEquals("Preserve order of instances.", simpleKMeans0.preserveInstancesOrderTipText());
      assertEquals("set maximum number of iterations", simpleKMeans0.maxIterationsTipText());
      assertFalse(simpleKMeans0.getDisplayStdDevs());
      assertEquals("Uses cut-off values for speeding up distance calculation, but suppresses also the calculation and output of the within cluster sum of squared errors/sum of distances.", simpleKMeans0.fastDistanceCalcTipText());
      assertEquals(500, simpleKMeans0.getMaxIterations());
      
      Instances instances1 = simpleKMeans0.getClusterCentroids();
      assertNull(instances1);
      assertEquals(2, simpleKMeans0.getNumClusters());
      assertEquals("set number of clusters", simpleKMeans0.numClustersTipText());
      assertFalse(simpleKMeans0.getPreserveInstancesOrder());
      assertEquals("Initialize cluster centers using the probabilistic  farthest first method of the k-means++ algorithm", simpleKMeans0.initializeUsingKMeansPlusPlusMethodTipText());
      assertEquals(2, simpleKMeans0.numberOfClusters());
      assertFalse(simpleKMeans0.getDontReplaceMissingValues());
      assertEquals(10, simpleKMeans0.getSeed());
      assertEquals("Display std deviations of numeric attributes and counts of nominal attributes.", simpleKMeans0.displayStdDevsTipText());
      assertEquals("Replace missing values globally with mean/mode.", simpleKMeans0.dontReplaceMissingValuesTipText());
      assertEquals("The distance function to use for instances comparison (default: weka.core.EuclideanDistance). ", simpleKMeans0.distanceFunctionTipText());
      assertFalse(simpleKMeans0.getFastDistanceCalc());
      assertFalse(simpleKMeans0.getInitializeUsingKMeansPlusPlusMethod());
      assertEquals("Preserve order of instances.", simpleKMeans0.preserveInstancesOrderTipText());
      assertEquals("set maximum number of iterations", simpleKMeans0.maxIterationsTipText());
      assertFalse(simpleKMeans0.getDisplayStdDevs());
      assertEquals("Uses cut-off values for speeding up distance calculation, but suppresses also the calculation and output of the within cluster sum of squared errors/sum of distances.", simpleKMeans0.fastDistanceCalcTipText());
      assertEquals(500, simpleKMeans0.getMaxIterations());
      
      try { 
        evaluation1.evaluateModel((Classifier) kStar0, (Instances) null, (Object[]) kStar0.TAGS_MISSING);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 40
  /*Coverage entropy=1.9535761275244625
  */
  @Test(timeout = 4000)
  public void test040()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      Stacking stacking0 = new Stacking();
      assertNotNull(stacking0);
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      assertEquals(10, stacking0.getNumFolds());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertEquals(1, stacking0.getSeed());
      assertFalse(stacking0.getDebug());
      
      stacking0.setSeed(107);
      assertEquals(107, stacking0.getSeed());
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      assertEquals(10, stacking0.getNumFolds());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertFalse(stacking0.getDebug());
      
      ZeroR zeroR0 = (ZeroR)stacking0.getMetaClassifier();
      assertNotNull(zeroR0);
      assertEquals(107, stacking0.getSeed());
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      assertEquals(10, stacking0.getNumFolds());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertFalse(stacking0.getDebug());
      assertEquals("Class for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).", zeroR0.globalInfo());
      assertFalse(zeroR0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", zeroR0.debugTipText());
      
      ZeroR zeroR1 = new ZeroR();
      assertNotNull(zeroR1);
      assertEquals("If set to true, classifier may output additional info to the console.", zeroR1.debugTipText());
      assertEquals("Class for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).", zeroR1.globalInfo());
      assertFalse(zeroR1.getDebug());
      assertFalse(zeroR1.equals((Object)zeroR0));
      
      String[] stringArray0 = zeroR1.getOptions();
      assertNotNull(stringArray0);
      assertEquals(0, stringArray0.length);
      assertEquals("If set to true, classifier may output additional info to the console.", zeroR1.debugTipText());
      assertEquals("Class for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).", zeroR1.globalInfo());
      assertFalse(zeroR1.getDebug());
      assertFalse(zeroR1.equals((Object)zeroR0));
      assertNotSame(zeroR1, zeroR0);
      
      ZeroR zeroR2 = new ZeroR();
      assertNotNull(zeroR2);
      assertEquals("Class for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).", zeroR2.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", zeroR2.debugTipText());
      assertFalse(zeroR2.getDebug());
      assertFalse(zeroR2.equals((Object)zeroR0));
      assertFalse(zeroR2.equals((Object)zeroR1));
      
      zeroR2.setDebug(false);
      assertEquals("Class for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).", zeroR2.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", zeroR2.debugTipText());
      assertFalse(zeroR2.getDebug());
      assertFalse(zeroR2.equals((Object)zeroR0));
      assertFalse(zeroR2.equals((Object)zeroR1));
      assertNotSame(zeroR2, zeroR0);
      assertNotSame(zeroR2, zeroR1);
      
      WrapperSubsetEval wrapperSubsetEval0 = new WrapperSubsetEval();
      assertNotNull(wrapperSubsetEval0);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      
      Instances instances0 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader0);
      assertNotNull(instances0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      instances0.sort(comparator0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(5);
      assertNotNull(binarySparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(5, binarySparseInstance0.numAttributes());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(5, binarySparseInstance0.numValues());
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(5, binarySparseInstance0.numAttributes());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(5, binarySparseInstance0.numValues());
      assertEquals(5, binarySparseInstance1.numAttributes());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertEquals(5, binarySparseInstance1.numValues());
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance0);
      assertNotNull(sparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(5, sparseInstance0.numAttributes());
      assertEquals(5, sparseInstance0.numValues());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(5, binarySparseInstance0.numAttributes());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(5, binarySparseInstance0.numValues());
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance1));
      
      double[] doubleArray0 = new double[9];
      doubleArray0[0] = (double) 3;
      doubleArray0[1] = (double) 2;
      doubleArray0[2] = (double) 23;
      doubleArray0[3] = (double) 2;
      doubleArray0[4] = (double) 23;
      doubleArray0[5] = 1.2177628261641306E-8;
      doubleArray0[6] = (double) 6;
      doubleArray0[7] = (double) 2;
      doubleArray0[8] = (double) 2;
      try { 
        evaluation0.evaluationForSingleInstance(doubleArray0, (Instance) null, false);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 41
  /*Coverage entropy=3.256982174534803
  */
  @Test(timeout = 4000)
  public void test041()  throws Throwable  {
      MultilayerPerceptron multilayerPerceptron0 = new MultilayerPerceptron();
      assertNotNull(multilayerPerceptron0);
      assertEquals(0, multilayerPerceptron0.getSeed());
      assertEquals("This defines the hidden layers of the neural network. This is a list of positive whole numbers. 1 for each hidden layer. Comma seperated. To have no hidden layers put a single 0 here. This will only be used if autobuild is set. There are also wildcard values 'a' = (attribs + classes) / 2, 'i' = attribs, 'o' = classes , 't' = attribs + classes.", multilayerPerceptron0.hiddenLayersTipText());
      assertTrue(multilayerPerceptron0.getNormalizeAttributes());
      assertEquals("The number of epochs to train through. If the validation set is non-zero then it can terminate the network early", multilayerPerceptron0.trainingTimeTipText());
      assertEquals("This will allow the network to reset with a lower learning rate. If the network diverges from the answer this will automatically reset the network with a lower learning rate and begin training again. This option is only available if the gui is not set. Note that if the network diverges but isn't allowed to reset it will fail the training process and return an error message.", multilayerPerceptron0.resetTipText());
      assertTrue(multilayerPerceptron0.getReset());
      assertEquals("a", multilayerPerceptron0.getHiddenLayers());
      assertEquals("Used to terminate validation testing.The value here dictates how many times in a row the validation set error can get worse before training is terminated.", multilayerPerceptron0.validationThresholdTipText());
      assertTrue(multilayerPerceptron0.getNominalToBinaryFilter());
      assertEquals(0.2, multilayerPerceptron0.getMomentum(), 0.01);
      assertFalse(multilayerPerceptron0.getGUI());
      assertEquals("This will normalize the attributes. This could help improve performance of the network. This is not reliant on the class being numeric. This will also normalize nominal attributes as well (after they have been run through the nominal to binary filter if that is in use) so that the nominal values are between -1 and 1", multilayerPerceptron0.normalizeAttributesTipText());
      assertEquals("This will preprocess the instances with the filter. This could help improve performance if there are nominal attributes in the data.", multilayerPerceptron0.nominalToBinaryFilterTipText());
      assertEquals("Adds and connects up hidden layers in the network.", multilayerPerceptron0.autoBuildTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", multilayerPerceptron0.debugTipText());
      assertFalse(multilayerPerceptron0.getDecay());
      assertTrue(multilayerPerceptron0.getAutoBuild());
      assertEquals("Momentum applied to the weights during updating.", multilayerPerceptron0.momentumTipText());
      assertEquals("The percentage size of the validation set.(The training will continue until it is observed that the error on the validation set has been consistently getting worse, or if the training time is reached).\nIf This is set to zero no validation set will be used and instead the network will train for the specified number of epochs.", multilayerPerceptron0.validationSetSizeTipText());
      assertFalse(multilayerPerceptron0.getDebug());
      assertEquals("This will normalize the class if it's numeric. This could help improve performance of the network, It normalizes the class to be between -1 and 1. Note that this is only internally, the output will be scaled back to the original range.", multilayerPerceptron0.normalizeNumericClassTipText());
      assertEquals("Seed used to initialise the random number generator.Random numbers are used for setting the initial weights of the connections betweem nodes, and also for shuffling the training data.", multilayerPerceptron0.seedTipText());
      assertEquals(500, multilayerPerceptron0.getTrainingTime());
      assertEquals("A Classifier that uses backpropagation to classify instances.\nThis network can be built by hand, created by an algorithm or both. The network can also be monitored and modified during training time. The nodes in this network are all sigmoid (except for when the class is numeric in which case the the output nodes become unthresholded linear units).", multilayerPerceptron0.globalInfo());
      assertTrue(multilayerPerceptron0.getNormalizeNumericClass());
      assertEquals(20, multilayerPerceptron0.getValidationThreshold());
      assertEquals("This will cause the learning rate to decrease. This will divide the starting learning rate by the epoch number, to determine what the current learning rate should be. This may help to stop the network from diverging from the target output, as well as improve general performance. Note that the decaying learning rate will not be shown in the gui, only the original learning rate. If the learning rate is changed in the gui, this is treated as the starting learning rate.", multilayerPerceptron0.decayTipText());
      assertEquals("The amount the weights are updated.", multilayerPerceptron0.learningRateTipText());
      assertEquals(0.3, multilayerPerceptron0.getLearningRate(), 0.01);
      assertEquals(0, multilayerPerceptron0.getValidationSetSize());
      
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      LinearRegression linearRegression0 = new LinearRegression();
      assertNotNull(linearRegression0);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertFalse(linearRegression0.getMinimal());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression0.globalInfo());
      assertFalse(linearRegression0.getDebug());
      assertEquals(1.0E-8, linearRegression0.getRidge(), 0.01);
      assertEquals("Eliminate colinear attributes.", linearRegression0.eliminateColinearAttributesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression0.debugTipText());
      assertTrue(linearRegression0.getEliminateColinearAttributes());
      assertEquals("The value of the Ridge parameter.", linearRegression0.ridgeTipText());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression0.attributeSelectionMethodTipText());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression0.minimalTipText());
      
      LinearRegression linearRegression1 = new LinearRegression();
      assertNotNull(linearRegression1);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression1.debugTipText());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression1.attributeSelectionMethodTipText());
      assertEquals("Eliminate colinear attributes.", linearRegression1.eliminateColinearAttributesTipText());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression1.minimalTipText());
      assertFalse(linearRegression1.getDebug());
      assertTrue(linearRegression1.getEliminateColinearAttributes());
      assertEquals(1.0E-8, linearRegression1.getRidge(), 0.01);
      assertFalse(linearRegression1.getMinimal());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression1.globalInfo());
      assertEquals("The value of the Ridge parameter.", linearRegression1.ridgeTipText());
      assertFalse(linearRegression1.equals((Object)linearRegression0));
      
      SelectedTag selectedTag0 = linearRegression1.getAttributeSelectionMethod();
      assertNotNull(selectedTag0);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression1.debugTipText());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression1.attributeSelectionMethodTipText());
      assertEquals("Eliminate colinear attributes.", linearRegression1.eliminateColinearAttributesTipText());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression1.minimalTipText());
      assertFalse(linearRegression1.getDebug());
      assertTrue(linearRegression1.getEliminateColinearAttributes());
      assertEquals(1.0E-8, linearRegression1.getRidge(), 0.01);
      assertFalse(linearRegression1.getMinimal());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression1.globalInfo());
      assertEquals("The value of the Ridge parameter.", linearRegression1.ridgeTipText());
      assertEquals("0", selectedTag0.toString());
      assertFalse(linearRegression1.equals((Object)linearRegression0));
      assertNotSame(linearRegression1, linearRegression0);
      
      WrapperSubsetEval wrapperSubsetEval0 = new WrapperSubsetEval();
      assertNotNull(wrapperSubsetEval0);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      
      String string0 = wrapperSubsetEval0.toString();
      assertEquals("\tWrapper subset evaluator has not been built yet\n", string0);
      assertNotNull(string0);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      SelectedTag selectedTag1 = new SelectedTag(2, wrapperSubsetEval0.TAGS_EVALUATION);
      assertNotNull(selectedTag1);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      assertEquals("2", selectedTag1.toString());
      assertFalse(selectedTag1.equals((Object)selectedTag0));
      
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumNumeric());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals(1, lMT0.graphType());
      assertFalse(lMT0.getDebug());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      
      InfoGainAttributeEval infoGainAttributeEval0 = new InfoGainAttributeEval();
      assertNotNull(infoGainAttributeEval0);
      assertTrue(infoGainAttributeEval0.getMissingMerge());
      assertEquals("InfoGainAttributeEval :\n\nEvaluates the worth of an attribute by measuring the information gain with respect to the class.\n\nInfoGain(Class,Attribute) = H(Class) - H(Class | Attribute).\n", infoGainAttributeEval0.globalInfo());
      assertEquals("Just binarize numeric attributes instead of properly discretizing them.", infoGainAttributeEval0.binarizeNumericAttributesTipText());
      assertEquals("Distribute counts for missing values. Counts are distributed across other values in proportion to their frequency. Otherwise, missing is treated as a separate value.", infoGainAttributeEval0.missingMergeTipText());
      assertFalse(infoGainAttributeEval0.getBinarizeNumericAttributes());
      
      Enumeration enumeration0 = infoGainAttributeEval0.listOptions();
      assertNotNull(enumeration0);
      assertTrue(infoGainAttributeEval0.getMissingMerge());
      assertEquals("InfoGainAttributeEval :\n\nEvaluates the worth of an attribute by measuring the information gain with respect to the class.\n\nInfoGain(Class,Attribute) = H(Class) - H(Class | Attribute).\n", infoGainAttributeEval0.globalInfo());
      assertEquals("Just binarize numeric attributes instead of properly discretizing them.", infoGainAttributeEval0.binarizeNumericAttributesTipText());
      assertEquals("Distribute counts for missing values. Counts are distributed across other values in proportion to their frequency. Otherwise, missing is treated as a separate value.", infoGainAttributeEval0.missingMergeTipText());
      assertFalse(infoGainAttributeEval0.getBinarizeNumericAttributes());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      assertNotNull(gridBagLayout0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      
      double double0 = evaluation0.falsePositiveRate(8);
      assertEquals(0.0, double0, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      
      DenseInstance denseInstance0 = (DenseInstance)instances0.remove(8);
      assertNotNull(denseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(1, denseInstance0.classIndex());
      assertEquals(2, denseInstance0.numAttributes());
      assertEquals(2, denseInstance0.numClasses());
      assertEquals(2, denseInstance0.numValues());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(19.0, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(19, instances0.size());
      assertEquals(19, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      
      double double1 = evaluation0.weightedFalseNegativeRate();
      assertEquals(Double.NaN, double1, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(19.0, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(19, instances0.size());
      assertEquals(19, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertNotEquals(double1, double0, 0.01);
      
      double double2 = evaluation0.weightedMatthewsCorrelation();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(19.0, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(19, instances0.size());
      assertEquals(19, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertNotEquals(double2, double0, 0.01);
      assertEquals(double2, double1, 0.01);
      
      double double3 = evaluation0.weightedPrecision();
      assertEquals(Double.NaN, double3, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(19.0, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(19, instances0.size());
      assertEquals(19, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(double3, double2, 0.01);
      assertNotEquals(double3, double0, 0.01);
      assertEquals(double3, double1, 0.01);
      
      CoverTree coverTree0 = new CoverTree();
      assertNotNull(coverTree0);
      assertEquals(1.3, coverTree0.getBase(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01);
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01);
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertFalse(coverTree0.getMeasurePerformance());
      
      double double4 = evaluation0.weightedFalsePositiveRate();
      assertEquals(Double.NaN, double4, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(19.0, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(19, instances0.size());
      assertEquals(19, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(double4, double1, 0.01);
      assertNotEquals(double4, double0, 0.01);
      assertEquals(double4, double2, 0.01);
      assertEquals(double4, double3, 0.01);
      
      double double5 = evaluation0.pctIncorrect();
      assertEquals(Double.NaN, double5, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(19.0, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(19, instances0.size());
      assertEquals(19, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertNotEquals(double5, double0, 0.01);
      assertEquals(double5, double3, 0.01);
      assertEquals(double5, double2, 0.01);
      assertEquals(double5, double4, 0.01);
      assertEquals(double5, double1, 0.01);
      
      double double6 = evaluation0.SFMeanEntropyGain();
      assertEquals(Double.NaN, double6, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(19.0, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(19, instances0.size());
      assertEquals(19, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertNotEquals(double6, double0, 0.01);
      assertEquals(double6, double2, 0.01);
      assertEquals(double6, double5, 0.01);
      assertEquals(double6, double3, 0.01);
      assertEquals(double6, double4, 0.01);
      assertEquals(double6, double1, 0.01);
      
      double double7 = evaluation0.numTrueNegatives(3);
      assertEquals(0.0, double7, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(19.0, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(19, instances0.size());
      assertEquals(19, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertNotEquals(double7, double3, 0.01);
      assertNotEquals(double7, double1, 0.01);
      assertNotEquals(double7, double5, 0.01);
      assertNotEquals(double7, double2, 0.01);
      assertNotEquals(double7, double6, 0.01);
      assertNotEquals(double7, double4, 0.01);
      assertEquals(double7, double0, 0.01);
      
      // Undeclared exception!
      try { 
        evaluation0.recall(5);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 5
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 42
  /*Coverage entropy=2.2198385585424116
  */
  @Test(timeout = 4000)
  public void test042()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getClassType());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getClassType());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getClassType());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      
      double[] doubleArray0 = new double[2];
      doubleArray0[0] = 0.693147181;
      doubleArray0[1] = (double) (-1);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0.693147181, doubleArray0);
      assertNotNull(binarySparseInstance0);
      assertEquals(2, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {0.693147181, (-1.0)}, doubleArray0, 0.01);
      assertEquals(2, binarySparseInstance0.numAttributes());
      assertEquals(0.693147181, binarySparseInstance0.weight(), 0.01);
      assertEquals(2, binarySparseInstance0.numValues());
      
      SparseInstance sparseInstance0 = new SparseInstance(2, doubleArray0);
      assertNotNull(sparseInstance0);
      assertEquals(2, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {0.693147181, (-1.0)}, doubleArray0, 0.01);
      assertEquals(2.0, sparseInstance0.weight(), 0.01);
      assertEquals(2, sparseInstance0.numValues());
      assertEquals(2, sparseInstance0.numAttributes());
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance(sparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(2, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {0.693147181, (-1.0)}, doubleArray0, 0.01);
      assertEquals(2.0, sparseInstance0.weight(), 0.01);
      assertEquals(2, sparseInstance0.numValues());
      assertEquals(2, sparseInstance0.numAttributes());
      assertEquals(2, binarySparseInstance1.numValues());
      assertEquals(2, binarySparseInstance1.numAttributes());
      assertEquals(2.0, binarySparseInstance1.weight(), 0.01);
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((SparseInstance) binarySparseInstance1);
      assertNotNull(binarySparseInstance2);
      assertEquals(2, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {0.693147181, (-1.0)}, doubleArray0, 0.01);
      assertEquals(2.0, sparseInstance0.weight(), 0.01);
      assertEquals(2, sparseInstance0.numValues());
      assertEquals(2, sparseInstance0.numAttributes());
      assertEquals(2, binarySparseInstance1.numValues());
      assertEquals(2, binarySparseInstance1.numAttributes());
      assertEquals(2.0, binarySparseInstance1.weight(), 0.01);
      assertEquals(2.0, binarySparseInstance2.weight(), 0.01);
      assertEquals(2, binarySparseInstance2.numValues());
      assertEquals(2, binarySparseInstance2.numAttributes());
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      assertFalse(binarySparseInstance2.equals((Object)binarySparseInstance1));
      assertFalse(binarySparseInstance2.equals((Object)binarySparseInstance0));
      
      Evaluation evaluation1 = new Evaluation(instances0);
      assertNotNull(evaluation1);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getClassType());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      AdditiveRegression additiveRegression0 = new AdditiveRegression();
      assertNotNull(additiveRegression0);
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01);
      assertEquals(10, additiveRegression0.getNumIterations());
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01);
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertFalse(additiveRegression0.getDebug());
      
      String string0 = Evaluation.getGlobalInfo(additiveRegression0);
      assertEquals("\nSynopsis for weka.classifiers.meta.AdditiveRegression:\n\n Meta classifier that enhances the performance of a regression base classifier. Each iteration fits a model to the residuals left by the classifier on the previous iteration. Prediction is accomplished by adding the predictions of each classifier. Reducing the shrinkage (learning rate) parameter helps prevent overfitting and has a smoothing effect but increases the learning time.\n\nFor more information see:\n\nJ.H. Friedman (1999). Stochastic Gradient Boosting.", string0);
      assertNotNull(string0);
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01);
      assertEquals(10, additiveRegression0.getNumIterations());
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01);
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertFalse(additiveRegression0.getDebug());
      
      double double0 = evaluation1.m_SumAbsErr;
      assertEquals(0.0, double0, 0.01);
      
      ArffLoader arffLoader0 = new ArffLoader();
      assertNotNull(arffLoader0);
      assertEquals("Arff data files", arffLoader0.getFileDescription());
      assertEquals("Use relative rather than absolute paths", arffLoader0.useRelativePathTipText());
      assertFalse(arffLoader0.getUseRelativePath());
      assertEquals("Reads a source that is in arff (attribute relation file format) format. ", arffLoader0.globalInfo());
      assertEquals("http://", arffLoader0.retrieveURL());
      assertEquals(".arff", arffLoader0.getFileExtension());
      
      Instance instance0 = arffLoader0.getNextInstance(instances0);
      assertNull(instance0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getClassType());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals("Arff data files", arffLoader0.getFileDescription());
      assertEquals("Use relative rather than absolute paths", arffLoader0.useRelativePathTipText());
      assertFalse(arffLoader0.getUseRelativePath());
      assertEquals("Reads a source that is in arff (attribute relation file format) format. ", arffLoader0.globalInfo());
      assertEquals("http://", arffLoader0.retrieveURL());
      assertEquals(".arff", arffLoader0.getFileExtension());
      
      boolean boolean0 = FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      Evaluation evaluation2 = new Evaluation(instances0, (CostMatrix) null);
      assertNotNull(evaluation2);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getClassType());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertTrue(evaluation2.equals((Object)evaluation0));
      
      try { 
        evaluation2.evaluateModelOnce(doubleArray0, (Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 43
  /*Coverage entropy=2.5639967245132667
  */
  @Test(timeout = 4000)
  public void test043()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      boolean boolean0 = FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(15, lMT0.getMinNumInstances());
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.classIndex());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier3);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      
      CostMatrix costMatrix0 = costSensitiveClassifier3.getCostMatrix();
      assertNotNull(costMatrix0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals(1, costMatrix0.numRows());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.size());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier2);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier1);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      
      double double0 = evaluation0.weightedTrueNegativeRate();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      
      double double1 = evaluation0.unweightedMacroFmeasure();
      assertEquals(Double.NaN, double1, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(double1, double0, 0.01);
      
      DenseInstance denseInstance0 = new DenseInstance(1);
      assertNotNull(denseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(1, denseInstance0.numAttributes());
      assertEquals(1, denseInstance0.numValues());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      
      double double2 = evaluation0.m_MissingClass;
      assertEquals(0.0, double2, 0.01);
      assertNotEquals(double2, double1, 0.01);
      assertNotEquals(double2, double0, 0.01);
      
      double double3 = evaluation0.trueNegativeRate(2);
      assertEquals(0.0, double3, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertNotEquals(double3, double1, 0.01);
      assertNotEquals(double3, double0, 0.01);
      assertEquals(double3, double2, 0.01);
      
      double double4 = evaluation0.falseNegativeRate(106);
      assertEquals(0.0, double4, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getUseAIC());
      assertTrue(lMT0.getFastRegression());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertFalse(lMT0.getConvertNominal());
      assertFalse(lMT0.getDebug());
      assertFalse(lMT0.getSplitOnResiduals());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertNotEquals(double4, double1, 0.01);
      assertNotEquals(double4, double0, 0.01);
      assertEquals(double4, double2, 0.01);
      assertEquals(double4, double3, 0.01);
  }

  /**
  //Test case number: 44
  /*Coverage entropy=2.593887521117397
  */
  @Test(timeout = 4000)
  public void test044()  throws Throwable  {
      boolean boolean0 = FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "relational-atts");
      assertFalse(boolean0);
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances0);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      
      TextDirectoryLoader textDirectoryLoader1 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader1);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertFalse(textDirectoryLoader1.getDebug());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      
      Instances instances1 = textDirectoryLoader1.getDataSet();
      assertNotNull(instances1);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertFalse(textDirectoryLoader1.getDebug());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertEquals(1, instances1.classIndex());
      assertEquals(0, instances1.numClasses());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(0, instances1.numInstances());
      assertEquals(2, instances1.numAttributes());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertTrue(instances1.equals((Object)instances0));
      assertNotSame(textDirectoryLoader1, textDirectoryLoader0);
      assertNotSame(instances1, instances0);
      
      double double0 = evaluation0.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertTrue(instances0.equals((Object)instances1));
      assertNotSame(textDirectoryLoader0, textDirectoryLoader1);
      assertNotSame(instances0, instances1);
      
      TextDirectoryLoader textDirectoryLoader2 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader2);
      assertFalse(textDirectoryLoader2.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader2.globalInfo());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader2.charSetTipText());
      assertFalse(textDirectoryLoader2.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader2.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader2.debugTipText());
      assertEquals("", textDirectoryLoader2.getCharSet());
      assertEquals("Directories", textDirectoryLoader2.getFileDescription());
      assertFalse(textDirectoryLoader2.equals((Object)textDirectoryLoader0));
      assertFalse(textDirectoryLoader2.equals((Object)textDirectoryLoader1));
      
      DatabaseLoader databaseLoader0 = new DatabaseLoader();
      assertNotNull(databaseLoader0);
      assertEquals("", databaseLoader0.getPassword());
      assertFalse(databaseLoader0.getSparseData());
      assertEquals("Select * from Results0", databaseLoader0.getQuery());
      assertEquals("For incremental loading a unique identiefer has to be specified.\nIf the query includes all columns of a table (SELECT *...) a primary key\ncan be detected automatically depending on the JDBC driver. If that is not possible\nspecify the key columns here in a comma separated list.", databaseLoader0.keysTipText());
      assertEquals("The custom properties that the user can use to override the default ones.", databaseLoader0.customPropsFileTipText());
      assertEquals("The query that should load the instances.\n The query has to be of the form SELECT <column-list>|* FROM <table> [WHERE <conditions>]", databaseLoader0.queryTipText());
      assertEquals("The database password", databaseLoader0.passwordTipText());
      assertEquals("The user name for the database", databaseLoader0.userTipText());
      assertEquals("jdbc:idb=experiments.prp", databaseLoader0.getUrl());
      assertEquals("Encode data as sparse instances.", databaseLoader0.sparseDataTipText());
      assertEquals("", databaseLoader0.getUser());
      assertEquals("The URL of the database", databaseLoader0.urlTipText());
      
      Instances instances2 = ConverterUtils.DataSource.read((Loader) databaseLoader0);
      assertNull(instances2);
      assertEquals("", databaseLoader0.getPassword());
      assertFalse(databaseLoader0.getSparseData());
      assertEquals("Select * from Results0", databaseLoader0.getQuery());
      assertEquals("For incremental loading a unique identiefer has to be specified.\nIf the query includes all columns of a table (SELECT *...) a primary key\ncan be detected automatically depending on the JDBC driver. If that is not possible\nspecify the key columns here in a comma separated list.", databaseLoader0.keysTipText());
      assertEquals("The custom properties that the user can use to override the default ones.", databaseLoader0.customPropsFileTipText());
      assertEquals("The query that should load the instances.\n The query has to be of the form SELECT <column-list>|* FROM <table> [WHERE <conditions>]", databaseLoader0.queryTipText());
      assertEquals("The database password", databaseLoader0.passwordTipText());
      assertEquals("The user name for the database", databaseLoader0.userTipText());
      assertEquals("jdbc:idb=experiments.prp", databaseLoader0.getUrl());
      assertEquals("Encode data as sparse instances.", databaseLoader0.sparseDataTipText());
      assertEquals("", databaseLoader0.getUser());
      assertEquals("The URL of the database", databaseLoader0.urlTipText());
      
      Evaluation evaluation1 = new Evaluation(instances0);
      assertNotNull(evaluation1);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader2));
      assertTrue(instances0.equals((Object)instances1));
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      double double1 = evaluation0.truePositiveRate(11);
      assertEquals(0.0, double1, 0.01);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader1));
      assertFalse(textDirectoryLoader0.equals((Object)textDirectoryLoader2));
      assertTrue(instances0.equals((Object)instances1));
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotEquals(double1, double0, 0.01);
      assertNotSame(textDirectoryLoader0, textDirectoryLoader1);
      assertNotSame(textDirectoryLoader0, textDirectoryLoader2);
      assertNotSame(instances0, instances1);
      assertNotSame(evaluation0, evaluation1);
      
      DenseInstance denseInstance0 = new DenseInstance(10);
      assertNotNull(denseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(10, denseInstance0.numAttributes());
      assertEquals(10, denseInstance0.numValues());
      
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = 0.0;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(11);
      assertNotNull(binarySparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(11, binarySparseInstance0.numValues());
      assertEquals(11, binarySparseInstance0.numAttributes());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      
      LinkedList<String> linkedList0 = new LinkedList<String>();
      assertNotNull(linkedList0);
      assertEquals(0, linkedList0.size());
      assertFalse(linkedList0.contains("relational-atts"));
      
      Properties properties0 = new Properties();
      assertNotNull(properties0);
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      assertNotNull(protectedProperties0);
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      assertEquals(0, protectedProperties0.size());
      assertTrue(protectedProperties0.isEmpty());
      
      Attribute attribute0 = new Attribute("(", linkedList0, protectedProperties0);
      assertNotNull(attribute0);
      assertEquals(1, Attribute.ORDERING_ORDERED);
      assertEquals(1, Attribute.NOMINAL);
      assertEquals(2, Attribute.STRING);
      assertEquals(2, Attribute.ORDERING_MODULO);
      assertEquals(3, Attribute.DATE);
      assertEquals(4, Attribute.RELATIONAL);
      assertEquals(0, Attribute.ORDERING_SYMBOLIC);
      assertEquals(0, Attribute.NUMERIC);
      assertEquals(0, linkedList0.size());
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      assertEquals(0, protectedProperties0.size());
      assertTrue(protectedProperties0.isEmpty());
      assertFalse(attribute0.lowerNumericBoundIsOpen());
      assertFalse(attribute0.hasZeropoint());
      assertFalse(attribute0.isRelationValued());
      assertFalse(attribute0.isRegular());
      assertFalse(attribute0.isString());
      assertEquals(0, attribute0.ordering());
      assertEquals(0.0, attribute0.getLowerNumericBound(), 0.01);
      assertEquals(0.0, attribute0.getUpperNumericBound(), 0.01);
      assertFalse(attribute0.isAveragable());
      assertEquals((-1), attribute0.index());
      assertEquals("", attribute0.getDateFormat());
      assertEquals(1.0, attribute0.weight(), 0.01);
      assertFalse(attribute0.isNumeric());
      assertFalse(attribute0.upperNumericBoundIsOpen());
      assertEquals("(", attribute0.name());
      assertEquals(0, attribute0.numValues());
      assertFalse(attribute0.isDate());
      assertEquals(1, attribute0.type());
      assertTrue(attribute0.isNominal());
      assertFalse(linkedList0.contains("("));
      
      binarySparseInstance0.setMissing(attribute0);
      assertEquals(1, Attribute.ORDERING_ORDERED);
      assertEquals(1, Attribute.NOMINAL);
      assertEquals(2, Attribute.STRING);
      assertEquals(2, Attribute.ORDERING_MODULO);
      assertEquals(3, Attribute.DATE);
      assertEquals(4, Attribute.RELATIONAL);
      assertEquals(0, Attribute.ORDERING_SYMBOLIC);
      assertEquals(0, Attribute.NUMERIC);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, linkedList0.size());
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      assertEquals(0, protectedProperties0.size());
      assertTrue(protectedProperties0.isEmpty());
      assertFalse(attribute0.lowerNumericBoundIsOpen());
      assertFalse(attribute0.hasZeropoint());
      assertFalse(attribute0.isRelationValued());
      assertFalse(attribute0.isRegular());
      assertFalse(attribute0.isString());
      assertEquals(0, attribute0.ordering());
      assertEquals(0.0, attribute0.getLowerNumericBound(), 0.01);
      assertEquals(0.0, attribute0.getUpperNumericBound(), 0.01);
      assertFalse(attribute0.isAveragable());
      assertEquals((-1), attribute0.index());
      assertEquals("", attribute0.getDateFormat());
      assertEquals(1.0, attribute0.weight(), 0.01);
      assertFalse(attribute0.isNumeric());
      assertFalse(attribute0.upperNumericBoundIsOpen());
      assertEquals("(", attribute0.name());
      assertEquals(0, attribute0.numValues());
      assertFalse(attribute0.isDate());
      assertEquals(1, attribute0.type());
      assertTrue(attribute0.isNominal());
      assertEquals(12, binarySparseInstance0.numValues());
      assertEquals(11, binarySparseInstance0.numAttributes());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertFalse(linkedList0.contains("("));
      
      SparseInstance sparseInstance0 = new SparseInstance(4.9E-324, doubleArray0);
      assertNotNull(sparseInstance0);
      assertEquals(1, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {0.0}, doubleArray0, 0.01);
      assertEquals(4.9E-324, sparseInstance0.weight(), 0.01);
      assertEquals(1, sparseInstance0.numAttributes());
      assertEquals(0, sparseInstance0.numValues());
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance(sparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(1, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {0.0}, doubleArray0, 0.01);
      assertEquals(4.9E-324, sparseInstance0.weight(), 0.01);
      assertEquals(1, sparseInstance0.numAttributes());
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(0, binarySparseInstance1.numValues());
      assertEquals(4.9E-324, binarySparseInstance1.weight(), 0.01);
      assertEquals(1, binarySparseInstance1.numAttributes());
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      Evaluation evaluation2 = new Evaluation(instances1);
      assertNotNull(evaluation2);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertFalse(textDirectoryLoader1.getDebug());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertEquals(1, instances1.classIndex());
      assertEquals(0, instances1.numClasses());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(0, instances1.numInstances());
      assertEquals(2, instances1.numAttributes());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader2));
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertTrue(instances1.equals((Object)instances0));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      
      BinarySparseInstance binarySparseInstance2 = (BinarySparseInstance)binarySparseInstance0.mergeInstance(sparseInstance0);
      assertNotNull(binarySparseInstance2);
      assertEquals(1, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {0.0}, doubleArray0, 0.01);
      assertEquals(4.9E-324, sparseInstance0.weight(), 0.01);
      assertEquals(1, sparseInstance0.numAttributes());
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance2.weight(), 0.01);
      assertEquals(12, binarySparseInstance2.numValues());
      assertEquals(12, binarySparseInstance2.numAttributes());
      assertEquals(12, binarySparseInstance0.numValues());
      assertEquals(11, binarySparseInstance0.numAttributes());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertFalse(binarySparseInstance2.equals((Object)binarySparseInstance0));
      assertFalse(binarySparseInstance2.equals((Object)binarySparseInstance1));
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance1));
      assertNotSame(binarySparseInstance2, binarySparseInstance0);
      assertNotSame(binarySparseInstance2, binarySparseInstance1);
      assertNotSame(binarySparseInstance0, binarySparseInstance2);
      assertNotSame(binarySparseInstance0, binarySparseInstance1);
      
      double double2 = evaluation2.priorEntropy();
      assertEquals(0.0, double2, 0.01);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertFalse(textDirectoryLoader1.getDebug());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertEquals(1, instances1.classIndex());
      assertEquals(0, instances1.numClasses());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(0, instances1.numInstances());
      assertEquals(2, instances1.numAttributes());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader2));
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertTrue(instances1.equals((Object)instances0));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertEquals(double2, double1, 0.01);
      assertNotEquals(double2, double0, 0.01);
      assertNotSame(textDirectoryLoader1, textDirectoryLoader2);
      assertNotSame(textDirectoryLoader1, textDirectoryLoader0);
      assertNotSame(instances1, instances0);
      assertNotSame(evaluation2, evaluation1);
      assertNotSame(evaluation2, evaluation0);
      
      double double3 = evaluation2.meanPriorAbsoluteError();
      assertEquals(Double.NaN, double3, 0.01);
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertFalse(textDirectoryLoader1.getDebug());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertEquals(1, instances1.classIndex());
      assertEquals(0, instances1.numClasses());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(0, instances1.numInstances());
      assertEquals(2, instances1.numAttributes());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader2));
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertTrue(instances1.equals((Object)instances0));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertNotEquals(double3, double2, 0.01);
      assertNotEquals(double3, double1, 0.01);
      assertEquals(double3, double0, 0.01);
      assertNotSame(textDirectoryLoader1, textDirectoryLoader2);
      assertNotSame(textDirectoryLoader1, textDirectoryLoader0);
      assertNotSame(instances1, instances0);
      assertNotSame(evaluation2, evaluation1);
      assertNotSame(evaluation2, evaluation0);
      
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      assertNotNull(adaBoostM1_0);
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      
      try { 
        evaluation1.evaluateModelOnce((Classifier) adaBoostM1_0, (Instance) binarySparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 45
  /*Coverage entropy=1.9753765189753083
  */
  @Test(timeout = 4000)
  public void test045()  throws Throwable  {
      boolean boolean0 = FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "&c.)(d/?o_.ZQE'^.(");
      assertFalse(boolean0);
      
      Stacking stacking0 = new Stacking();
      assertNotNull(stacking0);
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      assertFalse(stacking0.getDebug());
      assertEquals(10, stacking0.getNumFolds());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertEquals(1, stacking0.getSeed());
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertNotNull(instances0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numInstances());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      
      Instances instances1 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances1);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals(0, instances1.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(1, instances1.classIndex());
      assertEquals(0, instances1.numInstances());
      assertEquals(2, instances1.numAttributes());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertSame(instances1, instances0);
      
      double double0 = evaluation0.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(0, instances0.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertSame(instances0, instances1);
      
      int int0 = 11;
      TextDirectoryLoader textDirectoryLoader1 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader1);
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertFalse(textDirectoryLoader1.getDebug());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      
      Instances instances2 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader1);
      assertNotNull(instances2);
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertEquals(0, instances2.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertEquals(1, instances2.classIndex());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertTrue(instances2.checkForStringAttributes());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertTrue(instances2.equals((Object)instances1));
      assertTrue(instances2.equals((Object)instances0));
      assertNotSame(textDirectoryLoader1, textDirectoryLoader0);
      assertNotSame(instances2, instances1);
      assertNotSame(instances2, instances0);
      
      Evaluation evaluation1 = new Evaluation(instances2);
      assertNotNull(evaluation1);
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertEquals(0, instances2.numClasses());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertEquals(1, instances2.classIndex());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertTrue(instances2.equals((Object)instances1));
      assertTrue(instances2.equals((Object)instances0));
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      DenseInstance denseInstance0 = new DenseInstance(11);
      assertNotNull(denseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(11, denseInstance0.numAttributes());
      assertEquals(11, denseInstance0.numValues());
      
      double[] doubleArray0 = new double[3];
      doubleArray0[0] = (-1208.3155);
      doubleArray0[1] = Double.NaN;
      doubleArray0[2] = (double) 2;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(2, doubleArray0);
      assertNotNull(binarySparseInstance0);
      assertEquals(3, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {(-1208.3155), Double.NaN, 2.0}, doubleArray0, 0.01);
      assertEquals(3, binarySparseInstance0.numAttributes());
      assertEquals(3, binarySparseInstance0.numValues());
      assertEquals(2.0, binarySparseInstance0.weight(), 0.01);
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(3, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {(-1208.3155), Double.NaN, 2.0}, doubleArray0, 0.01);
      assertEquals(3, binarySparseInstance0.numAttributes());
      assertEquals(3, binarySparseInstance0.numValues());
      assertEquals(2.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(2.0, binarySparseInstance1.weight(), 0.01);
      assertEquals(3, binarySparseInstance1.numValues());
      assertEquals(3, binarySparseInstance1.numAttributes());
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      try { 
        evaluation1.updateStatsForClassifier(doubleArray0, denseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 46
  /*Coverage entropy=1.9535761275244625
  */
  @Test(timeout = 4000)
  public void test046()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      
      DenseInstance denseInstance0 = new DenseInstance(25);
      assertNotNull(denseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(25, denseInstance0.numValues());
      assertEquals(25, denseInstance0.numAttributes());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      
      Instances instances1 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader0);
      assertNotNull(instances1);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(1, instances1.classIndex());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(2, instances1.numAttributes());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertEquals(0, instances1.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances1.relationName());
      assertEquals(0, instances1.numClasses());
      assertTrue(instances1.equals((Object)instances0));
      assertNotSame(instances1, instances0);
      
      Instances instances2 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances2);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(1, instances2.classIndex());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertEquals(0, instances2.numClasses());
      assertTrue(instances2.equals((Object)instances0));
      assertNotSame(instances2, instances0);
      assertSame(instances2, instances1);
      
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      instances2.sort(comparator0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(1, instances2.classIndex());
      assertEquals(0.0, instances2.sumOfWeights(), 0.01);
      assertEquals(0, instances2.size());
      assertTrue(instances2.checkForStringAttributes());
      assertEquals(0, instances2.numInstances());
      assertEquals(2, instances2.numAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances2.relationName());
      assertEquals(0, instances2.numClasses());
      assertTrue(instances2.equals((Object)instances0));
      assertNotSame(instances2, instances0);
      assertSame(instances2, instances1);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertTrue(instances0.equals((Object)instances2));
      assertTrue(instances0.equals((Object)instances1));
      
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(16);
      assertNotNull(binarySparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(16, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(16, binarySparseInstance0.numAttributes());
      
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      assertNotNull(binarySparseInstance1);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(16, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(16, binarySparseInstance0.numAttributes());
      assertEquals(16, binarySparseInstance1.numValues());
      assertEquals(16, binarySparseInstance1.numAttributes());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      
      binarySparseInstance1.deleteAttributeAt(2);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(16, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(16, binarySparseInstance0.numAttributes());
      assertEquals(15, binarySparseInstance1.numValues());
      assertEquals(15, binarySparseInstance1.numAttributes());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance1));
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      assertNotSame(binarySparseInstance0, binarySparseInstance1);
      assertNotSame(binarySparseInstance1, binarySparseInstance0);
      
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((Instance) binarySparseInstance1);
      assertNotNull(binarySparseInstance2);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(16, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(16, binarySparseInstance0.numAttributes());
      assertEquals(15, binarySparseInstance1.numValues());
      assertEquals(15, binarySparseInstance1.numAttributes());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertEquals(15, binarySparseInstance2.numAttributes());
      assertEquals(1.0, binarySparseInstance2.weight(), 0.01);
      assertEquals(15, binarySparseInstance2.numValues());
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance1));
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      assertFalse(binarySparseInstance2.equals((Object)binarySparseInstance1));
      assertFalse(binarySparseInstance2.equals((Object)binarySparseInstance0));
      
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance2);
      assertNotNull(sparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(16, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(16, binarySparseInstance0.numAttributes());
      assertEquals(15, binarySparseInstance1.numValues());
      assertEquals(15, binarySparseInstance1.numAttributes());
      assertEquals(1.0, binarySparseInstance1.weight(), 0.01);
      assertEquals(15, binarySparseInstance2.numAttributes());
      assertEquals(1.0, binarySparseInstance2.weight(), 0.01);
      assertEquals(15, binarySparseInstance2.numValues());
      assertEquals(15, sparseInstance0.numValues());
      assertEquals(15, sparseInstance0.numAttributes());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance2));
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance1));
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance2));
      assertFalse(binarySparseInstance2.equals((Object)binarySparseInstance1));
      assertFalse(binarySparseInstance2.equals((Object)binarySparseInstance0));
      
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = (double) 2;
      try { 
        evaluation0.evaluationForSingleInstance(doubleArray0, binarySparseInstance0, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 47
  /*Coverage entropy=1.5713732392451203
  */
  @Test(timeout = 4000)
  public void test047()  throws Throwable  {
      Stacking stacking0 = new Stacking();
      assertNotNull(stacking0);
      assertEquals(1, stacking0.getSeed());
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertEquals(10, stacking0.getNumFolds());
      assertFalse(stacking0.getDebug());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      
      LinearRegression linearRegression0 = new LinearRegression();
      assertNotNull(linearRegression0);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression0.debugTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression0.ridgeTipText());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression0.minimalTipText());
      assertTrue(linearRegression0.getEliminateColinearAttributes());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression0.attributeSelectionMethodTipText());
      assertFalse(linearRegression0.getMinimal());
      assertEquals(1.0E-8, linearRegression0.getRidge(), 0.01);
      assertFalse(linearRegression0.getDebug());
      assertEquals("Eliminate colinear attributes.", linearRegression0.eliminateColinearAttributesTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression0.globalInfo());
      
      LinearRegression linearRegression1 = new LinearRegression();
      assertNotNull(linearRegression1);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression1.debugTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression1.ridgeTipText());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression1.attributeSelectionMethodTipText());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression1.minimalTipText());
      assertTrue(linearRegression1.getEliminateColinearAttributes());
      assertFalse(linearRegression1.getMinimal());
      assertEquals(1.0E-8, linearRegression1.getRidge(), 0.01);
      assertFalse(linearRegression1.getDebug());
      assertEquals("Eliminate colinear attributes.", linearRegression1.eliminateColinearAttributesTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression1.globalInfo());
      assertFalse(linearRegression1.equals((Object)linearRegression0));
      
      Enumeration enumeration0 = linearRegression0.listOptions();
      assertNotNull(enumeration0);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression0.debugTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression0.ridgeTipText());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression0.minimalTipText());
      assertTrue(linearRegression0.getEliminateColinearAttributes());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression0.attributeSelectionMethodTipText());
      assertFalse(linearRegression0.getMinimal());
      assertEquals(1.0E-8, linearRegression0.getRidge(), 0.01);
      assertFalse(linearRegression0.getDebug());
      assertEquals("Eliminate colinear attributes.", linearRegression0.eliminateColinearAttributesTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression0.globalInfo());
      assertFalse(linearRegression0.equals((Object)linearRegression1));
      assertNotSame(linearRegression0, linearRegression1);
      
      SelectedTag selectedTag0 = linearRegression1.getAttributeSelectionMethod();
      assertNotNull(selectedTag0);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression1.debugTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression1.ridgeTipText());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression1.attributeSelectionMethodTipText());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression1.minimalTipText());
      assertTrue(linearRegression1.getEliminateColinearAttributes());
      assertFalse(linearRegression1.getMinimal());
      assertEquals(1.0E-8, linearRegression1.getRidge(), 0.01);
      assertFalse(linearRegression1.getDebug());
      assertEquals("Eliminate colinear attributes.", linearRegression1.eliminateColinearAttributesTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression1.globalInfo());
      assertEquals("0", selectedTag0.toString());
      assertFalse(linearRegression1.equals((Object)linearRegression0));
      assertNotSame(linearRegression1, linearRegression0);
      
      WrapperSubsetEval wrapperSubsetEval0 = new WrapperSubsetEval();
      assertNotNull(wrapperSubsetEval0);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      
      String string0 = wrapperSubsetEval0.toString();
      assertEquals("\tWrapper subset evaluator has not been built yet\n", string0);
      assertNotNull(string0);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      
      SelectedTag selectedTag1 = new SelectedTag(0, linearRegression1.TAGS_SELECTION);
      assertNotNull(selectedTag1);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression1.debugTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression1.ridgeTipText());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression1.attributeSelectionMethodTipText());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression1.minimalTipText());
      assertTrue(linearRegression1.getEliminateColinearAttributes());
      assertFalse(linearRegression1.getMinimal());
      assertEquals(1.0E-8, linearRegression1.getRidge(), 0.01);
      assertFalse(linearRegression1.getDebug());
      assertEquals("Eliminate colinear attributes.", linearRegression1.eliminateColinearAttributesTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression1.globalInfo());
      assertEquals("0", selectedTag1.toString());
      assertFalse(linearRegression1.equals((Object)linearRegression0));
      assertTrue(selectedTag1.equals((Object)selectedTag0));
      
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertFalse(lMT0.getSplitOnResiduals());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getDebug());
      
      InfoGainAttributeEval infoGainAttributeEval0 = new InfoGainAttributeEval();
      assertNotNull(infoGainAttributeEval0);
      assertFalse(infoGainAttributeEval0.getBinarizeNumericAttributes());
      assertEquals("InfoGainAttributeEval :\n\nEvaluates the worth of an attribute by measuring the information gain with respect to the class.\n\nInfoGain(Class,Attribute) = H(Class) - H(Class | Attribute).\n", infoGainAttributeEval0.globalInfo());
      assertEquals("Just binarize numeric attributes instead of properly discretizing them.", infoGainAttributeEval0.binarizeNumericAttributesTipText());
      assertTrue(infoGainAttributeEval0.getMissingMerge());
      assertEquals("Distribute counts for missing values. Counts are distributed across other values in proportion to their frequency. Otherwise, missing is treated as a separate value.", infoGainAttributeEval0.missingMergeTipText());
      
      Enumeration enumeration1 = infoGainAttributeEval0.listOptions();
      assertNotNull(enumeration1);
      assertFalse(infoGainAttributeEval0.getBinarizeNumericAttributes());
      assertEquals("InfoGainAttributeEval :\n\nEvaluates the worth of an attribute by measuring the information gain with respect to the class.\n\nInfoGain(Class,Attribute) = H(Class) - H(Class | Attribute).\n", infoGainAttributeEval0.globalInfo());
      assertEquals("Just binarize numeric attributes instead of properly discretizing them.", infoGainAttributeEval0.binarizeNumericAttributesTipText());
      assertTrue(infoGainAttributeEval0.getMissingMerge());
      assertEquals("Distribute counts for missing values. Counts are distributed across other values in proportion to their frequency. Otherwise, missing is treated as a separate value.", infoGainAttributeEval0.missingMergeTipText());
      assertFalse(enumeration1.equals((Object)enumeration0));
      assertNotSame(enumeration1, enumeration0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      
      GaussianProcesses gaussianProcesses0 = new GaussianProcesses();
      assertNotNull(gaussianProcesses0);
      assertEquals(0, GaussianProcesses.FILTER_NORMALIZE);
      assertEquals(1, GaussianProcesses.FILTER_STANDARDIZE);
      assertEquals(2, GaussianProcesses.FILTER_NONE);
      assertEquals("The kernel to use.", gaussianProcesses0.kernelTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", gaussianProcesses0.debugTipText());
      assertEquals(1.0, gaussianProcesses0.getNoise(), 0.01);
      assertEquals("Determines how/if the data will be transformed.", gaussianProcesses0.filterTypeTipText());
      assertEquals(" Implements Gaussian processes for regression without hyperparameter-tuning. To make choosing an appropriate noise level easier, this implementation applies normalization/standardization to the target attribute as well as the other attributes (if  normalization/standardizaton is turned on). Missing values are replaced by the global mean/mode. Nominal attributes are converted to binary ones. Note that kernel caching is turned off if the kernel used implements CachedKernel.", gaussianProcesses0.globalInfo());
      assertFalse(gaussianProcesses0.getDebug());
      assertEquals("The level of Gaussian Noise (added to the diagonal of the Covariance Matrix), after the target has been normalized/standardized/left unchanged).", gaussianProcesses0.noiseTipText());
      
      try { 
        evaluation0.updateStatsForIntervalEstimator(gaussianProcesses0, (Instance) null, (-1604.7869550939));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.GaussianProcesses", e);
      }
  }

  /**
  //Test case number: 48
  /*Coverage entropy=2.179270351202691
  */
  @Test(timeout = 4000)
  public void test048()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      
      Double double0 = new Double(0.0);
      assertEquals(0.0, (double)double0, 0.01);
      assertNotNull(double0);
      
      instances0.deleteStringAttributes();
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      CostMatrix costMatrix0 = new CostMatrix(2);
      assertNotNull(costMatrix0);
      assertEquals(2, costMatrix0.numColumns());
      assertEquals(2, costMatrix0.size());
      assertEquals(2, costMatrix0.numRows());
      
      Instances instances1 = testInstances0.generate();
      assertNotNull(instances1);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, instances1.numInstances());
      assertEquals(20.0, instances1.sumOfWeights(), 0.01);
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(20, instances1.size());
      assertEquals(2, instances1.numAttributes());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(1, instances1.classIndex());
      assertEquals(2, instances1.numClasses());
      assertFalse(instances1.equals((Object)instances0));
      assertNotSame(instances1, instances0);
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      instances0.deleteStringAttributes();
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.equals((Object)instances1));
      assertNotSame(instances0, instances1);
      
      Instances instances2 = testInstances0.generate();
      assertNotNull(instances2);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, instances2.numInstances());
      assertEquals(1, instances2.classIndex());
      assertEquals(2, instances2.numAttributes());
      assertEquals(2, instances2.numClasses());
      assertEquals("Testdata", instances2.relationName());
      assertEquals(20.0, instances2.sumOfWeights(), 0.01);
      assertFalse(instances2.checkForStringAttributes());
      assertEquals(20, instances2.size());
      assertFalse(instances2.equals((Object)instances0));
      assertFalse(instances2.equals((Object)instances1));
      assertNotSame(instances2, instances0);
      assertNotSame(instances2, instances1);
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      
      CostMatrix costMatrix1 = costSensitiveClassifier2.getCostMatrix();
      assertNotNull(costMatrix1);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals(1, costMatrix1.numColumns());
      assertEquals(1, costMatrix1.size());
      assertEquals(1, costMatrix1.numRows());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costMatrix1.equals((Object)costMatrix0));
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier1);
      assertNotSame(costMatrix1, costMatrix0);
      
      Evaluation evaluation1 = new Evaluation(instances0, costMatrix0);
      assertNotNull(evaluation1);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, costMatrix0.numColumns());
      assertEquals(2, costMatrix0.size());
      assertEquals(2, costMatrix0.numRows());
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertFalse(instances0.equals((Object)instances1));
      assertFalse(instances0.equals((Object)instances2));
      assertFalse(costMatrix0.equals((Object)costMatrix1));
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      evaluation1.m_SumPriorAbsErr = (double) 2;
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, costMatrix0.numColumns());
      assertEquals(2, costMatrix0.size());
      assertEquals(2, costMatrix0.numRows());
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.POSITIVE_INFINITY, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      
      Enumeration enumeration0 = costSensitiveClassifier2.listOptions();
      assertNotNull(enumeration0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier2, costSensitiveClassifier1);
      
      double double1 = evaluation1.SFEntropyGain();
      assertEquals(0.0, double1, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, costMatrix0.numColumns());
      assertEquals(2, costMatrix0.size());
      assertEquals(2, costMatrix0.numRows());
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(Double.POSITIVE_INFINITY, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertFalse(instances0.equals((Object)instances1));
      assertFalse(instances0.equals((Object)instances2));
      assertFalse(costMatrix0.equals((Object)costMatrix1));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(instances0, instances1);
      assertNotSame(instances0, instances2);
      assertNotSame(costMatrix0, costMatrix1);
      assertNotSame(evaluation1, evaluation0);
      
      double double2 = evaluation0.precision(1);
      assertEquals(0.0, double2, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertFalse(instances0.equals((Object)instances1));
      assertFalse(instances0.equals((Object)instances2));
      assertEquals(double2, double1, 0.01);
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotSame(instances0, instances1);
      assertNotSame(instances0, instances2);
      assertNotSame(evaluation0, evaluation1);
      
      double double3 = evaluation0.falseNegativeRate(1);
      assertEquals(0.0, double3, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertFalse(instances0.equals((Object)instances1));
      assertFalse(instances0.equals((Object)instances2));
      assertEquals(double3, double2, 0.01);
      assertEquals(double3, double1, 0.01);
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotSame(instances0, instances1);
      assertNotSame(instances0, instances2);
      assertNotSame(evaluation0, evaluation1);
      
      try { 
        evaluation1.updatePriors((Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 49
  /*Coverage entropy=3.134160559315724
  */
  @Test(timeout = 4000)
  public void test049()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      
      Evaluation evaluation1 = new Evaluation(instances0);
      assertNotNull(evaluation1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      Evaluation evaluation2 = new Evaluation(instances0);
      assertNotNull(evaluation2);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertTrue(evaluation2.equals((Object)evaluation0));
      
      double double0 = evaluation2.SFMeanPriorEntropy();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertNotSame(evaluation2, evaluation0);
      assertNotSame(evaluation2, evaluation1);
      
      double double1 = evaluation2.incorrect();
      assertEquals(0.0, double1, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertNotEquals(double1, double0, 0.01);
      assertNotSame(evaluation2, evaluation0);
      assertNotSame(evaluation2, evaluation1);
      
      testInstances0.setNumNominal(1432);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1432, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      
      instances0.deleteStringAttributes();
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1432, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      assertNotNull(gridBagLayout0);
      
      GridBagLayout gridBagLayout1 = new GridBagLayout();
      assertNotNull(gridBagLayout1);
      assertFalse(gridBagLayout1.equals((Object)gridBagLayout0));
      
      GridBagLayout gridBagLayout2 = new GridBagLayout();
      assertNotNull(gridBagLayout2);
      assertFalse(gridBagLayout2.equals((Object)gridBagLayout0));
      assertFalse(gridBagLayout2.equals((Object)gridBagLayout1));
      
      Point point0 = gridBagLayout0.getLayoutOrigin();
      assertNotNull(point0);
      assertEquals(0, point0.y);
      assertEquals(0, point0.x);
      assertEquals(0.0, point0.getY(), 0.01);
      assertEquals(0.0, point0.getX(), 0.01);
      assertFalse(gridBagLayout0.equals((Object)gridBagLayout2));
      assertFalse(gridBagLayout0.equals((Object)gridBagLayout1));
      assertNotSame(gridBagLayout0, gridBagLayout2);
      assertNotSame(gridBagLayout0, gridBagLayout1);
      
      GridBagLayout gridBagLayout3 = new GridBagLayout();
      assertNotNull(gridBagLayout3);
      assertFalse(gridBagLayout3.equals((Object)gridBagLayout1));
      assertFalse(gridBagLayout3.equals((Object)gridBagLayout0));
      assertFalse(gridBagLayout3.equals((Object)gridBagLayout2));
      
      double double2 = evaluation0.falsePositiveRate(0);
      assertEquals(0.0, double2, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1432, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertTrue(evaluation0.equals((Object)evaluation2));
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotEquals(double2, double0, 0.01);
      assertEquals(double2, double1, 0.01);
      assertNotSame(evaluation0, evaluation1);
      assertNotSame(evaluation0, evaluation2);
      
      double double3 = evaluation2.weightedFalseNegativeRate();
      assertEquals(Double.NaN, double3, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1432, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertEquals(double3, double0, 0.01);
      assertNotEquals(double3, double1, 0.01);
      assertNotEquals(double3, double2, 0.01);
      assertNotSame(evaluation2, evaluation0);
      assertNotSame(evaluation2, evaluation1);
      
      double double4 = evaluation1.SFMeanSchemeEntropy();
      assertEquals(Double.NaN, double4, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1432, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertNotEquals(double4, double2, 0.01);
      assertNotEquals(double4, double1, 0.01);
      assertEquals(double4, double0, 0.01);
      assertEquals(double4, double3, 0.01);
      assertTrue(evaluation1.equals((Object)evaluation2));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertNotSame(evaluation1, evaluation0);
      assertNotSame(evaluation1, evaluation2);
      
      double double5 = evaluation0.weightedMatthewsCorrelation();
      assertEquals(Double.NaN, double5, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1432, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertNotEquals(double5, double1, 0.01);
      assertEquals(double5, double4, 0.01);
      assertEquals(double5, double0, 0.01);
      assertEquals(double5, double3, 0.01);
      assertNotEquals(double5, double2, 0.01);
      assertTrue(evaluation0.equals((Object)evaluation2));
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotSame(evaluation0, evaluation1);
      assertNotSame(evaluation0, evaluation2);
      
      double double6 = evaluation2.weightedPrecision();
      assertEquals(Double.NaN, double6, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1432, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(double6, double4, 0.01);
      assertNotEquals(double6, double2, 0.01);
      assertNotEquals(double6, double1, 0.01);
      assertEquals(double6, double0, 0.01);
      assertEquals(double6, double5, 0.01);
      assertEquals(double6, double3, 0.01);
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertNotSame(evaluation2, evaluation0);
      assertNotSame(evaluation2, evaluation1);
      
      double double7 = evaluation2.falseNegativeRate(103);
      assertEquals(0.0, double7, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1433, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1432, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(double7, double1, 0.01);
      assertNotEquals(double7, double3, 0.01);
      assertNotEquals(double7, double5, 0.01);
      assertNotEquals(double7, double6, 0.01);
      assertNotEquals(double7, double4, 0.01);
      assertEquals(double7, double2, 0.01);
      assertNotEquals(double7, double0, 0.01);
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertNotSame(evaluation2, evaluation0);
      assertNotSame(evaluation2, evaluation1);
  }

  /**
  //Test case number: 50
  /*Coverage entropy=2.4775431275048834
  */
  @Test(timeout = 4000)
  public void test050()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertNotNull(instances0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      
      double double0 = evaluation0.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double0, 0.01);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      
      DenseInstance denseInstance0 = new DenseInstance(11);
      assertNotNull(denseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(11, denseInstance0.numValues());
      assertEquals(11, denseInstance0.numAttributes());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      
      double double1 = evaluation0.weightedTruePositiveRate();
      assertEquals(Double.NaN, double1, 0.01);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(double1, double0, 0.01);
      
      double double2 = evaluation0.sizeOfPredictedRegions();
      assertEquals(Double.NaN, double2, 0.01);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(double2, double0, 0.01);
      assertEquals(double2, double1, 0.01);
      
      boolean boolean0 = evaluation0.getDiscardPredictions();
      assertFalse(boolean0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      
      double double3 = evaluation0.priorEntropy();
      assertEquals(0.0, double3, 0.01);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertNotEquals(double3, double1, 0.01);
      assertNotEquals(double3, double2, 0.01);
      assertNotEquals(double3, double0, 0.01);
  }

  /**
  //Test case number: 51
  /*Coverage entropy=2.578435553006793
  */
  @Test(timeout = 4000)
  public void test051()  throws Throwable  {
      Stacking stacking0 = new Stacking();
      assertNotNull(stacking0);
      assertFalse(stacking0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertEquals(10, stacking0.getNumFolds());
      assertEquals(1, stacking0.getSeed());
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      assertNotNull(adaBoostM1_0);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      
      Capabilities capabilities0 = adaBoostM1_0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      
      instances0.deleteStringAttributes();
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      
      double double0 = evaluation0.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      
      double double1 = evaluation0.m_WithClass;
      assertEquals(0.0, double1, 0.01);
      assertNotEquals(double1, double0, 0.01);
      
      double double2 = evaluation0.weightedTrueNegativeRate();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(double2, double0, 0.01);
      assertNotEquals(double2, double1, 0.01);
      
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      assertNotNull(findWithCapabilities0);
      assertEquals("", findWithCapabilities0.getFilename());
      
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      assertNotNull(findWithCapabilities1);
      assertEquals("", findWithCapabilities1.getFilename());
      assertFalse(findWithCapabilities1.equals((Object)findWithCapabilities0));
      
      Capabilities capabilities1 = findWithCapabilities0.getNotCapabilities();
      assertNotNull(capabilities1);
      assertEquals("", findWithCapabilities0.getFilename());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertFalse(findWithCapabilities0.equals((Object)findWithCapabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(findWithCapabilities0, findWithCapabilities1);
      assertNotSame(capabilities1, capabilities0);
      
      FilteredClusterer filteredClusterer0 = new FilteredClusterer();
      assertNotNull(filteredClusterer0);
      assertEquals("Class for running an arbitrary clusterer on data that has been passed through an arbitrary filter. Like the clusterer, the structure of the filter is based exclusively on the training data and test instances will be processed by the filter without changing their structure.", filteredClusterer0.globalInfo());
      assertEquals("The base clusterer to be used.", filteredClusterer0.clustererTipText());
      assertEquals("The filter to be used.", filteredClusterer0.filterTipText());
      
      TestInstances testInstances1 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(0, testInstances1.getNumString());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(4, testInstances1.getNumClasses());
      assertEquals(1, testInstances1.getNumNumeric());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances1, testInstances0);
      
      testInstances1.setNumRelationalString((-1));
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(0, testInstances1.getNumString());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals((-1), testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(4, testInstances1.getNumClasses());
      assertEquals(1, testInstances1.getNumNumeric());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances1, testInstances0);
      
      testInstances1.setNumNominalValues(531);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(0, testInstances1.getNumString());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals(531, testInstances1.getNumNominalValues());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals((-1), testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(4, testInstances1.getNumClasses());
      assertEquals(1, testInstances1.getNumNumeric());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances1, testInstances0);
      
      TestInstances testInstances2 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances2);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(20, testInstances2.getNumInstances());
      assertFalse(testInstances2.getNoClass());
      assertEquals(2, testInstances2.getNumRelationalNominalValues());
      assertEquals(10, testInstances2.getNumInstancesRelational());
      assertEquals(1, testInstances2.getNumDate());
      assertEquals(2, testInstances2.getNumNominalValues());
      assertEquals(1, testInstances2.getNumRelationalNumeric());
      assertEquals(0, testInstances2.getNumRelationalString());
      assertEquals(1, testInstances2.getSeed());
      assertEquals(1, testInstances2.getNumRelationalNominal());
      assertFalse(testInstances2.getMultiInstance());
      assertEquals(0, testInstances2.getNumString());
      assertEquals(1, testInstances2.getClassType());
      assertEquals(1, testInstances2.getNumNominal());
      assertEquals(1, testInstances2.getNumNumeric());
      assertEquals(" ", testInstances2.getWordSeparators());
      assertEquals("Testdata", testInstances2.getRelation());
      assertEquals(1, testInstances2.getNumRelationalDate());
      assertEquals(0, testInstances2.getNumRelational());
      assertEquals(4, testInstances2.getNumAttributes());
      assertEquals(4, testInstances2.getNumClasses());
      assertEquals((-1), testInstances2.getClassIndex());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertFalse(testInstances2.equals((Object)testInstances1));
      assertFalse(testInstances2.equals((Object)testInstances0));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertNotSame(testInstances2, testInstances1);
      assertNotSame(testInstances2, testInstances0);
      assertNotSame(capabilities0, capabilities1);
      
      TestInstances testInstances3 = new TestInstances();
      assertNotNull(testInstances3);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances3.getNumNominal());
      assertEquals((-1), testInstances3.getClassIndex());
      assertEquals(1, testInstances3.getNumRelationalNominal());
      assertEquals(0, testInstances3.getNumRelationalString());
      assertEquals(" ", testInstances3.getWordSeparators());
      assertEquals(0, testInstances3.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances3.getRelation());
      assertEquals(1, testInstances3.getClassType());
      assertEquals(2, testInstances3.getNumNominalValues());
      assertEquals(10, testInstances3.getNumInstancesRelational());
      assertFalse(testInstances3.getMultiInstance());
      assertEquals(0, testInstances3.getNumDate());
      assertEquals(1, testInstances3.getSeed());
      assertEquals(0, testInstances3.getNumRelationalDate());
      assertEquals(0, testInstances3.getNumNumeric());
      assertEquals(2, testInstances3.getNumAttributes());
      assertEquals(2, testInstances3.getNumClasses());
      assertEquals(20, testInstances3.getNumInstances());
      assertEquals(0, testInstances3.getNumRelational());
      assertEquals(0, testInstances3.getNumString());
      assertEquals(2, testInstances3.getNumRelationalNominalValues());
      assertFalse(testInstances3.getNoClass());
      assertFalse(testInstances3.equals((Object)testInstances1));
      assertFalse(testInstances3.equals((Object)testInstances0));
      assertFalse(testInstances3.equals((Object)testInstances2));
      
      String[] stringArray0 = testInstances1.getOptions();
      assertNotNull(stringArray0);
      assertEquals(40, stringArray0.length);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(0, testInstances1.getNumString());
      assertEquals(4, testInstances1.getNumAttributes());
      assertEquals(531, testInstances1.getNumNominalValues());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals((-1), testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(4, testInstances1.getNumClasses());
      assertEquals(1, testInstances1.getNumNumeric());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances1.equals((Object)testInstances3));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertFalse(testInstances1.equals((Object)testInstances2));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances1, testInstances3);
      assertNotSame(testInstances1, testInstances0);
      assertNotSame(testInstances1, testInstances2);
      
      Evaluation evaluation1 = new Evaluation(instances0);
      assertNotNull(evaluation1);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances0.equals((Object)testInstances3));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertFalse(testInstances0.equals((Object)testInstances2));
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      String string0 = evaluation0.toCumulativeMarginDistributionString();
      assertEquals(" -1       0    \n", string0);
      assertNotNull(string0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances0.equals((Object)testInstances3));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertFalse(testInstances0.equals((Object)testInstances2));
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances0, testInstances3);
      assertNotSame(testInstances0, testInstances1);
      assertNotSame(testInstances0, testInstances2);
      assertNotSame(evaluation0, evaluation1);
      
      CoverTree coverTree0 = new CoverTree();
      assertNotNull(coverTree0);
      assertEquals(1.3, coverTree0.getBase(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01);
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01);
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertFalse(coverTree0.getMeasurePerformance());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      
      // Undeclared exception!
      try { 
        evaluation1.fMeasure(41);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 41
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 52
  /*Coverage entropy=2.9297369955237142
  */
  @Test(timeout = 4000)
  public void test052()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      
      Evaluation evaluation1 = new Evaluation(instances0);
      assertNotNull(evaluation1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertTrue(evaluation1.equals((Object)evaluation0));
      
      Evaluation evaluation2 = new Evaluation(instances0);
      assertNotNull(evaluation2);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      
      double double0 = evaluation2.SFMeanPriorEntropy();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertNotSame(evaluation2, evaluation1);
      assertNotSame(evaluation2, evaluation0);
      
      double double1 = evaluation2.incorrect();
      assertEquals(0.0, double1, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertNotEquals(double1, double0, 0.01);
      assertNotSame(evaluation2, evaluation1);
      assertNotSame(evaluation2, evaluation0);
      
      testInstances0.setNumNominal((-1134));
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1134), testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1133), testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      assertNotNull(gridBagLayout0);
      
      GridBagLayout gridBagLayout1 = new GridBagLayout();
      assertNotNull(gridBagLayout1);
      assertFalse(gridBagLayout1.equals((Object)gridBagLayout0));
      
      GridBagLayout gridBagLayout2 = new GridBagLayout();
      assertNotNull(gridBagLayout2);
      assertFalse(gridBagLayout2.equals((Object)gridBagLayout0));
      assertFalse(gridBagLayout2.equals((Object)gridBagLayout1));
      
      Point point0 = gridBagLayout2.getLayoutOrigin();
      assertNotNull(point0);
      assertEquals(0, point0.x);
      assertEquals(0, point0.y);
      assertEquals(0.0, point0.getY(), 0.01);
      assertEquals(0.0, point0.getX(), 0.01);
      assertFalse(gridBagLayout2.equals((Object)gridBagLayout0));
      assertFalse(gridBagLayout2.equals((Object)gridBagLayout1));
      assertNotSame(gridBagLayout2, gridBagLayout0);
      assertNotSame(gridBagLayout2, gridBagLayout1);
      
      GridBagLayout gridBagLayout3 = new GridBagLayout();
      assertNotNull(gridBagLayout3);
      assertFalse(gridBagLayout3.equals((Object)gridBagLayout2));
      assertFalse(gridBagLayout3.equals((Object)gridBagLayout1));
      assertFalse(gridBagLayout3.equals((Object)gridBagLayout0));
      
      evaluation2.m_SumKBInfo = 1.8398085630472226E-4;
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1134), testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1133), testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      
      double double2 = evaluation1.falsePositiveRate(120);
      assertEquals(0.0, double2, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1134), testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1133), testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertTrue(evaluation1.equals((Object)evaluation2));
      assertNotEquals(double2, double0, 0.01);
      assertEquals(double2, double1, 0.01);
      assertNotSame(evaluation1, evaluation2);
      assertNotSame(evaluation1, evaluation0);
      
      double double3 = evaluation0.weightedFalseNegativeRate();
      assertEquals(Double.NaN, double3, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1134), testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1133), testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertTrue(evaluation0.equals((Object)evaluation2));
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotEquals(double3, double2, 0.01);
      assertEquals(double3, double0, 0.01);
      assertNotEquals(double3, double1, 0.01);
      assertNotSame(evaluation0, evaluation2);
      assertNotSame(evaluation0, evaluation1);
      
      double double4 = evaluation1.SFMeanSchemeEntropy();
      assertEquals(Double.NaN, double4, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1134), testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1133), testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertNotEquals(double4, double2, 0.01);
      assertEquals(double4, double0, 0.01);
      assertNotEquals(double4, double1, 0.01);
      assertEquals(double4, double3, 0.01);
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertTrue(evaluation1.equals((Object)evaluation2));
      assertNotSame(evaluation1, evaluation2);
      assertNotSame(evaluation1, evaluation0);
      
      double double5 = evaluation0.weightedPrecision();
      assertEquals(Double.NaN, double5, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1134), testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1133), testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(double5, double3, 0.01);
      assertEquals(double5, double0, 0.01);
      assertNotEquals(double5, double1, 0.01);
      assertNotEquals(double5, double2, 0.01);
      assertEquals(double5, double4, 0.01);
      assertTrue(evaluation0.equals((Object)evaluation2));
      assertTrue(evaluation0.equals((Object)evaluation1));
      assertNotSame(evaluation0, evaluation2);
      assertNotSame(evaluation0, evaluation1);
      
      double double6 = evaluation1.falseNegativeRate((-1));
      assertEquals(0.0, double6, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1134), testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1133), testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertEquals(double6, double2, 0.01);
      assertEquals(double6, double1, 0.01);
      assertNotEquals(double6, double4, 0.01);
      assertNotEquals(double6, double0, 0.01);
      assertNotEquals(double6, double3, 0.01);
      assertNotEquals(double6, double5, 0.01);
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertTrue(evaluation1.equals((Object)evaluation2));
      assertNotSame(evaluation1, evaluation2);
      assertNotSame(evaluation1, evaluation0);
      
      Instances instances1 = evaluation2.getHeader();
      assertNotNull(instances1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1134), testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1133), testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(0, instances1.numInstances());
      assertEquals(0.0, instances1.sumOfWeights(), 0.01);
      assertEquals(0, instances1.size());
      assertEquals(2, instances1.numAttributes());
      assertEquals(1, instances1.classIndex());
      assertEquals(2, instances1.numClasses());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertFalse(instances1.equals((Object)instances0));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertNotSame(instances0, instances1);
      assertNotSame(instances1, instances0);
      assertNotSame(evaluation2, evaluation1);
      assertNotSame(evaluation2, evaluation0);
      
      String string0 = evaluation1.toCumulativeMarginDistributionString();
      assertEquals(" -1       0    \n", string0);
      assertNotNull(string0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1134), testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1133), testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertFalse(instances0.equals((Object)instances1));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertTrue(evaluation1.equals((Object)evaluation2));
      assertNotSame(instances0, instances1);
      assertNotSame(evaluation1, evaluation2);
      assertNotSame(evaluation1, evaluation0);
      
      double double7 = evaluation2.pctCorrect();
      assertEquals(Double.NaN, double7, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1134), testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1133), testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(Double.NaN, evaluation2.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation2.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.correct(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTrueNegativeRate(), 0.01);
      assertEquals(1.0, evaluation2.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation2.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation2.numInstances(), 0.01);
      assertEquals(0.0, evaluation2.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation2.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation2.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation2.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation2.relativeAbsoluteError(), 0.01);
      assertFalse(evaluation2.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation2.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation2.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation2.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation2.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation2.weightedFalseNegativeRate(), 0.01);
      assertFalse(instances0.equals((Object)instances1));
      assertTrue(evaluation2.equals((Object)evaluation0));
      assertTrue(evaluation2.equals((Object)evaluation1));
      assertEquals(double7, double4, 0.01);
      assertNotEquals(double7, double2, 0.01);
      assertNotEquals(double7, double1, 0.01);
      assertEquals(double7, double3, 0.01);
      assertNotEquals(double7, double6, 0.01);
      assertEquals(double7, double0, 0.01);
      assertEquals(double7, double5, 0.01);
      assertNotSame(instances0, instances1);
      assertNotSame(evaluation2, evaluation1);
      assertNotSame(evaluation2, evaluation0);
      
      double double8 = evaluation1.coverageOfTestCasesByPredictedRegions();
      assertEquals(Double.NaN, double8, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1134), testInstances0.getNumNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1133), testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(0.0, evaluation1.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation1.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation1.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation1.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.incorrect(), 0.01);
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation1.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation1.totalCost(), 0.01);
      assertEquals(0.0, evaluation1.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation1.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation1.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation1.unweightedMicroFmeasure(), 0.01);
      assertFalse(evaluation1.getDiscardPredictions());
      assertEquals(0.0, evaluation1.correct(), 0.01);
      assertEquals(Double.NaN, evaluation1.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation1.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation1.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation1.rootRelativeSquaredError(), 0.01);
      assertFalse(instances0.equals((Object)instances1));
      assertTrue(evaluation1.equals((Object)evaluation0));
      assertTrue(evaluation1.equals((Object)evaluation2));
      assertEquals(double8, double5, 0.01);
      assertNotEquals(double8, double2, 0.01);
      assertEquals(double8, double7, 0.01);
      assertEquals(double8, double0, 0.01);
      assertEquals(double8, double4, 0.01);
      assertEquals(double8, double3, 0.01);
      assertNotEquals(double8, double1, 0.01);
      assertNotEquals(double8, double6, 0.01);
      assertNotSame(instances0, instances1);
      assertNotSame(evaluation1, evaluation2);
      assertNotSame(evaluation1, evaluation0);
  }

  /**
  //Test case number: 53
  /*Coverage entropy=2.290863053756535
  */
  @Test(timeout = 4000)
  public void test053()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier3);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      
      costSensitiveClassifier3.setDebug(false);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier2);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier3, costSensitiveClassifier0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      
      String string0 = evaluation0.toMatrixString();
      assertEquals("=== Confusion Matrix ===\n\n a b c d   <-- classified as\n 0 0 0 0 | a = class1\n 0 0 0 0 | b = class2\n 0 0 0 0 | c = class3\n 0 0 0 0 | d = class4\n", string0);
      assertNotNull(string0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      
      double double0 = evaluation0.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      
      double double1 = evaluation0.truePositiveRate(2);
      assertEquals(0.0, double1, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertNotEquals(double1, double0, 0.01);
      
      double double2 = evaluation0.weightedAreaUnderPRC();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertTrue(lMT0.getFastRegression());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getUseAIC());
      assertFalse(lMT0.getDebug());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals(15, lMT0.getMinNumInstances());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(double2, double0, 0.01);
      assertNotEquals(double2, double1, 0.01);
      
      // Undeclared exception!
      try { 
        evaluation0.truePositiveRate(122);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 122
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 54
  /*Coverage entropy=2.6047122738623925
  */
  @Test(timeout = 4000)
  public void test054()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      LMT lMT0 = new LMT();
      assertNotNull(lMT0);
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      
      Capabilities capabilities0 = lMT0.getCapabilities();
      assertNotNull(capabilities0);
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier1);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(1, costSensitiveClassifier1.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier1.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier1.globalInfo());
      assertFalse(costSensitiveClassifier1.getDebug());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier1.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier1.classifierTipText());
      assertFalse(costSensitiveClassifier1.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier1.minimizeExpectedCostTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier1.seedTipText());
      assertEquals(0, costSensitiveClassifier1.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier1.costMatrixTipText());
      assertFalse(costSensitiveClassifier1.equals((Object)costSensitiveClassifier0));
      
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier2);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier2.getMinimizeExpectedCost());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier2.costMatrixTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier2.minimizeExpectedCostTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier2.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier2.seedTipText());
      assertEquals(1, costSensitiveClassifier2.getSeed());
      assertEquals(0, costSensitiveClassifier2.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier2.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier2.classifierTipText());
      assertFalse(costSensitiveClassifier2.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier2.globalInfo());
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier2.equals((Object)costSensitiveClassifier0));
      
      MultiClassClassifierUpdateable multiClassClassifierUpdateable0 = new MultiClassClassifierUpdateable();
      assertNotNull(multiClassClassifierUpdateable0);
      assertEquals(2, MultiClassClassifier.METHOD_ERROR_EXHAUSTIVE);
      assertEquals(0, MultiClassClassifier.METHOD_1_AGAINST_ALL);
      assertEquals(1, MultiClassClassifier.METHOD_ERROR_RANDOM);
      assertEquals(3, MultiClassClassifier.METHOD_1_AGAINST_1);
      assertEquals("Sets the method to use for transforming the multi-class problem into several 2-class ones.", multiClassClassifierUpdateable0.methodTipText());
      assertEquals("The base classifier to be used.", multiClassClassifierUpdateable0.classifierTipText());
      assertEquals(2.0, multiClassClassifierUpdateable0.getRandomWidthFactor(), 0.01);
      assertEquals("Sets the width multiplier when using random codes. The number of codes generated will be thus number multiplied by the number of classes.", multiClassClassifierUpdateable0.randomWidthFactorTipText());
      assertFalse(multiClassClassifierUpdateable0.getUsePairwiseCoupling());
      assertEquals(1, multiClassClassifierUpdateable0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", multiClassClassifierUpdateable0.debugTipText());
      assertEquals("A metaclassifier for handling multi-class datasets with 2-class classifiers. This classifier is also capable of applying error correcting output codes for increased accuracy. The base classifier must be an updateable classifier", multiClassClassifierUpdateable0.globalInfo());
      assertEquals("Use pairwise coupling (only has an effect for 1-against-1).", multiClassClassifierUpdateable0.usePairwiseCouplingTipText());
      assertEquals("The random number seed to be used.", multiClassClassifierUpdateable0.seedTipText());
      assertFalse(multiClassClassifierUpdateable0.getDebug());
      
      Capabilities capabilities1 = costSensitiveClassifier0.getCapabilities();
      assertNotNull(capabilities1);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertTrue(capabilities1.hasDependencies());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertFalse(costSensitiveClassifier0.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier0.equals((Object)costSensitiveClassifier1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(costSensitiveClassifier0, costSensitiveClassifier2);
      assertNotSame(costSensitiveClassifier0, costSensitiveClassifier1);
      assertNotSame(capabilities1, capabilities0);
      
      Instances instances1 = testInstances0.generate();
      assertNotNull(instances1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(20, instances1.size());
      assertEquals(3, instances1.classIndex());
      assertEquals(20, instances1.numInstances());
      assertEquals(4, instances1.numAttributes());
      assertEquals(20.0, instances1.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances1.relationName());
      assertEquals(4, instances1.numClasses());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(instances1.equals((Object)instances0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(instances1, instances0);
      
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier3);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier3.costMatrixTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier3.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier3.classifierTipText());
      assertFalse(costSensitiveClassifier3.getMinimizeExpectedCost());
      assertEquals(0, costSensitiveClassifier3.graphType());
      assertFalse(costSensitiveClassifier3.getDebug());
      assertEquals(1, costSensitiveClassifier3.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier3.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier3.globalInfo());
      assertEquals("The random number seed to be used.", costSensitiveClassifier3.seedTipText());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier3.minimizeExpectedCostTipText());
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier3.equals((Object)costSensitiveClassifier2));
      
      CostSensitiveClassifier costSensitiveClassifier4 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier4);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      
      costSensitiveClassifier4.setDebug(false);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertEquals(1, costSensitiveClassifier4.getSeed());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier3);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier2);
      
      CostSensitiveClassifier costSensitiveClassifier5 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier5);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertFalse(costSensitiveClassifier5.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier5.minimizeExpectedCostTipText());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier5.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier5.classifierTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier5.costMatrixTipText());
      assertFalse(costSensitiveClassifier5.getDebug());
      assertEquals(1, costSensitiveClassifier5.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier5.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier5.globalInfo());
      assertEquals(0, costSensitiveClassifier5.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier5.seedTipText());
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier4));
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier5.equals((Object)costSensitiveClassifier3));
      
      CostSensitiveClassifier costSensitiveClassifier6 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier6);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier6.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier6.costMatrixTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier6.debugTipText());
      assertFalse(costSensitiveClassifier6.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier6.getSeed());
      assertEquals("The random number seed to be used.", costSensitiveClassifier6.seedTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier6.globalInfo());
      assertEquals(0, costSensitiveClassifier6.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier6.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier6.classifierTipText());
      assertFalse(costSensitiveClassifier6.getDebug());
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier5));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier2));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier6.equals((Object)costSensitiveClassifier4));
      
      costSensitiveClassifier4.setSeed(0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertEquals(0, costSensitiveClassifier4.getSeed());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier5));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier6));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier3);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier5);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier6);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier2);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(instances0.equals((Object)instances1));
      
      double double0 = evaluation0.weightedTrueNegativeRate();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(instances0.equals((Object)instances1));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(instances0, instances1);
      
      SelectedTag selectedTag0 = costSensitiveClassifier4.getCostMatrixSource();
      assertNotNull(selectedTag0);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals("1", selectedTag0.toString());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier4.debugTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier4.globalInfo());
      assertEquals(0, costSensitiveClassifier4.getSeed());
      assertEquals(0, costSensitiveClassifier4.graphType());
      assertEquals("The random number seed to be used.", costSensitiveClassifier4.seedTipText());
      assertFalse(costSensitiveClassifier4.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier4.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier4.costMatrixTipText());
      assertFalse(costSensitiveClassifier4.getDebug());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier4.onDemandDirectoryTipText());
      assertEquals("The base classifier to be used.", costSensitiveClassifier4.classifierTipText());
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier3));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier1));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier0));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier5));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier6));
      assertFalse(costSensitiveClassifier4.equals((Object)costSensitiveClassifier2));
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier3);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier1);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier0);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier5);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier6);
      assertNotSame(costSensitiveClassifier4, costSensitiveClassifier2);
      
      DenseInstance denseInstance0 = new DenseInstance(2);
      assertNotNull(denseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(2, denseInstance0.numValues());
      assertEquals(2, denseInstance0.numAttributes());
      
      double double1 = evaluation0.unweightedMicroFmeasure();
      assertEquals(Double.NaN, double1, 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertFalse(lMT0.getDebug());
      assertEquals("Convert all nominal attributes to binary ones before building the tree. This means that all splits in the final tree will be binary.", lMT0.convertNominalTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lMT0.debugTipText());
      assertFalse(lMT0.getUseAIC());
      assertEquals(15, lMT0.getMinNumInstances());
      assertTrue(lMT0.getFastRegression());
      assertEquals("The AIC is used to determine when to stop LogitBoost iterations. The default is not to use AIC.", lMT0.useAICTipText());
      assertFalse(lMT0.getSplitOnResiduals());
      assertFalse(lMT0.getErrorOnProbabilities());
      assertEquals((-1), lMT0.getNumBoostingIterations());
      assertFalse(lMT0.getConvertNominal());
      assertEquals("Set a fixed number of iterations for LogitBoost. If >= 0, this sets a fixed number of LogitBoost iterations that is used everywhere in the tree. If < 0, the number is cross-validated.", lMT0.numBoostingIterationsTipText());
      assertEquals("Minimize error on probabilities instead of misclassification error when cross-validating the number of LogitBoost iterations. When set, the number of LogitBoost iterations is chosen that minimizes the root mean squared error instead of the misclassification error.", lMT0.errorOnProbabilitiesTipText());
      assertEquals("Set splitting criterion based on the residuals of LogitBoost. There are two possible splitting criteria for LMT: the default is to use the C4.5 splitting criterion that uses information gain on the class variable. The other splitting criterion tries to improve the purity in the residuals produces when fitting the logistic regression functions. The choice of the splitting criterion does not usually affect classification accuracy much, but can produce different trees.", lMT0.splitOnResidualsTipText());
      assertEquals("Set the beta value used for weight trimming in LogitBoost. Only instances carrying (1 - beta)% of the weight from previous iteration are used in the next iteration. Set to 0 for no weight trimming. The default value is 0.", lMT0.weightTrimBetaTipText());
      assertEquals(1, lMT0.graphType());
      assertEquals("Set the minimum number of instances at which a node is considered for splitting. The default value is 15.", lMT0.minNumInstancesTipText());
      assertEquals("Use heuristic that avoids cross-validating the number of Logit-Boost iterations at every node. When fitting the logistic regression functions at a node, LMT has to determine the number of LogitBoost iterations to run. Originally, this number was cross-validated at every node in the tree. To save time, this heuristic cross-validates the number only once and then uses that number at every node in the tree. Usually this does not decrease accuracy but improves runtime considerably.", lMT0.fastRegressionTipText());
      assertEquals(0.0, lMT0.getWeightTrimBeta(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(instances0.equals((Object)instances1));
      assertEquals(double1, double0, 0.01);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(instances0, instances1);
      
      int int0 = 108;
      try { 
        evaluation0.updatePriors(denseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 55
  /*Coverage entropy=1.5713732392451203
  */
  @Test(timeout = 4000)
  public void test055()  throws Throwable  {
      KStar kStar0 = new KStar();
      assertNotNull(kStar0);
      assertEquals(20, kStar0.getGlobalBlend());
      assertEquals("If set to true, classifier may output additional info to the console.", kStar0.debugTipText());
      assertEquals("Whether entropy-based blending is to be used.", kStar0.entropicAutoBlendTipText());
      assertFalse(kStar0.getEntropicAutoBlend());
      assertFalse(kStar0.getDebug());
      assertEquals("The parameter for global blending. Values are restricted to [0,100].", kStar0.globalBlendTipText());
      assertEquals("Determines how missing attribute values are treated.", kStar0.missingModeTipText());
      
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      assertNotNull(adaBoostM1_0);
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      
      Capabilities capabilities0 = adaBoostM1_0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      InfoGainAttributeEval infoGainAttributeEval0 = new InfoGainAttributeEval();
      assertNotNull(infoGainAttributeEval0);
      assertEquals("InfoGainAttributeEval :\n\nEvaluates the worth of an attribute by measuring the information gain with respect to the class.\n\nInfoGain(Class,Attribute) = H(Class) - H(Class | Attribute).\n", infoGainAttributeEval0.globalInfo());
      assertEquals("Just binarize numeric attributes instead of properly discretizing them.", infoGainAttributeEval0.binarizeNumericAttributesTipText());
      assertTrue(infoGainAttributeEval0.getMissingMerge());
      assertEquals("Distribute counts for missing values. Counts are distributed across other values in proportion to their frequency. Otherwise, missing is treated as a separate value.", infoGainAttributeEval0.missingMergeTipText());
      assertFalse(infoGainAttributeEval0.getBinarizeNumericAttributes());
      
      Capabilities capabilities1 = infoGainAttributeEval0.getCapabilities();
      assertNotNull(capabilities1);
      assertEquals("InfoGainAttributeEval :\n\nEvaluates the worth of an attribute by measuring the information gain with respect to the class.\n\nInfoGain(Class,Attribute) = H(Class) - H(Class | Attribute).\n", infoGainAttributeEval0.globalInfo());
      assertEquals("Just binarize numeric attributes instead of properly discretizing them.", infoGainAttributeEval0.binarizeNumericAttributesTipText());
      assertTrue(infoGainAttributeEval0.getMissingMerge());
      assertEquals("Distribute counts for missing values. Counts are distributed across other values in proportion to their frequency. Otherwise, missing is treated as a separate value.", infoGainAttributeEval0.missingMergeTipText());
      assertFalse(infoGainAttributeEval0.getBinarizeNumericAttributes());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities1, capabilities0);
      
      Capabilities capabilities2 = capabilities0.getAttributeCapabilities();
      assertNotNull(capabilities2);
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities2.hasDependencies());
      assertEquals(1, capabilities2.getMinimumNumberInstances());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertFalse(capabilities2.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities2, capabilities1);
      assertNotSame(capabilities2, capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities2);
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities2.hasDependencies());
      assertEquals(1, capabilities2.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertFalse(capabilities2.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities2, capabilities1);
      assertNotSame(capabilities2, capabilities0);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities2.hasDependencies());
      assertEquals(1, capabilities2.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(3, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertFalse(capabilities2.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities2, capabilities1);
      assertNotSame(capabilities2, capabilities0);
      
      Enumeration enumeration0 = infoGainAttributeEval0.listOptions();
      assertNotNull(enumeration0);
      assertEquals("InfoGainAttributeEval :\n\nEvaluates the worth of an attribute by measuring the information gain with respect to the class.\n\nInfoGain(Class,Attribute) = H(Class) - H(Class | Attribute).\n", infoGainAttributeEval0.globalInfo());
      assertEquals("Just binarize numeric attributes instead of properly discretizing them.", infoGainAttributeEval0.binarizeNumericAttributesTipText());
      assertTrue(infoGainAttributeEval0.getMissingMerge());
      assertEquals("Distribute counts for missing values. Counts are distributed across other values in proportion to their frequency. Otherwise, missing is treated as a separate value.", infoGainAttributeEval0.missingMergeTipText());
      assertFalse(infoGainAttributeEval0.getBinarizeNumericAttributes());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertNotNull(costSensitiveClassifier0);
      assertEquals(1, CostSensitiveClassifier.MATRIX_ON_DEMAND);
      assertEquals(2, CostSensitiveClassifier.MATRIX_SUPPLIED);
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      
      capabilities1.enableAll();
      assertEquals("InfoGainAttributeEval :\n\nEvaluates the worth of an attribute by measuring the information gain with respect to the class.\n\nInfoGain(Class,Attribute) = H(Class) - H(Class | Attribute).\n", infoGainAttributeEval0.globalInfo());
      assertEquals("Just binarize numeric attributes instead of properly discretizing them.", infoGainAttributeEval0.binarizeNumericAttributesTipText());
      assertTrue(infoGainAttributeEval0.getMissingMerge());
      assertEquals("Distribute counts for missing values. Counts are distributed across other values in proportion to their frequency. Otherwise, missing is treated as a separate value.", infoGainAttributeEval0.missingMergeTipText());
      assertFalse(infoGainAttributeEval0.getBinarizeNumericAttributes());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertTrue(capabilities1.hasDependencies());
      assertFalse(capabilities1.equals((Object)capabilities2));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotSame(capabilities1, capabilities2);
      assertNotSame(capabilities1, capabilities0);
      
      double[] doubleArray0 = new double[2];
      doubleArray0[0] = (double) 2;
      doubleArray0[1] = (double) (-1219);
      int[] intArray0 = new int[10];
      SparseInstance sparseInstance0 = new SparseInstance((-3255.794715316), doubleArray0, intArray0, (-1));
      assertNotNull(sparseInstance0);
      assertEquals(2, doubleArray0.length);
      assertEquals(10, intArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertArrayEquals(new double[] {2.0, (-1219.0)}, doubleArray0, 0.01);
      assertArrayEquals(new int[] {0, 0, 0, 0, 0, 0, 0, 0, 0, 0}, intArray0);
      assertEquals(10, sparseInstance0.numValues());
      assertEquals((-1), sparseInstance0.numAttributes());
      assertEquals((-3255.794715316), sparseInstance0.weight(), 0.01);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities2.hasDependencies());
      assertEquals(1, capabilities2.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(3, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertFalse(capabilities2.equals((Object)capabilities0));
      
      double double0 = evaluation0.m_SumSchemeEntropy;
      assertEquals(0.0, double0, 0.01);
      
      evaluation0.useNoPriors();
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", adaBoostM1_0.debugTipText());
      assertEquals("Weight threshold for weight pruning.", adaBoostM1_0.weightThresholdTipText());
      assertEquals("Whether resampling is used instead of reweighting.", adaBoostM1_0.useResamplingTipText());
      assertEquals("The number of iterations to be performed.", adaBoostM1_0.numIterationsTipText());
      assertFalse(adaBoostM1_0.getDebug());
      assertEquals(100, adaBoostM1_0.getWeightThreshold());
      assertEquals("The base classifier to be used.", adaBoostM1_0.classifierTipText());
      assertEquals("The random number seed to be used.", adaBoostM1_0.seedTipText());
      assertEquals(1, adaBoostM1_0.getSeed());
      assertFalse(adaBoostM1_0.getUseResampling());
      assertEquals(10, adaBoostM1_0.getNumIterations());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities2.hasDependencies());
      assertEquals(1, capabilities2.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(3, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertFalse(capabilities2.equals((Object)capabilities0));
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(capabilities2, capabilities1);
      assertNotSame(capabilities2, capabilities0);
  }

  /**
  //Test case number: 56
  /*Coverage entropy=1.9593337382266454
  */
  @Test(timeout = 4000)
  public void test056()  throws Throwable  {
      Stacking stacking0 = new Stacking();
      assertNotNull(stacking0);
      assertEquals("The base classifiers to be used.", stacking0.classifiersTipText());
      assertEquals(1, stacking0.getSeed());
      assertEquals("The number of folds used for cross-validation.", stacking0.numFoldsTipText());
      assertEquals("The random number seed to be used.", stacking0.seedTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", stacking0.numExecutionSlotsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", stacking0.debugTipText());
      assertEquals(10, stacking0.getNumFolds());
      assertEquals(1, stacking0.getNumExecutionSlots());
      assertFalse(stacking0.getDebug());
      assertEquals("The meta classifiers to be used.", stacking0.metaClassifierTipText());
      
      LinearRegression linearRegression0 = new LinearRegression();
      assertNotNull(linearRegression0);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertFalse(linearRegression0.getMinimal());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression0.globalInfo());
      assertEquals("Eliminate colinear attributes.", linearRegression0.eliminateColinearAttributesTipText());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression0.attributeSelectionMethodTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression0.ridgeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression0.debugTipText());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression0.minimalTipText());
      assertFalse(linearRegression0.getDebug());
      assertTrue(linearRegression0.getEliminateColinearAttributes());
      assertEquals(1.0E-8, linearRegression0.getRidge(), 0.01);
      
      LinearRegression linearRegression1 = new LinearRegression();
      assertNotNull(linearRegression1);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression1.debugTipText());
      assertEquals(1.0E-8, linearRegression1.getRidge(), 0.01);
      assertEquals("Eliminate colinear attributes.", linearRegression1.eliminateColinearAttributesTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression1.globalInfo());
      assertFalse(linearRegression1.getDebug());
      assertFalse(linearRegression1.getMinimal());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression1.minimalTipText());
      assertTrue(linearRegression1.getEliminateColinearAttributes());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression1.attributeSelectionMethodTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression1.ridgeTipText());
      assertFalse(linearRegression1.equals((Object)linearRegression0));
      
      SelectedTag selectedTag0 = linearRegression1.getAttributeSelectionMethod();
      assertNotNull(selectedTag0);
      assertEquals(1, LinearRegression.SELECTION_NONE);
      assertEquals(2, LinearRegression.SELECTION_GREEDY);
      assertEquals(0, LinearRegression.SELECTION_M5);
      assertEquals("If set to true, classifier may output additional info to the console.", linearRegression1.debugTipText());
      assertEquals(1.0E-8, linearRegression1.getRidge(), 0.01);
      assertEquals("Eliminate colinear attributes.", linearRegression1.eliminateColinearAttributesTipText());
      assertEquals("Class for using linear regression for prediction. Uses the Akaike criterion for model selection, and is able to deal with weighted instances.", linearRegression1.globalInfo());
      assertFalse(linearRegression1.getDebug());
      assertFalse(linearRegression1.getMinimal());
      assertEquals("If enabled, dataset header, means and stdevs get discarded to conserve memory; also, the model cannot be printed out.", linearRegression1.minimalTipText());
      assertTrue(linearRegression1.getEliminateColinearAttributes());
      assertEquals("Set the method used to select attributes for use in the linear regression. Available methods are: no attribute selection, attribute selection using M5's method (step through the attributes removing the one with the smallest standardised coefficient until no improvement is observed in the estimate of the error given by the Akaike information criterion), and a greedy selection using the Akaike information metric.", linearRegression1.attributeSelectionMethodTipText());
      assertEquals("The value of the Ridge parameter.", linearRegression1.ridgeTipText());
      assertEquals("0", selectedTag0.toString());
      assertFalse(linearRegression1.equals((Object)linearRegression0));
      assertNotSame(linearRegression1, linearRegression0);
      
      WrapperSubsetEval wrapperSubsetEval0 = new WrapperSubsetEval();
      assertNotNull(wrapperSubsetEval0);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      
      String string0 = wrapperSubsetEval0.toString();
      assertEquals("\tWrapper subset evaluator has not been built yet\n", string0);
      assertNotNull(string0);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      
      SelectedTag selectedTag1 = wrapperSubsetEval0.getEvaluationMeasure();
      assertNotNull(selectedTag1);
      assertEquals(1, WrapperSubsetEval.EVAL_DEFAULT);
      assertEquals(4, WrapperSubsetEval.EVAL_MAE);
      assertEquals(3, WrapperSubsetEval.EVAL_RMSE);
      assertEquals(6, WrapperSubsetEval.EVAL_AUC);
      assertEquals(7, WrapperSubsetEval.EVAL_AUPRC);
      assertEquals(2, WrapperSubsetEval.EVAL_ACCURACY);
      assertEquals(5, WrapperSubsetEval.EVAL_FMEASURE);
      assertEquals("Classifier to use for estimating the accuracy of subsets", wrapperSubsetEval0.classifierTipText());
      assertEquals("Number of xval folds to use when estimating subset accuracy.", wrapperSubsetEval0.foldsTipText());
      assertEquals("The measure used to evaluate the performance of attribute combinations.", wrapperSubsetEval0.evaluationMeasureTipText());
      assertEquals("Repeat xval if stdev of mean exceeds this value.", wrapperSubsetEval0.thresholdTipText());
      assertEquals(1, wrapperSubsetEval0.getSeed());
      assertEquals(5, wrapperSubsetEval0.getFolds());
      assertEquals(0.01, wrapperSubsetEval0.getThreshold(), 0.01);
      assertEquals("Seed to use for randomly generating xval splits.", wrapperSubsetEval0.seedTipText());
      assertEquals("1", selectedTag1.toString());
      assertFalse(selectedTag1.equals((Object)selectedTag0));
      assertNotSame(selectedTag1, selectedTag0);
      
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      
      MouseWheelListener mouseWheelListener0 = mock(MouseWheelListener.class, new ViolatedAssumptionAnswer());
      MouseWheelListener mouseWheelListener1 = AWTEventMulticaster.remove(mouseWheelListener0, mouseWheelListener0);
      assertNull(mouseWheelListener1);
      
      MouseWheelListener mouseWheelListener2 = AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener2);
      
      MouseWheelListener mouseWheelListener3 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener3);
      
      MouseWheelListener mouseWheelListener4 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener4);
      
      WekaTaskMonitor wekaTaskMonitor0 = new WekaTaskMonitor();
      assertNotNull(wekaTaskMonitor0);
      assertTrue(wekaTaskMonitor0.getFocusTraversalKeysEnabled());
      assertFalse(wekaTaskMonitor0.isFocusCycleRoot());
      assertFalse(wekaTaskMonitor0.isFocusTraversalPolicyProvider());
      assertFalse(wekaTaskMonitor0.getIgnoreRepaint());
      assertFalse(wekaTaskMonitor0.isFocusTraversalPolicySet());
      
      MouseWheelListener mouseWheelListener5 = AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener5);
      
      MouseWheelListener mouseWheelListener6 = AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      assertNull(mouseWheelListener6);
      
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      TextDirectoryLoader textDirectoryLoader1 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader1);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertFalse(textDirectoryLoader1.getDebug());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      
      Instances instances0 = textDirectoryLoader1.getDataSet();
      assertNotNull(instances0);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertNotSame(textDirectoryLoader1, textDirectoryLoader0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertNotNull(evaluation0);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      
      double double0 = evaluation0.correct();
      assertEquals(0.0, double0, 0.01);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader1.debugTipText());
      assertEquals("", textDirectoryLoader1.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader1.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader1.outputFilenameTipText());
      assertFalse(textDirectoryLoader1.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader1.globalInfo());
      assertEquals("Directories", textDirectoryLoader1.getFileDescription());
      assertFalse(textDirectoryLoader1.getDebug());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertFalse(textDirectoryLoader1.equals((Object)textDirectoryLoader0));
      assertNotSame(textDirectoryLoader1, textDirectoryLoader0);
      
      try { 
        evaluation0.evaluateModel((Classifier) stacking0, instances0, (Object[]) testInstances0.DEFAULT_WORDS);
        fail("Expecting exception: ClassCastException");
      
      } catch(ClassCastException e) {
         //
         // java.lang.String cannot be cast to weka.classifiers.evaluation.output.prediction.AbstractOutput
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 57
  /*Coverage entropy=2.1995219101255112
  */
  @Test(timeout = 4000)
  public void test057()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertNotNull(textDirectoryLoader0);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertNotNull(instances0);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numInstances());
      
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      instances0.sort(comparator0);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numInstances());
      
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      assertNotNull(gridBagLayout0);
      
      Point point0 = gridBagLayout0.getLayoutOrigin();
      assertNotNull(point0);
      assertEquals(0, point0.x);
      assertEquals(0, point0.y);
      assertEquals(0.0, point0.getY(), 0.01);
      assertEquals(0.0, point0.getX(), 0.01);
      
      GridBagLayout gridBagLayout1 = new GridBagLayout();
      assertNotNull(gridBagLayout1);
      assertFalse(gridBagLayout1.equals((Object)gridBagLayout0));
      
      Evaluation evaluation0 = new Evaluation(instances0, (CostMatrix) null);
      assertNotNull(evaluation0);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      
      double double0 = evaluation0.weightedFalseNegativeRate();
      assertEquals(Double.NaN, double0, 0.01);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      
      double double1 = evaluation0.SFMeanSchemeEntropy();
      assertEquals(Double.NaN, double1, 0.01);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(double1, double0, 0.01);
      
      double double2 = evaluation0.weightedMatthewsCorrelation();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(double2, double0, 0.01);
      assertEquals(double2, double1, 0.01);
      
      double double3 = evaluation0.weightedPrecision();
      assertEquals(Double.NaN, double3, 0.01);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("_home_ubuntu_termite_projects_107_weka", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01);
      assertEquals(0, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01);
      assertEquals(0.0, evaluation0.totalCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01);
      assertEquals(0.0, evaluation0.incorrect(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01);
      assertEquals(1.0, evaluation0.kappa(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01);
      assertEquals(0.0, evaluation0.numInstances(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01);
      assertEquals(0.0, evaluation0.unclassified(), 0.01);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01);
      assertEquals(double3, double1, 0.01);
      assertEquals(double3, double2, 0.01);
      assertEquals(double3, double0, 0.01);
      
      CoverTree coverTree0 = new CoverTree();
      assertNotNull(coverTree0);
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertFalse(coverTree0.getMeasurePerformance());
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertEquals(1.3, coverTree0.getBase(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01);
      
      CoverTree coverTree1 = new CoverTree();
      assertNotNull(coverTree1);
      assertFalse(coverTree1.getMeasurePerformance());
      assertEquals(0.0, coverTree1.measureTreeSize(), 0.01);
      assertEquals("The base for the expansion constant.", coverTree1.baseTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree1.distanceFunctionTipText());
      assertEquals(1.3, coverTree1.getBase(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree1.measurePerformanceTipText());
      assertEquals(0.0, coverTree1.measureNumLeaves(), 0.01);
      assertEquals(0.0, coverTree1.measureMaxDepth(), 0.01);
      assertFalse(coverTree1.equals((Object)coverTree0));
      
      try { 
        coverTree1.getDistances();
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // The tree has not been supplied with a set of instances or getDistances() has been called before calling kNearestNeighbours().
         //
         verifyException("weka.core.neighboursearch.CoverTree", e);
      }
  }

  /**
  //Test case number: 58
  /*Coverage entropy=2.011376091893468
  */
  @Test(timeout = 4000)
  public void test058()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      LMT lMT0 = new LMT();
      Capabilities capabilities0 = lMT0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      byte[] byteArray0 = new byte[4];
      byteArray0[0] = (byte) (-59);
      byteArray0[1] = (byte)93;
      byteArray0[2] = (byte)25;
      byteArray0[3] = (byte)7;
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      lMT0.setUseAIC(false);
      costSensitiveClassifier3.setDebug(false);
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      doReturn(0, 0, 0, 0, 0).when(comparator0).compare(any() , any());
      instances0.sort(comparator0);
      costSensitiveClassifier3.getCostMatrix();
      Evaluation evaluation0 = new Evaluation(instances0);
      evaluation0.setDiscardPredictions(false);
      GaussianProcesses gaussianProcesses0 = new GaussianProcesses();
      double double0 = evaluation0.numFalseNegatives(0);
      assertEquals(0.0, double0, 0.01);
      
      double double1 = evaluation0.falseNegativeRate(0);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, double1, 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
  }

  /**
  //Test case number: 59
  /*Coverage entropy=1.3265171534282083
  */
  @Test(timeout = 4000)
  public void test059()  throws Throwable  {
      Stacking stacking0 = new Stacking();
      ZeroR zeroR0 = (ZeroR)stacking0.getMetaClassifier();
      zeroR0.getOptions();
      zeroR0.setDebug(false);
      zeroR0.listOptions();
      String string0 = Evaluation.makeOptionString(zeroR0, true);
      CoverTree coverTree0 = new CoverTree();
      KDTree kDTree0 = new KDTree();
      KDTree kDTree1 = new KDTree();
      Evaluation.wekaStaticWrapper(zeroR0, string0);
      LibSVMLoader libSVMLoader0 = new LibSVMLoader();
      try { 
        libSVMLoader0.getStructure();
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // No source has been specified
         //
         verifyException("weka.core.converters.LibSVMLoader", e);
      }
  }

  /**
  //Test case number: 60
  /*Coverage entropy=2.420523209731701
  */
  @Test(timeout = 4000)
  public void test060()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      SGDText sGDText0 = new SGDText();
      sGDText0.setUseWordFrequencies(false);
      sGDText0.setSeed(4);
      sGDText0.setPeriodicPruning(4);
      AlphabeticTokenizer alphabeticTokenizer1 = new AlphabeticTokenizer();
      sGDText0.setTokenizer(alphabeticTokenizer0);
      MouseWheelListener mouseWheelListener0 = mock(MouseWheelListener.class, new ViolatedAssumptionAnswer());
      MouseWheelListener mouseWheelListener1 = AWTEventMulticaster.add(mouseWheelListener0, mouseWheelListener0);
      AWTEventMulticaster.remove(mouseWheelListener1, mouseWheelListener1);
      MouseWheelListener mouseWheelListener2 = AWTEventMulticaster.add((MouseWheelListener) null, mouseWheelListener1);
      AWTEventMulticaster.remove(mouseWheelListener1, mouseWheelListener2);
      MouseWheelListener mouseWheelListener3 = AWTEventMulticaster.remove(mouseWheelListener1, (MouseWheelListener) null);
      MouseWheelListener mouseWheelListener4 = AWTEventMulticaster.add(mouseWheelListener3, mouseWheelListener3);
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      AWTEventMulticaster.add(mouseWheelListener1, mouseWheelListener4);
      AWTEventMulticaster.remove((MouseWheelListener) null, mouseWheelListener1);
      MouseWheelListener mouseWheelListener5 = AWTEventMulticaster.remove(mouseWheelListener2, (MouseWheelListener) null);
      WekaTaskMonitor wekaTaskMonitor0 = new WekaTaskMonitor();
      AWTEventMulticaster.remove(mouseWheelListener1, (MouseWheelListener) null);
      MouseWheelListener mouseWheelListener6 = AWTEventMulticaster.add(mouseWheelListener5, mouseWheelListener5);
      wekaTaskMonitor0.addMouseWheelListener(mouseWheelListener6);
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      Evaluation evaluation0 = new Evaluation(instances0);
      SimpleLogistic simpleLogistic0 = new SimpleLogistic();
      Evaluation.getGlobalInfo(sGDText0);
      evaluation0.rootRelativeSquaredError();
      ArffLoader arffLoader0 = new ArffLoader();
      arffLoader0.getNextInstance(instances0);
      try { 
        evaluation0.evaluateModelOnce((double[]) null, (Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Utils", e);
      }
  }

  /**
  //Test case number: 61
  /*Coverage entropy=2.6948571380413573
  */
  @Test(timeout = 4000)
  public void test061()  throws Throwable  {
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      Capabilities capabilities0 = adaBoostM1_0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      instances0.deleteStringAttributes();
      FileSystemHandling.shouldAllThrowIOExceptions();
      Evaluation evaluation0 = new Evaluation(instances0);
      double double0 = evaluation0.weightedAreaUnderPRC();
      double double1 = evaluation0.m_WithClass;
      double double2 = evaluation0.weightedTrueNegativeRate();
      assertEquals(Double.NaN, double2, 0.01);
      
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      findWithCapabilities0.getNotCapabilities();
      FilteredClusterer filteredClusterer0 = new FilteredClusterer();
      TestInstances testInstances1 = TestInstances.forCapabilities(capabilities0);
      TestInstances testInstances2 = TestInstances.forCapabilities(capabilities1);
      testInstances2.setNumRelationalString((-1));
      testInstances1.setNumNominalValues(531);
      TestInstances testInstances3 = TestInstances.forCapabilities(capabilities0);
      TestInstances testInstances4 = new TestInstances();
      Instances instances1 = testInstances3.generate();
      testInstances1.getOptions();
      Evaluation evaluation1 = new Evaluation(instances1);
      evaluation1.toSummaryString(" ", true);
      evaluation1.falseNegativeRate((-1));
      double double3 = evaluation1.weightedFalseNegativeRate();
      assertEquals(0.0, evaluation1.SFEntropyGain(), 0.01);
      assertEquals(double3, double0, 0.01);
      assertEquals(Double.NaN, double3, 0.01);
  }

  /**
  //Test case number: 62
  /*Coverage entropy=2.340772828815339
  */
  @Test(timeout = 4000)
  public void test062()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      Evaluation evaluation0 = new Evaluation(instances0);
      Evaluation evaluation1 = new Evaluation(instances0);
      testInstances0.setNumRelationalDate((-2));
      evaluation1.weightedAreaUnderROC();
      evaluation0.toMatrixString(" ");
      evaluation1.toSummaryString();
      TestInstances testInstances1 = new TestInstances();
      JInternalFrame jInternalFrame0 = new JInternalFrame("</pre\n<p/>\n", false);
      JSlider jSlider0 = new JSlider();
      JSlider jSlider1 = null;
      try {
        jSlider1 = new JSlider((-1));
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // orientation must be one of: VERTICAL, HORIZONTAL
         //
         verifyException("javax.swing.JSlider", e);
      }
  }

  /**
  //Test case number: 63
  /*Coverage entropy=2.3819475579028357
  */
  @Test(timeout = 4000)
  public void test063()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      Evaluation evaluation0 = new Evaluation(instances0);
      Evaluation evaluation1 = new Evaluation(instances0);
      testInstances0.setNumRelationalDate((-1134));
      evaluation0.SFMeanPriorEntropy();
      evaluation0.incorrect();
      testInstances0.setNumNominal(1448);
      instances0.deleteStringAttributes();
      evaluation0.SFPriorEntropy();
      CostMatrix costMatrix0 = new CostMatrix(3);
      evaluation1.KBInformation();
      evaluation1.m_SumPriorEntropy = 0.9972963629416215;
      evaluation1.KBMeanInformation();
      double double0 = evaluation1.m_MaxTarget;
      KDTree kDTree0 = new KDTree();
      KDTree kDTree1 = new KDTree();
      WekaTaskMonitor wekaTaskMonitor0 = new WekaTaskMonitor();
      MouseWheelListener mouseWheelListener0 = mock(MouseWheelListener.class, new ViolatedAssumptionAnswer());
      MouseWheelListener mouseWheelListener1 = AWTEventMulticaster.add(mouseWheelListener0, mouseWheelListener0);
      AWTEventMulticaster.remove(mouseWheelListener1, mouseWheelListener1);
      AWTEventMulticaster.add((MouseWheelListener) null, mouseWheelListener1);
      JFrame jFrame0 = null;
      try {
        jFrame0 = new JFrame((String) null);
        fail("Expecting exception: HeadlessException");
      
      } catch(HeadlessException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("java.awt.GraphicsEnvironment", e);
      }
  }

  /**
  //Test case number: 64
  /*Coverage entropy=2.055194297132192
  */
  @Test(timeout = 4000)
  public void test064()  throws Throwable  {
      MultilayerPerceptron multilayerPerceptron0 = new MultilayerPerceptron();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      LinearRegression linearRegression0 = new LinearRegression();
      LinearRegression linearRegression1 = new LinearRegression();
      linearRegression1.toString();
      Enumeration enumeration0 = new NGramTokenizer();
      linearRegression1.getAttributeSelectionMethod();
      WrapperSubsetEval wrapperSubsetEval0 = new WrapperSubsetEval();
      wrapperSubsetEval0.toString();
      SelectedTag selectedTag0 = new SelectedTag(2, wrapperSubsetEval0.TAGS_EVALUATION);
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      LMT lMT0 = new LMT();
      InfoGainAttributeEval infoGainAttributeEval0 = new InfoGainAttributeEval();
      infoGainAttributeEval0.listOptions();
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = (double) 1;
      int[] intArray0 = new int[9];
      intArray0[0] = (-2);
      intArray0[1] = (-1219);
      intArray0[2] = 1;
      Evaluation evaluation0 = new Evaluation(instances0);
      double double0 = evaluation0.m_MissingClass;
      evaluation0.trueNegativeRate(5);
      evaluation0.falseNegativeRate(98819232);
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      gridBagLayout0.getLayoutOrigin();
      GridBagLayout gridBagLayout1 = new GridBagLayout();
      try { 
        Evaluation.wekaStaticWrapper((Sourcable) null, "r$Y}Nq+FSnxL }<e$'");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 65
  /*Coverage entropy=2.216613430637375
  */
  @Test(timeout = 4000)
  public void test065()  throws Throwable  {
      MultilayerPerceptron multilayerPerceptron0 = new MultilayerPerceptron();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      LinearRegression linearRegression0 = new LinearRegression();
      LinearRegression linearRegression1 = new LinearRegression();
      linearRegression1.listOptions();
      linearRegression1.getAttributeSelectionMethod();
      WrapperSubsetEval wrapperSubsetEval0 = new WrapperSubsetEval();
      wrapperSubsetEval0.toString();
      wrapperSubsetEval0.getEvaluationMeasure();
      TestInstances testInstances0 = new TestInstances();
      TestInstances testInstances1 = new TestInstances();
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      Capabilities capabilities0 = findWithCapabilities0.getCapabilities();
      TestInstances testInstances2 = TestInstances.forCapabilities(capabilities0);
      testInstances0.setOptions(testInstances1.DEFAULT_WORDS);
      testInstances2.setNumNominalValues((-2));
      Instances instances0 = testInstances0.generate();
      Evaluation evaluation0 = new Evaluation(instances0);
      testInstances0.setMultiInstance(true);
      evaluation0.meanAbsoluteError();
      Evaluation evaluation1 = new Evaluation(instances0);
      evaluation1.coverageOfTestCasesByPredictedRegions();
      evaluation0.falseNegativeRate((-1));
      evaluation0.numTruePositives(1);
      WekaTaskMonitor wekaTaskMonitor0 = new WekaTaskMonitor();
      LogPanel logPanel0 = null;
      try {
        logPanel0 = new LogPanel(wekaTaskMonitor0);
        fail("Expecting exception: HeadlessException");
      
      } catch(HeadlessException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("java.awt.GraphicsEnvironment", e);
      }
  }

  /**
  //Test case number: 66
  /*Coverage entropy=2.1341639076887824
  */
  @Test(timeout = 4000)
  public void test066()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      Instances instances0 = textDirectoryLoader0.getDataSet();
      Evaluation evaluation0 = new Evaluation(instances0);
      DenseInstance denseInstance0 = new DenseInstance(124);
      double double0 = evaluation0.numTrueNegatives((-1760));
      assertEquals(0.0, double0, 0.01);
      
      double double1 = evaluation0.numTruePositives(25);
      assertEquals(0.0, double1, 0.01);
      
      double double2 = evaluation0.errorRate();
      assertEquals(Double.NaN, double2, 0.01);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
  }

  /**
  //Test case number: 67
  /*Coverage entropy=1.6492377541392287
  */
  @Test(timeout = 4000)
  public void test067()  throws Throwable  {
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props/Capabilities.props");
      FileSystemHandling.setPermissions(evoSuiteFile0, true, true, true);
      KStar kStar0 = new KStar();
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      Capabilities capabilities0 = adaBoostM1_0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getOtherCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      capabilities0.enableAll();
      double[] doubleArray0 = new double[7];
      doubleArray0[1] = (double) (-1219);
      int[] intArray0 = new int[10];
      SparseInstance sparseInstance0 = new SparseInstance((-3229.801115930514), doubleArray0, intArray0, (-1));
      Evaluation evaluation0 = new Evaluation(instances0);
      KDTree kDTree0 = new KDTree();
      kDTree0.setMinBoxRelWidth((-956.5374047088475));
      CoverTree coverTree0 = new CoverTree();
      // Undeclared exception!
      try { 
        evaluation0.makeDistribution(2);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 2
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 68
  /*Coverage entropy=1.8466435205545344
  */
  @Test(timeout = 4000)
  public void test068()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      Evaluation evaluation0 = new Evaluation(instances0);
      Double double0 = new Double((-1935.4593093));
      Double double1 = new Double((-2.147483648E9));
      instances0.deleteStringAttributes();
      KDTree kDTree0 = new KDTree();
      WekaTaskMonitor wekaTaskMonitor0 = new WekaTaskMonitor();
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      WekaTaskMonitor wekaTaskMonitor1 = new WekaTaskMonitor();
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      TextDirectoryLoader textDirectoryLoader1 = new TextDirectoryLoader();
      FileSystemHandling.shouldAllThrowIOExceptions();
      textDirectoryLoader1.getDataSet();
      Evaluation evaluation1 = new Evaluation(instances0);
      try { 
        evaluation1.evaluateModelOnce((-2.147483648E9), (Instance) null);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // -2147483648
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 69
  /*Coverage entropy=2.3555766801816582
  */
  @Test(timeout = 4000)
  public void test069()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      Evaluation evaluation0 = new Evaluation(instances0);
      evaluation0.SFMeanPriorEntropy();
      Double double0 = new Double(0.0);
      instances0.deleteStringAttributes();
      FileSystemHandling.shouldAllThrowIOExceptions();
      CostMatrix costMatrix0 = new CostMatrix(2);
      Instances instances1 = testInstances0.generate();
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      costSensitiveClassifier0.getCostMatrix();
      instances0.deleteStringAttributes();
      Double double1 = new Double((-2));
      testInstances0.generate();
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      costSensitiveClassifier2.getCostMatrix();
      Evaluation evaluation1 = new Evaluation(instances0, costMatrix0);
      evaluation1.addNumericTrainClass(1, (-4.389760124144186));
      costSensitiveClassifier2.listOptions();
      evaluation1.SFEntropyGain();
      evaluation0.precision(1);
      evaluation1.areaUnderPRC(1);
      ArffLoader arffLoader0 = new ArffLoader();
      arffLoader0.getNextInstance(instances1);
      try { 
        evaluation0.updateStatsForPredictor(1, (Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 70
  /*Coverage entropy=1.6999267696231979
  */
  @Test(timeout = 4000)
  public void test070()  throws Throwable  {
      KStar kStar0 = new KStar();
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      Instances instances0 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader0);
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      Evaluation evaluation0 = new Evaluation(instances0);
      evaluation0.setNumericPriorsFromBuffer();
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "% Rows\tColumns\n");
      double[] doubleArray0 = new double[4];
      doubleArray0[0] = (-0.25);
      doubleArray0[1] = (-0.25);
      doubleArray0[2] = (-0.25);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(2334.0, doubleArray0);
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance0);
      assertEquals(2334.0, sparseInstance0.weight(), 0.01);
  }

  /**
  //Test case number: 71
  /*Coverage entropy=2.3551092551394888
  */
  @Test(timeout = 4000)
  public void test071()  throws Throwable  {
      MultilayerPerceptron multilayerPerceptron0 = new MultilayerPerceptron();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      LinearRegression linearRegression0 = new LinearRegression();
      LinearRegression linearRegression1 = new LinearRegression();
      linearRegression1.listOptions();
      linearRegression1.getAttributeSelectionMethod();
      WrapperSubsetEval wrapperSubsetEval0 = new WrapperSubsetEval();
      wrapperSubsetEval0.toString();
      wrapperSubsetEval0.getEvaluationMeasure();
      TestInstances testInstances0 = new TestInstances();
      TestInstances testInstances1 = new TestInstances();
      testInstances1.setNumNominalValues(5);
      Instances instances0 = testInstances0.generate(" ");
      Evaluation evaluation0 = new Evaluation(instances0);
      evaluation0.meanAbsoluteError();
      evaluation0.coverageOfTestCasesByPredictedRegions();
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = (double) (-2);
      doubleArray0[1] = (double) 6;
      doubleArray0[2] = (double) 1;
      doubleArray0[3] = Double.NaN;
      doubleArray0[4] = (-1002.3552951328);
      doubleArray0[5] = (double) 2;
      doubleArray0[6] = (-4.6011568088876);
      doubleArray0[7] = Double.NaN;
      int[] intArray0 = new int[4];
      intArray0[0] = (-1);
      intArray0[1] = 2;
      intArray0[2] = 4;
      evaluation0.falseNegativeRate(4);
      evaluation0.priorEntropy();
      // Undeclared exception!
      try { 
        evaluation0.num2ShortID(0, (char[]) null, 6);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 72
  /*Coverage entropy=3.018555975782502
  */
  @Test(timeout = 4000)
  public void test072()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      LMT lMT0 = new LMT();
      Capabilities capabilities0 = lMT0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      WrapperSubsetEval wrapperSubsetEval0 = new WrapperSubsetEval();
      wrapperSubsetEval0.getEvaluationMeasure();
      MultiClassClassifierUpdateable multiClassClassifierUpdateable0 = new MultiClassClassifierUpdateable();
      costSensitiveClassifier0.getCapabilities();
      testInstances0.generate();
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier4 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier5 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier6 = new CostSensitiveClassifier();
      Evaluation evaluation0 = new Evaluation(instances0);
      evaluation0.weightedTrueNegativeRate();
      lMT0.setUseAIC(true);
      costSensitiveClassifier4.getCostMatrixSource();
      DenseInstance denseInstance0 = new DenseInstance(2);
      evaluation0.unweightedMicroFmeasure();
      evaluation0.trueNegativeRate(108);
      evaluation0.weightedFMeasure();
      evaluation0.trueNegativeRate(2);
      char[] charArray0 = new char[0];
      // Undeclared exception!
      try { 
        evaluation0.num2ShortID(2, charArray0, 2);
        fail("Expecting exception: ArithmeticException");
      
      } catch(ArithmeticException e) {
         //
         // / by zero
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 73
  /*Coverage entropy=2.5743631318055766
  */
  @Test(timeout = 4000)
  public void test073()  throws Throwable  {
      KStar kStar0 = new KStar();
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      Instances instances0 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader0);
      textDirectoryLoader0.setRetrieval(72);
      adaBoostM1_0.getCapabilities();
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      instances0.sort(comparator0);
      Evaluation evaluation0 = new Evaluation(instances0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(153);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      binarySparseInstance1.deleteAttributeAt(2);
      instances0.add((Instance) binarySparseInstance0);
      SparseInstance sparseInstance0 = new SparseInstance((SparseInstance) binarySparseInstance0);
      FileSystemHandling.shouldAllThrowIOExceptions();
      double double0 = evaluation0.m_MissingClass;
      evaluation0.trueNegativeRate(23);
      evaluation0.falseNegativeRate((-846));
      evaluation0.recall(2);
      evaluation0.confusionMatrix();
      char[] charArray0 = new char[3];
      charArray0[0] = 'U';
      charArray0[1] = 'k';
      charArray0[2] = 'v';
      // Undeclared exception!
      try { 
        evaluation0.num2ShortID(2, charArray0, (-3522));
        fail("Expecting exception: NegativeArraySizeException");
      
      } catch(NegativeArraySizeException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 74
  /*Coverage entropy=1.6999267696231979
  */
  @Test(timeout = 4000)
  public void test074()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      Instances instances0 = textDirectoryLoader0.getDataSet();
      DenseInstance denseInstance0 = new DenseInstance(25);
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = (double) 25;
      doubleArray0[1] = (double) 25;
      doubleArray0[2] = 5.019028201623571E-4;
      doubleArray0[3] = (double) 25;
      doubleArray0[4] = (double) 25;
      doubleArray0[5] = (double) 2;
      doubleArray0[6] = (-1262.0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(2, doubleArray0);
      SparseInstance sparseInstance0 = new SparseInstance(0.002683355685264898, doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance(sparseInstance0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((SparseInstance) binarySparseInstance1);
      Evaluation evaluation0 = new Evaluation(instances0);
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      MockRandom mockRandom0 = new MockRandom(3748L);
      String[] stringArray0 = new String[6];
      stringArray0[0] = "d";
      stringArray0[1] = "@relation";
      stringArray0[2] = ".arff";
      stringArray0[3] = "@relation";
      stringArray0[4] = "@data";
      stringArray0[5] = "@relation";
      try { 
        evaluation0.crossValidateModel("@data", instances0, 25, stringArray0, (Random) mockRandom0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Can't find class called: @data
         //
         verifyException("weka.core.Utils", e);
      }
  }

  /**
  //Test case number: 75
  /*Coverage entropy=2.982322823691787
  */
  @Test(timeout = 4000)
  public void test075()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      Evaluation evaluation0 = new Evaluation(instances0);
      Double double0 = new Double(0.0);
      instances0.deleteStringAttributes();
      FileSystemHandling.shouldAllThrowIOExceptions();
      CostMatrix costMatrix0 = new CostMatrix(2);
      Instances instances1 = testInstances0.generate();
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      costSensitiveClassifier0.getCostMatrix();
      instances0.deleteStringAttributes();
      Double double1 = new Double((-2));
      testInstances0.generate();
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      costSensitiveClassifier0.getCostMatrix();
      Evaluation evaluation1 = new Evaluation(instances0, costMatrix0);
      evaluation1.addNumericTrainClass(1, (-1853.8104163393));
      costSensitiveClassifier2.listOptions();
      Integer integer0 = new Integer(1);
      double double2 = evaluation0.precision(1);
      evaluation1.areaUnderPRC(1);
      evaluation0.falseNegativeRate(1);
      double double3 = evaluation1.unweightedMicroFmeasure();
      double double4 = evaluation0.trueNegativeRate(1);
      assertEquals(double4, double2, 0.01);
      
      Evaluation evaluation2 = new Evaluation(instances1);
      double double5 = evaluation2.weightedFalsePositiveRate();
      assertEquals(double5, double3, 0.01);
      assertEquals(Double.NaN, double5, 0.01);
  }

  /**
  //Test case number: 76
  /*Coverage entropy=1.6999267696231979
  */
  @Test(timeout = 4000)
  public void test076()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "getFastRegression");
      Instances instances0 = textDirectoryLoader0.getDataSet();
      DenseInstance denseInstance0 = new DenseInstance(25);
      Instances instances1 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader0);
      Instances instances2 = textDirectoryLoader0.getDataSet();
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      instances2.sort(comparator0);
      Evaluation evaluation0 = new Evaluation(instances0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(16);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((Instance) binarySparseInstance0);
      binarySparseInstance1.deleteAttributeAt(2);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((Instance) binarySparseInstance1);
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "@relation");
      KDTree kDTree0 = new KDTree(instances1);
      kDTree0.setMinBoxRelWidth(2);
      CoverTree coverTree0 = new CoverTree();
      JSONLoader jSONLoader0 = new JSONLoader();
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      double double0 = evaluation0.avgCost();
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01);
      assertEquals(Double.NaN, double0, 0.01);
  }

  /**
  //Test case number: 77
  /*Coverage entropy=2.337827080898473
  */
  @Test(timeout = 4000)
  public void test077()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      Instances instances0 = textDirectoryLoader0.getDataSet();
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "@data");
      TextDirectoryLoader textDirectoryLoader1 = new TextDirectoryLoader();
      textDirectoryLoader1.getDataSet();
      Evaluation evaluation0 = new Evaluation(instances0);
      double double0 = evaluation0.weightedAreaUnderPRC();
      double double1 = evaluation0.weightedTruePositiveRate();
      assertEquals(double1, double0, 0.01);
      
      evaluation0.sizeOfPredictedRegions();
      evaluation0.getRevision();
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01);
  }

  /**
  //Test case number: 78
  /*Coverage entropy=2.1105300498329425
  */
  @Test(timeout = 4000)
  public void test078()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      Evaluation evaluation0 = new Evaluation(instances0);
      evaluation0.m_DiscardPredictions = true;
      Double double0 = new Double((-1935.4593093));
      evaluation0.KBRelativeInformation();
      LogPanel logPanel0 = new LogPanel();
      logPanel0.getAlignmentY();
      TransferHandler transferHandler0 = new TransferHandler(".arff");
      transferHandler0.getDragImageOffset();
      JSlider jSlider0 = new JSlider();
      JSlider jSlider1 = null;
      try {
        jSlider1 = new JSlider((-1));
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // orientation must be one of: VERTICAL, HORIZONTAL
         //
         verifyException("javax.swing.JSlider", e);
      }
  }

  /**
  //Test case number: 79
  /*Coverage entropy=1.7653291326451686
  */
  @Test(timeout = 4000)
  public void test079()  throws Throwable  {
      Stacking stacking0 = new Stacking();
      ZeroR zeroR0 = (ZeroR)stacking0.getMetaClassifier();
      zeroR0.getOptions();
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      Instances instances0 = ConverterUtils.DataSource.read((Loader) textDirectoryLoader0);
      Evaluation evaluation0 = new Evaluation(instances0);
      evaluation0.truePositiveRate(24);
      double[] doubleArray0 = new double[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(73, doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      try { 
        evaluation0.updateStatsForPredictor((-1208.3155), binarySparseInstance1);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 80
  /*Coverage entropy=2.4256721419585143
  */
  @Test(timeout = 4000)
  public void test080()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      Instances instances0 = textDirectoryLoader0.getStructure();
      textDirectoryLoader0.reset();
      Evaluation evaluation0 = new Evaluation(instances0);
      evaluation0.toMatrixString();
      evaluation0.weightedAreaUnderPRC();
      double double0 = evaluation0.truePositiveRate(25);
      CSVLoader cSVLoader0 = new CSVLoader();
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      Box.createVerticalBox();
      GridBagLayout gridBagLayout1 = new GridBagLayout();
      gridBagLayout0.getLayoutOrigin();
      double double1 = evaluation0.falseNegativeRate(2711);
      assertEquals(double1, double0, 0.01);
      assertEquals(0.0, double1, 0.01);
  }

  /**
  //Test case number: 81
  /*Coverage entropy=3.2180066109782044
  */
  @Test(timeout = 4000)
  public void test081()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      Evaluation evaluation0 = new Evaluation(instances0);
      Evaluation evaluation1 = new Evaluation(instances0);
      double double0 = evaluation0.m_MissingClass;
      evaluation0.incorrect();
      testInstances0.setNumNominal(1432);
      instances0.deleteStringAttributes();
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      GridBagLayout gridBagLayout1 = new GridBagLayout();
      GridBagLayout gridBagLayout2 = new GridBagLayout();
      gridBagLayout2.getLayoutOrigin();
      GridBagLayout gridBagLayout3 = new GridBagLayout();
      evaluation1.falsePositiveRate(120);
      evaluation0.weightedFalseNegativeRate();
      evaluation1.SFMeanSchemeEntropy();
      evaluation1.weightedMatthewsCorrelation();
      evaluation0.weightedPrecision();
      CoverTree coverTree0 = new CoverTree();
      evaluation1.weightedFalsePositiveRate();
      evaluation1.pctIncorrect();
      evaluation1.pctUnclassified();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      SparseInstance sparseInstance0 = null;
      try {
        sparseInstance0 = new SparseInstance(2021.474829868795, (double[]) null, (int[]) null, 500);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.SparseInstance", e);
      }
  }

  /**
  //Test case number: 82
  /*Coverage entropy=1.5713732392451203
  */
  @Test(timeout = 4000)
  public void test082()  throws Throwable  {
      KStar kStar0 = new KStar();
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      Capabilities capabilities0 = adaBoostM1_0.getCapabilities();
      InfoGainAttributeEval infoGainAttributeEval0 = new InfoGainAttributeEval();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      infoGainAttributeEval0.listOptions();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      InfoGainAttributeEval infoGainAttributeEval1 = new InfoGainAttributeEval();
      infoGainAttributeEval1.listOptions();
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      CostMatrix costMatrix0 = costSensitiveClassifier1.getCostMatrix();
      Evaluation evaluation0 = new Evaluation(instances0);
      MockRandom mockRandom0 = new MockRandom();
      costMatrix0.toMatlab();
      MockRandom mockRandom1 = new MockRandom();
      DatabaseLoader databaseLoader0 = new DatabaseLoader();
      databaseLoader0.getStructure();
      Object[] objectArray0 = new Object[6];
      objectArray0[0] = (Object) kStar0;
      objectArray0[1] = (Object) capabilities1;
      objectArray0[2] = (Object) costMatrix0;
      objectArray0[3] = (Object) mockRandom1;
      objectArray0[4] = (Object) adaBoostM1_0;
      objectArray0[5] = (Object) testInstances0;
      try { 
        evaluation0.crossValidateModel((Classifier) costSensitiveClassifier0, (Instances) null, (-2), (Random) mockRandom1, objectArray0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Instances", e);
      }
  }

  /**
  //Test case number: 83
  /*Coverage entropy=0.3333333333333333
  */
  @Test(timeout = 4000)
  public void test083()  throws Throwable  {
      SGDText sGDText0 = new SGDText();
      sGDText0.setLearningRate((-52.0));
      sGDText0.setUseWordFrequencies(false);
      sGDText0.setSeed(0);
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      sGDText0.setMinWordFrequency(1);
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      double[] doubleArray0 = new double[4];
      doubleArray0[0] = 197.72;
      doubleArray0[1] = (double) 1;
      doubleArray0[3] = (double) 0;
      int[] intArray0 = new int[3];
      intArray0[0] = 1;
      intArray0[1] = 1;
      intArray0[2] = 0;
      SparseInstance sparseInstance0 = new SparseInstance(0);
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      try { 
        Evaluation.wekaStaticWrapper(adaBoostM1_0, "");
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // No model built yet
         //
         verifyException("weka.classifiers.meta.AdaBoostM1", e);
      }
  }

  /**
  //Test case number: 84
  /*Coverage entropy=1.6492377541392287
  */
  @Test(timeout = 4000)
  public void test084()  throws Throwable  {
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "");
      InfoGainAttributeEval infoGainAttributeEval0 = new InfoGainAttributeEval();
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "setCostMatrix");
      SimpleLinearRegression simpleLinearRegression0 = new SimpleLinearRegression();
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, (String) null);
      Capabilities capabilities0 = simpleLinearRegression0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      capabilities0.clone();
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.STRING_CLASS;
      capabilities1.disableDependency(capabilities_Capability0);
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      int int0 = (-1219);
      capabilities0.enableAll();
      Evaluation evaluation0 = new Evaluation(instances0);
      try { 
        evaluation0.evaluateModel((Classifier) simpleLinearRegression0, instances0, (Object[]) testInstances0.DEFAULT_WORDS);
        fail("Expecting exception: ClassCastException");
      
      } catch(ClassCastException e) {
         //
         // java.lang.String cannot be cast to weka.classifiers.evaluation.output.prediction.AbstractOutput
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 85
  /*Coverage entropy=3.507305136385497
  */
  @Test(timeout = 4000)
  public void test085()  throws Throwable  {
      KStar kStar0 = new KStar();
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      Locale.getISOLanguages();
      Capabilities capabilities0 = adaBoostM1_0.getCapabilities();
      InfoGainAttributeEval infoGainAttributeEval0 = new InfoGainAttributeEval();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      infoGainAttributeEval0.getCapabilities();
      infoGainAttributeEval0.listOptions();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      costSensitiveClassifier0.getCostMatrix();
      Evaluation evaluation0 = new Evaluation(instances0);
      MockRandom mockRandom0 = new MockRandom();
      mockRandom0.nextFloat();
      evaluation0.toClassDetailsString();
      // Undeclared exception!
      try { 
        ConverterUtils.DataSource.read(" ");
        fail("Expecting exception: NoClassDefFoundError");
      
      } catch(NoClassDefFoundError e) {
         //
         // Could not initialize class weka.core.converters.ConverterUtils
         //
         verifyException("weka.core.converters.ConverterUtils$DataSource", e);
      }
  }

  /**
  //Test case number: 86
  /*Coverage entropy=0.3333333333333333
  */
  @Test(timeout = 4000)
  public void test086()  throws Throwable  {
      KStar kStar0 = new KStar();
      Evaluation.handleCostOption((String) null, (-665));
      DropTarget dropTarget0 = null;
      try {
        dropTarget0 = new DropTarget();
        fail("Expecting exception: HeadlessException");
      
      } catch(HeadlessException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("java.awt.dnd.DropTarget", e);
      }
  }

  /**
  //Test case number: 87
  /*Coverage entropy=2.9399778528995233
  */
  @Test(timeout = 4000)
  public void test087()  throws Throwable  {
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      Capabilities capabilities0 = adaBoostM1_0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      instances0.deleteStringAttributes();
      FileSystemHandling.shouldAllThrowIOExceptions();
      Evaluation evaluation0 = new Evaluation(instances0);
      double double0 = evaluation0.weightedAreaUnderPRC();
      double double1 = evaluation0.m_WithClass;
      double double2 = evaluation0.weightedTrueNegativeRate();
      assertEquals(Double.NaN, double2, 0.01);
      
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      FindWithCapabilities findWithCapabilities1 = new FindWithCapabilities();
      InfoGainAttributeEval infoGainAttributeEval0 = new InfoGainAttributeEval();
      infoGainAttributeEval0.listOptions();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      Evaluation evaluation1 = new Evaluation(instances0);
      SimpleLogistic simpleLogistic0 = new SimpleLogistic();
      boolean boolean0 = evaluation1.m_CoverageStatisticsAvailable;
      Evaluation evaluation2 = new Evaluation(instances0);
      double double3 = evaluation2.SFEntropyGain();
      assertEquals(0.0, double3, 0.01);
      assertNotEquals(double3, double0, 0.01);
      
      Evaluation.makeOptionString(simpleLogistic0, true);
      CheckGOE checkGOE0 = new CheckGOE();
      Evaluation evaluation3 = new Evaluation(instances0);
      evaluation3.equals(findWithCapabilities1);
      evaluation0.getClassPriors();
      double double4 = evaluation1.weightedTruePositiveRate();
      assertNotEquals(double4, double3, 0.01);
  }

  /**
  //Test case number: 88
  /*Coverage entropy=1.5713732392451203
  */
  @Test(timeout = 4000)
  public void test088()  throws Throwable  {
      Stacking stacking0 = new Stacking();
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      Evaluation evaluation0 = new Evaluation(instances0);
      evaluation0.areaUnderROC(5);
      JSlider jSlider0 = new JSlider();
      // Undeclared exception!
      try { 
        jSlider0.setBounds((Rectangle) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("java.awt.Component", e);
      }
  }

  /**
  //Test case number: 89
  /*Coverage entropy=1.6841038364988299
  */
  @Test(timeout = 4000)
  public void test089()  throws Throwable  {
      KStar kStar0 = new KStar();
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      Capabilities capabilities0 = adaBoostM1_0.getCapabilities();
      InfoGainAttributeEval infoGainAttributeEval0 = new InfoGainAttributeEval();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      infoGainAttributeEval0.toString();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      Instances instances0 = testInstances0.generate();
      infoGainAttributeEval0.listOptions();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      CostMatrix costMatrix0 = costSensitiveClassifier2.getCostMatrix();
      Evaluation evaluation0 = new Evaluation(instances0);
      MockRandom mockRandom0 = new MockRandom();
      costMatrix0.toMatlab();
      MockRandom mockRandom1 = new MockRandom();
      // Undeclared exception!
      try { 
        evaluation0.precision(870);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 870
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 90
  /*Coverage entropy=1.6714599763614508
  */
  @Test(timeout = 4000)
  public void test090()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      LMT lMT0 = new LMT();
      Capabilities capabilities0 = lMT0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      doReturn(0, 0, 0, 0, 0).when(comparator0).compare(any() , any());
      instances0.sort(comparator0);
      CostMatrix costMatrix0 = costSensitiveClassifier3.getCostMatrix();
      Evaluation evaluation0 = new Evaluation(instances0);
      Evaluation evaluation1 = new Evaluation(instances0);
      MockRandom mockRandom0 = new MockRandom();
      MockRandom mockRandom1 = new MockRandom();
      mockRandom1.nextGaussian();
      costMatrix0.toMatlab();
      MockRandom mockRandom2 = new MockRandom();
      costSensitiveClassifier3.setCostMatrix(costMatrix0);
      Stacking stacking0 = new Stacking();
      Classifier classifier0 = stacking0.getMetaClassifier();
      Object[] objectArray0 = new Object[8];
      objectArray0[0] = (Object) costSensitiveClassifier2;
      objectArray0[1] = (Object) "[0.0]";
      objectArray0[2] = (Object) costSensitiveClassifier2;
      objectArray0[3] = (Object) capabilities0;
      Object object0 = new Object();
      objectArray0[4] = object0;
      objectArray0[5] = (Object) instances0;
      objectArray0[6] = (Object) null;
      objectArray0[7] = (Object) testInstances0;
      try { 
        evaluation0.crossValidateModel(classifier0, instances0, (-2), (Random) mockRandom0, objectArray0);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // Number of folds must be greater than 1
         //
         verifyException("weka.core.Instances", e);
      }
  }

  /**
  //Test case number: 91
  /*Coverage entropy=1.0264805138932787
  */
  @Test(timeout = 4000)
  public void test091()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      Instances instances0 = textDirectoryLoader0.getDataSet();
      int int0 = 25;
      DenseInstance denseInstance0 = new DenseInstance(25);
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = (double) 25;
      doubleArray0[1] = (double) 25;
      doubleArray0[2] = 25.0;
      doubleArray0[3] = (double) 25;
      ArrayList<String> arrayList0 = new ArrayList<String>();
      Attribute attribute0 = new Attribute("rB\"eG/pl.Z_ iU-(Ro", arrayList0);
      Attribute attribute1 = attribute0.copy((String) null);
      instances0.setClass(attribute1);
      doubleArray0[4] = (double) 25;
      doubleArray0[5] = (double) 2;
      doubleArray0[6] = (-1262.0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(2, doubleArray0);
      SparseInstance sparseInstance0 = new SparseInstance(0.002683355685264898, doubleArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance(sparseInstance0);
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((SparseInstance) binarySparseInstance1);
      Evaluation evaluation0 = null;
      try {
        evaluation0 = new Evaluation(instances0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // Class index is negative (not set)!
         //
         verifyException("weka.core.Instances", e);
      }
  }

  /**
  //Test case number: 92
  /*Coverage entropy=1.6841038364988299
  */
  @Test(timeout = 4000)
  public void test092()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      SGDText sGDText0 = new SGDText();
      AbstractClassifier.makeCopy(sGDText0);
      sGDText0.setUseWordFrequencies(false);
      LMT lMT0 = new LMT();
      Capabilities capabilities0 = lMT0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      CostMatrix costMatrix0 = costSensitiveClassifier1.getCostMatrix();
      Evaluation evaluation0 = new Evaluation(instances0);
      MockRandom mockRandom0 = new MockRandom();
      mockRandom0.nextGaussian();
      CostMatrix costMatrix1 = costSensitiveClassifier0.getCostMatrix();
      costMatrix1.toMatlab();
      costMatrix0.toMatlab();
      MockRandom mockRandom1 = new MockRandom();
      CostMatrix costMatrix2 = costSensitiveClassifier2.getCostMatrix();
      CostMatrix costMatrix3 = new CostMatrix(costMatrix2);
      costSensitiveClassifier0.setCostMatrix(costMatrix0);
      try { 
        evaluation0.crossValidateModel((Classifier) costSensitiveClassifier2, instances0, 2, (Random) mockRandom1, (Object[]) costSensitiveClassifier0.TAGS_MATRIX_SOURCE);
        fail("Expecting exception: ClassCastException");
      
      } catch(ClassCastException e) {
         //
         // weka.core.Tag cannot be cast to weka.classifiers.evaluation.output.prediction.AbstractOutput
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 93
  /*Coverage entropy=1.7946065751184126
  */
  @Test(timeout = 4000)
  public void test093()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      FileSystemHandling.createFolder((EvoSuiteFile) null);
      LMT lMT0 = new LMT();
      M5Rules m5Rules0 = new M5Rules();
      Capabilities capabilities0 = m5Rules0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      FileSystemHandling.createFolder((EvoSuiteFile) null);
      Instances instances0 = testInstances0.generate("getClass");
      Evaluation evaluation0 = new Evaluation(instances0);
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      doReturn(0, 0, 0, 0, 0).when(comparator0).compare(any() , any());
      instances0.sort(comparator0);
      Evaluation evaluation1 = new Evaluation(instances0);
      GaussianProcesses gaussianProcesses0 = new GaussianProcesses();
      evaluation1.rootMeanPriorSquaredError();
      JFrame jFrame0 = null;
      try {
        jFrame0 = new JFrame();
        fail("Expecting exception: HeadlessException");
      
      } catch(HeadlessException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("java.awt.GraphicsEnvironment", e);
      }
  }

  /**
  //Test case number: 94
  /*Coverage entropy=0.3333333333333333
  */
  @Test(timeout = 4000)
  public void test094()  throws Throwable  {
      KStar kStar0 = new KStar();
      AdaBoostM1 adaBoostM1_0 = new AdaBoostM1();
      Capabilities capabilities0 = adaBoostM1_0.getCapabilities();
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      TestInstances.forCapabilities(capabilities1);
      InfoGainAttributeEval infoGainAttributeEval0 = new InfoGainAttributeEval();
      Capabilities capabilities2 = infoGainAttributeEval0.getCapabilities();
      TestInstances.forCapabilities(capabilities2);
      infoGainAttributeEval0.listOptions();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      CostMatrix costMatrix0 = costSensitiveClassifier0.getCostMatrix();
      MockRandom mockRandom0 = new MockRandom();
      MockRandom mockRandom1 = new MockRandom();
      mockRandom1.nextFloat();
      costMatrix0.toMatlab();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("[0.0]", arrayList0, 20);
      RegressionByDiscretization regressionByDiscretization0 = new RegressionByDiscretization();
      DatabaseLoader databaseLoader0 = new DatabaseLoader();
      databaseLoader0.getNextInstance(instances0);
      Evaluation evaluation0 = null;
      try {
        evaluation0 = new Evaluation(instances0, costMatrix0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // Class index is negative (not set)!
         //
         verifyException("weka.core.Instances", e);
      }
  }

  /**
  //Test case number: 95
  /*Coverage entropy=0.3333333333333333
  */
  @Test(timeout = 4000)
  public void test095()  throws Throwable  {
      FileSystemHandling.createFolder((EvoSuiteFile) null);
      LMT lMT0 = new LMT();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier2 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier3 = new CostSensitiveClassifier();
      costSensitiveClassifier3.getCostMatrix();
      MockRandom mockRandom0 = new MockRandom();
      mockRandom0.nextGaussian();
      FileSystemHandling.setPermissions((EvoSuiteFile) null, true, true, true);
      String[] stringArray0 = new String[6];
      stringArray0[0] = ".cost";
      stringArray0[1] = ".cost";
      stringArray0[2] = ".cost";
      stringArray0[3] = ".cost";
      stringArray0[4] = ".cost";
      stringArray0[5] = ".cost";
      try { 
        Evaluation.evaluateModel(".cost", stringArray0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Can't find class with name .cost.
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 96
  /*Coverage entropy=1.5713732392451203
  */
  @Test(timeout = 4000)
  public void test096()  throws Throwable  {
      String[] stringArray0 = new String[9];
      stringArray0[0] = "";
      stringArray0[1] = "";
      stringArray0[2] = "";
      stringArray0[3] = "";
      LinearRegression linearRegression0 = new LinearRegression();
      linearRegression0.getAttributeSelectionMethod();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      KDTree kDTree0 = new KDTree();
      KDTree kDTree1 = new KDTree();
      WekaTaskMonitor wekaTaskMonitor0 = new WekaTaskMonitor();
      MouseWheelListener mouseWheelListener0 = mock(MouseWheelListener.class, new ViolatedAssumptionAnswer());
      AWTEventMulticaster.remove(mouseWheelListener0, mouseWheelListener0);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      WekaTaskMonitor wekaTaskMonitor1 = new WekaTaskMonitor();
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      wekaTaskMonitor1.addMouseWheelListener((MouseWheelListener) null);
      DenseInstance denseInstance0 = new DenseInstance(2);
      Evaluation evaluation0 = new Evaluation(instances0);
      evaluation0.SFSchemeEntropy();
      // Undeclared exception!
      try { 
        ConverterUtils.DataSource.read("");
        fail("Expecting exception: NoClassDefFoundError");
      
      } catch(NoClassDefFoundError e) {
         //
         // Could not initialize class weka.core.converters.ConverterUtils
         //
         verifyException("weka.core.converters.ConverterUtils$DataSource", e);
      }
  }

  /**
  //Test case number: 97
  /*Coverage entropy=1.0365141682948127
  */
  @Test(timeout = 4000)
  public void test097()  throws Throwable  {
      LMT lMT0 = new LMT();
      Evaluation.makeOptionString(lMT0, true);
      SparseInstance sparseInstance0 = null;
      try {
        sparseInstance0 = new SparseInstance((-982.855083604), (double[]) null, (int[]) null, 3210);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.SparseInstance", e);
      }
  }

  /**
  //Test case number: 98
  /*Coverage entropy=0.3333333333333333
  */
  @Test(timeout = 4000)
  public void test098()  throws Throwable  {
      String[] stringArray0 = new String[4];
      stringArray0[0] = "thence";
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      stringArray0[1] = "\tThe directory to work on.\n\t(default: current directory)";
      stringArray0[2] = "=== Confusion Matrix ===\n";
      J48 j48_0 = new J48();
      j48_0.getOptions();
      try { 
        Evaluation.evaluateModel((Classifier) j48_0, stringArray0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Utils", e);
      }
  }

  /**
  //Test case number: 99
  /*Coverage entropy=0.3333333333333333
  */
  @Test(timeout = 4000)
  public void test099()  throws Throwable  {
      String string0 = "";
      Evaluation.handleCostOption("", 3044);
      WekaTaskMonitor wekaTaskMonitor0 = new WekaTaskMonitor();
      LogPanel logPanel0 = new LogPanel();
      // Undeclared exception!
      try { 
        logPanel0.getLocationOnScreen();
        fail("Expecting exception: IllegalComponentStateException");
      
      } catch(IllegalComponentStateException e) {
         //
         // component must be showing on the screen to determine its location
         //
         verifyException("java.awt.Component", e);
      }
  }

  /**
  //Test case number: 100
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test100()  throws Throwable  {
      Stacking stacking0 = new Stacking();
      String[] stringArray0 = new String[0];
      Evaluation.main(stringArray0);
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      GridBagLayout gridBagLayout1 = new GridBagLayout();
      GridBagLayout gridBagLayout2 = new GridBagLayout();
      gridBagLayout0.getLayoutOrigin();
      GridBagLayout gridBagLayout3 = new GridBagLayout();
      DatabaseLoader databaseLoader0 = new DatabaseLoader();
      databaseLoader0.getDataSet();
      CostMatrix costMatrix0 = new CostMatrix(0);
      CostMatrix costMatrix1 = new CostMatrix(costMatrix0);
      Evaluation evaluation0 = null;
      try {
        evaluation0 = new Evaluation((Instances) null, costMatrix1);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 101
  /*Coverage entropy=0.3333333333333333
  */
  @Test(timeout = 4000)
  public void test101()  throws Throwable  {
      Comparator<Object> comparator0 = (Comparator<Object>) mock(Comparator.class, new ViolatedAssumptionAnswer());
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate();
      KDTree kDTree0 = new KDTree();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props");
      FileSystemHandling.appendLineToFile(evoSuiteFile0, "Class must be nominal for margin distributions");
      KDTree kDTree1 = new KDTree();
      WekaTaskMonitor wekaTaskMonitor0 = new WekaTaskMonitor();
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.remove((MouseWheelListener) null, (MouseWheelListener) null);
      AWTEventMulticaster.add((MouseWheelListener) null, (MouseWheelListener) null);
      wekaTaskMonitor0.addMouseWheelListener((MouseWheelListener) null);
      SimpleLogistic simpleLogistic0 = new SimpleLogistic();
      EvoSuiteFile evoSuiteFile1 = new EvoSuiteFile("/home/ubuntu/wekafiles/props");
      FileSystemHandling.setPermissions(evoSuiteFile1, false, false, true);
      CostMatrix costMatrix0 = new CostMatrix(1);
      CostMatrix costMatrix1 = new CostMatrix(costMatrix0);
      Evaluation evaluation0 = null;
      try {
        evaluation0 = new Evaluation(instances0, costMatrix1);
        fail("Expecting exception: Exception");
      
      } catch(Throwable e) {
         //
         // Cost matrix not compatible with data!
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 102
  /*Coverage entropy=0.9799151714181009
  */
  @Test(timeout = 4000)
  public void test102()  throws Throwable  {
      String[] stringArray0 = new String[4];
      stringArray0[0] = "thence";
      stringArray0[1] = "\tThe directory to work on.\n\t(default: current directory)";
      stringArray0[2] = "=== Confusion Matrix ===\n";
      FileSystemHandling.shouldAllThrowIOExceptions();
      stringArray0[3] = "";
      try { 
        Evaluation.evaluateModel((Classifier) null, stringArray0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // 
         // Weka exception: No training file and no object input file given.
         // 
         // General options:
         // 
         // -h or -help
         // \tOutput help information.
         // -synopsis or -info
         // \tOutput synopsis for classifier (use in conjunction  with -h)
         // -t <name of training file>
         // \tSets training file.
         // -T <name of test file>
         // \tSets test file. If missing, a cross-validation will be performed
         // \ton the training data.
         // -c <class index>
         // \tSets index of class attribute (default: last).
         // -x <number of folds>
         // \tSets number of folds for cross-validation (default: 10).
         // -no-cv
         // \tDo not perform any cross validation.
         // -split-percentage <percentage>
         // \tSets the percentage for the train/test set split, e.g., 66.
         // -preserve-order
         // \tPreserves the order in the percentage split.
         // -s <random number seed>
         // \tSets random number seed for cross-validation or percentage split
         // \t(default: 1).
         // -m <name of file with cost matrix>
         // \tSets file with cost matrix.
         // -l <name of input file>
         // \tSets model input file. In case the filename ends with '.xml',
         // \ta PMML file is loaded or, if that fails, options are loaded
         // \tfrom the XML file.
         // -d <name of output file>
         // \tSets model output file. In case the filename ends with '.xml',
         // \tonly the options are saved to the XML file, not the model.
         // -v
         // \tOutputs no statistics for training data.
         // -o
         // \tOutputs statistics only, not the classifier.
         // -i
         // \tOutputs detailed information-retrieval statistics for each class.
         // -k
         // \tOutputs information-theoretic statistics.
         // -classifications \"weka.classifiers.evaluation.output.prediction.AbstractOutput + options\"
         // \tUses the specified class for generating the classification output.
         // \tE.g.: weka.classifiers.evaluation.output.prediction.PlainText
         // -p range
         // \tOutputs predictions for test instances (or the train instances if
         // \tno test instances provided and -no-cv is used), along with the 
         // \tattributes in the specified range (and nothing else). 
         // \tUse '-p 0' if no attributes are desired.
         // \tDeprecated: use \"-classifications ...\" instead.
         // -distribution
         // \tOutputs the distribution instead of only the prediction
         // \tin conjunction with the '-p' option (only nominal classes).
         // \tDeprecated: use \"-classifications ...\" instead.
         // -r
         // \tOnly outputs cumulative margin distribution.
         // -xml filename | xml-string
         // \tRetrieves the options from the XML-data instead of the command line.
         // -threshold-file <file>
         // \tThe file to save the threshold data to.
         // \tThe format is determined by the extensions, e.g., '.arff' for ARFF 
         // \tformat or '.csv' for CSV.
         // -threshold-label <label>
         // \tThe class label to determine the threshold data for
         // \t(default is the first label)
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 103
  /*Coverage entropy=0.3333333333333333
  */
  @Test(timeout = 4000)
  public void test103()  throws Throwable  {
      Stacking stacking0 = new Stacking();
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      String string0 = "#KmRfw";
      try { 
        Evaluation.handleCostOption("#KmRfw", 3044);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Can't open file null.
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 104
  /*Coverage entropy=1.0264805138932787
  */
  @Test(timeout = 4000)
  public void test104()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      SGDText sGDText0 = new SGDText();
      sGDText0.setUseWordFrequencies(false);
      sGDText0.setSeed(0);
      AlphabeticTokenizer alphabeticTokenizer1 = new AlphabeticTokenizer();
      sGDText0.setTokenizer(alphabeticTokenizer0);
      sGDText0.setMinWordFrequency(90.5629);
      sGDText0.setTokenizer((Tokenizer) null);
      sGDText0.setLearningRate(0);
      sGDText0.setLNorm(1);
      Evaluation evaluation0 = null;
      try {
        evaluation0 = new Evaluation((Instances) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 105
  /*Coverage entropy=1.049248354870898
  */
  @Test(timeout = 4000)
  public void test105()  throws Throwable  {
      SGDText sGDText0 = new SGDText();
      String[] stringArray0 = new String[1];
      sGDText0.setUseWordFrequencies(false);
      sGDText0.setSeed((-1638));
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      sGDText0.setMinWordFrequency((-1638));
      sGDText0.setTokenizer((Tokenizer) null);
      sGDText0.setLearningRate(2247.4802838362);
      stringArray0[0] = "";
      try { 
        Evaluation.evaluateModel((Classifier) sGDText0, stringArray0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // 
         // Weka exception: No training file and no object input file given.
         // 
         // General options:
         // 
         // -h or -help
         // \tOutput help information.
         // -synopsis or -info
         // \tOutput synopsis for classifier (use in conjunction  with -h)
         // -t <name of training file>
         // \tSets training file.
         // -T <name of test file>
         // \tSets test file. If missing, a cross-validation will be performed
         // \ton the training data.
         // -c <class index>
         // \tSets index of class attribute (default: last).
         // -x <number of folds>
         // \tSets number of folds for cross-validation (default: 10).
         // -no-cv
         // \tDo not perform any cross validation.
         // -split-percentage <percentage>
         // \tSets the percentage for the train/test set split, e.g., 66.
         // -preserve-order
         // \tPreserves the order in the percentage split.
         // -s <random number seed>
         // \tSets random number seed for cross-validation or percentage split
         // \t(default: 1).
         // -m <name of file with cost matrix>
         // \tSets file with cost matrix.
         // -l <name of input file>
         // \tSets model input file. In case the filename ends with '.xml',
         // \ta PMML file is loaded or, if that fails, options are loaded
         // \tfrom the XML file.
         // -d <name of output file>
         // \tSets model output file. In case the filename ends with '.xml',
         // \tonly the options are saved to the XML file, not the model.
         // -v
         // \tOutputs no statistics for training data.
         // -o
         // \tOutputs statistics only, not the classifier.
         // -i
         // \tOutputs detailed information-retrieval statistics for each class.
         // -k
         // \tOutputs information-theoretic statistics.
         // -classifications \"weka.classifiers.evaluation.output.prediction.AbstractOutput + options\"
         // \tUses the specified class for generating the classification output.
         // \tE.g.: weka.classifiers.evaluation.output.prediction.PlainText
         // -p range
         // \tOutputs predictions for test instances (or the train instances if
         // \tno test instances provided and -no-cv is used), along with the 
         // \tattributes in the specified range (and nothing else). 
         // \tUse '-p 0' if no attributes are desired.
         // \tDeprecated: use \"-classifications ...\" instead.
         // -distribution
         // \tOutputs the distribution instead of only the prediction
         // \tin conjunction with the '-p' option (only nominal classes).
         // \tDeprecated: use \"-classifications ...\" instead.
         // -r
         // \tOnly outputs cumulative margin distribution.
         // -xml filename | xml-string
         // \tRetrieves the options from the XML-data instead of the command line.
         // -threshold-file <file>
         // \tThe file to save the threshold data to.
         // \tThe format is determined by the extensions, e.g., '.arff' for ARFF 
         // \tformat or '.csv' for CSV.
         // -threshold-label <label>
         // \tThe class label to determine the threshold data for
         // \t(default is the first label)
         // 
         // Options specific to weka.classifiers.functions.SGDText:
         // 
         // -F
         // \tSet the loss function to minimize. 0 = hinge loss (SVM), 1 = log loss (logistic regression)
         // \t(default = 0)
         // -outputProbs
         // \tOutput probabilities for SVMs (fits a logsitic
         // \tmodel to the output of the SVM)
         // -L
         // \tThe learning rate (default = 0.01).
         // -R <double>
         // \tThe lambda regularization constant (default = 0.0001)
         // -E <integer>
         // \tThe number of epochs to perform (batch learning only, default = 500)
         // -W
         // \tUse word frequencies instead of binary bag of words.
         // -P <# instances>
         // \tHow often to prune the dictionary of low frequency words (default = 0, i.e. don't prune)
         // -M <double>
         // \tMinimum word frequency. Words with less than this frequence are ignored.
         // \tIf periodic pruning is turned on then this is also used to determine which
         // \twords to remove from the dictionary (default = 3).
         // -normalize
         // \tNormalize document length (use in conjunction with -norm and -lnorm)
         // -norm <num>
         // \tSpecify the norm that each instance must have (default 1.0)
         // -lnorm <num>
         // \tSpecify L-norm to use (default 2.0)
         // -lowercase
         // \tConvert all tokens to lowercase before adding to the dictionary.
         // -stoplist
         // \tIgnore words that are in the stoplist.
         // -stopwords <file>
         // \tA file containing stopwords to override the default ones.
         // \tUsing this option automatically sets the flag ('-stoplist') to use the
         // \tstoplist if the file exists.
         // \tFormat: one stopword per line, lines starting with '#'
         // \tare interpreted as comments and ignored.
         // -tokenizer <spec>
         // \tThe tokenizing algorihtm (classname plus parameters) to use.
         // \t(default: weka.core.tokenizers.WordTokenizer)
         // -stemmer <spec>
         // \tThe stemmering algorihtm (classname plus parameters) to use.
         //
         verifyException("weka.classifiers.Evaluation", e);
      }
  }

  /**
  //Test case number: 106
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test106()  throws Throwable  {
      String[] stringArray0 = new String[9];
      stringArray0[0] = "";
      stringArray0[1] = "";
      stringArray0[2] = "";
      stringArray0[3] = "";
      stringArray0[4] = "";
      stringArray0[5] = "";
      stringArray0[6] = "";
      stringArray0[7] = "";
      stringArray0[8] = "xXD";
      Evaluation.main(stringArray0);
      GridBagLayout gridBagLayout0 = new GridBagLayout();
      GridBagLayout gridBagLayout1 = new GridBagLayout();
      GridBagLayout gridBagLayout2 = new GridBagLayout();
      GridBagLayout gridBagLayout3 = new GridBagLayout();
      gridBagLayout3.getLayoutOrigin();
      GridBagLayout gridBagLayout4 = new GridBagLayout();
      assertFalse(gridBagLayout4.equals((Object)gridBagLayout2));
  }
}
