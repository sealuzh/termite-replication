/*
 * This file was automatically generated by EvoSuite
 * Tue Nov 19 11:23:30 GMT 2019
 */

package weka.classifiers.bayes;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.File;
import java.net.URI;
import java.util.ArrayList;
import java.util.function.Predicate;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.Random;
import org.evosuite.runtime.mock.java.io.MockFile;
import org.evosuite.runtime.mock.java.net.MockURI;
import org.evosuite.runtime.mock.java.util.MockRandom;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.evosuite.runtime.util.SystemInUtil;
import org.junit.runner.RunWith;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.bayes.BayesNet;
import weka.classifiers.bayes.NaiveBayesMultinomialText;
import weka.classifiers.functions.SGDText;
import weka.classifiers.functions.supportVector.PrecomputedKernelMatrixKernel;
import weka.classifiers.meta.CostSensitiveClassifier;
import weka.classifiers.misc.InputMappedClassifier;
import weka.core.Attribute;
import weka.core.BinarySparseInstance;
import weka.core.Capabilities;
import weka.core.DenseInstance;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.SparseInstance;
import weka.core.Stopwords;
import weka.core.TestInstances;
import weka.core.matrix.Matrix;
import weka.core.neighboursearch.CoverTree;
import weka.core.stemmers.SnowballStemmer;
import weka.core.stemmers.Stemmer;
import weka.core.tokenizers.AlphabeticTokenizer;
import weka.core.tokenizers.NGramTokenizer;
import weka.core.tokenizers.Tokenizer;
import weka.core.tokenizers.WordTokenizer;
import weka.estimators.MahalanobisEstimator;
import weka.filters.supervised.attribute.Discretize;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class NaiveBayesMultinomialText_ESTest extends NaiveBayesMultinomialText_ESTest_scaffolding {

  /**
  //Test case number: 0
  /*Coverage entropy=2.2377866549640077
  */
  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[6];
      stringArray0[0] = ";U/z?Rm+fZ$";
      stringArray0[1] = "cb<,";
      stringArray0[2] = "-stemmer";
      stringArray0[3] = "Invalid search specification string";
      stringArray0[4] = "";
      stringArray0[5] = "\tUse Kononenko's MDL criterion.";
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      File file0 = precomputedKernelMatrixKernel0.getKernelMatrixFile();
      naiveBayesMultinomialText0.setStopwords(file0);
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: ClassNotFoundException");
      
      } catch(ClassNotFoundException e) {
      }
  }

  /**
  //Test case number: 1
  /*Coverage entropy=2.2802759577666283
  */
  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_useStopList = true;
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("jm:1aLJAN%t^i}ncxI|");
      MockRandom mockRandom0 = new MockRandom();
      byte[] byteArray0 = new byte[8];
      byteArray0[0] = (byte)107;
      byteArray0[2] = (byte)107;
      byteArray0[3] = (byte)107;
      byteArray0[4] = (byte)107;
      byteArray0[5] = (byte)107;
      byteArray0[6] = (byte)107;
      byteArray0[7] = (byte)107;
      mockRandom0.nextBytes(byteArray0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      MockFile mockFile0 = new MockFile(" ");
      int[] intArray0 = new int[3];
      intArray0[0] = (int) (byte)107;
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      intArray0[1] = (-1);
      intArray0[2] = (int) (byte)0;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(9.0, intArray0, (-1));
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.LNormTipText();
      boolean boolean0 = naiveBayesMultinomialText0.getUseWordFrequencies();
      assertTrue(boolean0);
  }

  /**
  //Test case number: 2
  /*Coverage entropy=2.0548947705970715
  */
  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_useStopList = true;
      naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate("jm:1aLJAN%t^i}ncxI|");
      MockRandom mockRandom0 = new MockRandom();
      byte[] byteArray0 = new byte[4];
      mockRandom0.nextBytes(byteArray0);
      Instances instances1 = instances0.resample(mockRandom0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1498);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      instances1.add((Instance) binarySparseInstance1);
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances1);
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
         //
         // Index: 2, Size: 2
         //
         verifyException("java.util.ArrayList", e);
      }
  }

  /**
  //Test case number: 3
  /*Coverage entropy=2.5629273879918326
  */
  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("jm:1aLJAN%t^i}ncxI|");
      capabilities0.enableAllAttributes();
      byte[] byteArray0 = new byte[1];
      byteArray0[0] = (byte)107;
      naiveBayesMultinomialText0.buildClassifier(instances0);
      Capabilities capabilities1 = capabilities0.getClassCapabilities();
      MockFile mockFile0 = new MockFile(".arff", " ");
      Capabilities.forInstances(instances0);
      int[] intArray0 = new int[9];
      intArray0[0] = (-2);
      intArray0[1] = (-1);
      intArray0[2] = (-1);
      intArray0[5] = (int) (byte)107;
      intArray0[4] = (int) (byte)107;
      double[][] doubleArray0 = new double[2][8];
      capabilities0.disableAll();
      Matrix matrix0 = new Matrix(doubleArray0, (-2), 30000);
      MahalanobisEstimator mahalanobisEstimator0 = new MahalanobisEstimator(matrix0, (byte)107, 107);
      capabilities1.getOtherCapabilities();
      capabilities0.and(capabilities1);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t9.0\nclass2\t4.0\nclass3\t7.0\nclass4\t4.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\nover\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nthe\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nThe\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nquick\t7.38905609893065\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nlazy\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\njumps\t20.085536923187668\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nbrown\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\ndog\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nfox\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\n", string0);
      
      String string1 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string1);
      
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      String string2 = naiveBayesMultinomialText0.normTipText();
      assertEquals("The norm of the instances after normalization.", string2);
      
      String string3 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string3);
      
      String string4 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string4);
      
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 4
  /*Coverage entropy=2.7222197006410824
  */
  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      TestInstances testInstances0 = new TestInstances();
      TestInstances testInstances1 = new TestInstances();
      TestInstances testInstances2 = new TestInstances();
      testInstances0.getWords();
      Instances instances0 = testInstances2.generate("NeuroCOLT2 Technical Report NC2-TR-1998-030");
      testInstances1.setNumDate((-1));
      MockRandom mockRandom0 = new MockRandom();
      mockRandom0.ints((-1650), (-2));
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte)68;
      mockRandom0.nextBytes(byteArray0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      MockFile mockFile0 = new MockFile("");
      int[] intArray0 = new int[5];
      intArray0[0] = (int) (byte)16;
      intArray0[0] = (-1);
      intArray0[2] = (-1);
      intArray0[3] = (int) (byte)2;
      intArray0[4] = (-1);
      TestInstances testInstances3 = (TestInstances)testInstances2.clone();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((byte)16, intArray0, (-1));
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      naiveBayesMultinomialText0.normTipText();
      testInstances3.getData();
      try { 
        naiveBayesMultinomialText0.buildClassifier((Instances) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 5
  /*Coverage entropy=2.2214577655672363
  */
  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_useStopList = true;
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate(" ");
      testInstances0.setNumInstances(6);
      MockRandom mockRandom0 = new MockRandom();
      byte[] byteArray0 = new byte[8];
      byteArray0[0] = (byte)107;
      byteArray0[2] = (byte)107;
      byteArray0[3] = (byte)107;
      byteArray0[4] = (byte)107;
      byteArray0[5] = (byte)107;
      byteArray0[6] = (byte)107;
      byteArray0[7] = (byte)107;
      mockRandom0.nextBytes(byteArray0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      MockFile mockFile0 = new MockFile(" ");
      int[] intArray0 = new int[3];
      capabilities0.testWithFail(instances0);
      intArray0[0] = (int) (byte)107;
      intArray0[1] = (-1);
      intArray0[2] = (int) (byte)0;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(9.0, intArray0, (-1));
      String string0 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string0);
      
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertArrayEquals(new double[] {0.3750000000000001, 0.16666666666666669, 0.29166666666666663, 0.16666666666666669}, doubleArray0, 0.01);
      
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      String string1 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", string1);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 6
  /*Coverage entropy=2.2168627216015633
  */
  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      String[] stringArray0 = new String[7];
      stringArray0[4] = "If true, ignores all words that are on the stoplist.";
      stringArray0[1] = "Whether to convert all tokens to lowercase";
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      File file0 = naiveBayesMultinomialText1.getStopwords();
      File file1 = MockFile.createTempFile("Whether to convert all tokens to lowercase", "If true, ignores all words that are on the stoplist.", file0);
      naiveBayesMultinomialText0.setStopwords(file1);
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "b(]ZGE~%+(><8QVV,W");
      Instances instances0 = testInstances0.generate("BfK/?Pa,%Y");
      Instances instances1 = new Instances(instances0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      
      String string0 = naiveBayesMultinomialText1.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
  }

  /**
  //Test case number: 7
  /*Coverage entropy=3.2289874463186274
  */
  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string0);
      
      String string1 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string1);
      
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate("gJC KuyA\"2=WxLI");
      MockRandom mockRandom0 = new MockRandom();
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      Predicate<Object> predicate0 = Predicate.isEqual((Object) nGramTokenizer0);
      Predicate<Object> predicate1 = predicate0.negate();
      instances0.removeIf(predicate1);
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, ".bsi");
      mockRandom0.ints();
      File file0 = MockFile.createTempFile("Whether to convert all tokens to lowercase", ".arff", (File) null);
      naiveBayesMultinomialText0.m_stopwordsFile = file0;
      Stopwords stopwords0 = naiveBayesMultinomialText0.m_stopwords;
      naiveBayesMultinomialText0.m_stopwords = null;
      MockFile mockFile0 = new MockFile("\u0004");
      MockFile mockFile1 = new MockFile(mockFile0, ":=`0 ~j*x");
      MockFile mockFile2 = new MockFile(":=`0 ~j*x", ".bsi");
      mockFile0.setReadable(true);
      mockFile1.toPath();
      testInstances0.setNumRelationalNominalValues((-4893));
      mockFile0.toURL();
      String string2 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string2);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      String string3 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string3);
      
      String string4 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string4);
      
      String string5 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string5);
      
      String string6 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string6);
      
      String string7 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string7);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(14, stringArray0.length);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 8
  /*Coverage entropy=3.6511047240118906
  */
  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      TestInstances testInstances0 = new TestInstances();
      testInstances0.generate("gJC KuyA\"2=WxLI");
      MockRandom mockRandom0 = new MockRandom();
      Instances instances0 = testInstances0.getData();
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      Predicate<Object> predicate0 = Predicate.isEqual((Object) nGramTokenizer0);
      Predicate<Object> predicate1 = predicate0.negate();
      instances0.removeIf(predicate1);
      FileSystemHandling.setPermissions((EvoSuiteFile) null, true, true, true);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      mockRandom0.ints();
      File file0 = MockFile.createTempFile("Speedin= up Lo/istmc Model Tree Induc<ion", ".arff", (File) null);
      naiveBayesMultinomialText0.m_stopwordsFile = file0;
      MockFile mockFile0 = new MockFile("\u0004");
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("Discrete Estimator. Counts = ");
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.periodicPruningTipText();
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      NaiveBayesMultinomialText.main(stringArray0);
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.normTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText1.setOptions(stringArray0);
      naiveBayesMultinomialText1.useStopListTipText();
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText1.setStopwords((File) null);
      assertTrue(naiveBayesMultinomialText1.getUseStopList());
  }

  /**
  //Test case number: 9
  /*Coverage entropy=2.292568389172343
  */
  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      TestInstances testInstances0 = new TestInstances();
      TestInstances testInstances1 = new TestInstances();
      TestInstances testInstances2 = new TestInstances();
      testInstances2.generate("NeuroCOLT2 Technical Report NC2-TR-1998-030");
      testInstances1.setNumDate((-1));
      MockRandom mockRandom0 = new MockRandom();
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte)68;
      byteArray0[2] = (byte)2;
      byteArray0[3] = (byte)16;
      byteArray0[4] = (byte)107;
      int[] intArray0 = new int[8];
      intArray0[0] = 427;
      intArray0[1] = (int) (byte)2;
      intArray0[2] = 427;
      intArray0[3] = (int) (byte)68;
      intArray0[4] = (int) (byte)68;
      intArray0[5] = (int) (byte)16;
      intArray0[6] = (-2);
      intArray0[7] = 1937;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-2), intArray0, (-1));
      naiveBayesMultinomialText0.tokenizeInstance(binarySparseInstance0, false);
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.stopwordsTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 10
  /*Coverage entropy=2.71533220230399
  */
  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      TestInstances testInstances0 = new TestInstances();
      TestInstances testInstances1 = new TestInstances();
      TestInstances testInstances2 = new TestInstances();
      testInstances0.getWords();
      Instances instances0 = testInstances2.generate("NeuroCOLT2 Technical Report NC2-TR-1998-030");
      testInstances1.setNumDate((-1));
      MockRandom mockRandom0 = new MockRandom();
      mockRandom0.ints((-1650), (-2));
      testInstances0.setClassType((-1));
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte)68;
      mockRandom0.nextBytes(byteArray0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      MockFile mockFile0 = new MockFile("The tokenizing algorithm to use on the strings.", " ");
      int[] intArray0 = new int[5];
      intArray0[0] = (int) (byte)16;
      intArray0[0] = (-1);
      intArray0[2] = (-1);
      intArray0[3] = (int) (byte)2;
      intArray0[4] = (-1);
      TestInstances testInstances3 = (TestInstances)testInstances2.clone();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((byte)16, intArray0, (-1));
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      naiveBayesMultinomialText0.normTipText();
      testInstances3.getData();
      try { 
        naiveBayesMultinomialText0.buildClassifier((Instances) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 11
  /*Coverage entropy=2.194415008437776
  */
  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      FileSystemHandling.shouldAllThrowIOExceptions();
      naiveBayesMultinomialText0.m_useStopList = true;
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("jm:1aLJAN%t^i}ncxI|");
      MockRandom mockRandom0 = new MockRandom();
      capabilities0.enableAllAttributes();
      byte[] byteArray0 = new byte[6];
      byteArray0[0] = (byte)107;
      byteArray0[1] = (byte)107;
      byteArray0[2] = (byte)107;
      byteArray0[3] = (byte)107;
      byteArray0[4] = (byte)107;
      byteArray0[5] = (byte)107;
      mockRandom0.nextBytes(byteArray0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      MockFile mockFile0 = new MockFile("-------------\t");
      Capabilities.forInstances(instances0);
      int[] intArray0 = new int[1];
      intArray0[0] = (int) (byte)0;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-287.18156), intArray0, (byte)0);
      String string0 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string0);
      
      String string1 = naiveBayesMultinomialText0.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string1);
      
      String string2 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string2);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      String string3 = naiveBayesMultinomialText1.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string3);
      
      String string4 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string4);
      
      String string5 = naiveBayesMultinomialText1.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string5);
      
      naiveBayesMultinomialText1.normTipText();
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
  }

  /**
  //Test case number: 12
  /*Coverage entropy=2.7228120764467145
  */
  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      SystemInUtil.addInputLine(" >j8e?.7Q");
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setUseStopList(true);
      AbstractClassifier.makeCopy(naiveBayesMultinomialText0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      Stopwords stopwords0 = naiveBayesMultinomialText1.m_stopwords;
      naiveBayesMultinomialText0.m_stopwords = null;
      naiveBayesMultinomialText0.getUseStopList();
      naiveBayesMultinomialText0.getMinWordFrequency();
      naiveBayesMultinomialText0.stemmerTipText();
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      
      String[] stringArray0 = new String[6];
      stringArray0[0] = "The stemming algorithm to use on the words.";
      stringArray0[1] = " >j8e?.7Q";
      stringArray0[2] = "The stemming algorithm to use on the words.";
      stringArray0[3] = "The stemming algorithm to use on the words.";
      stringArray0[4] = "The stemming algorithm to use on the words.";
      stringArray0[5] = " >j8e?.7Q";
      naiveBayesMultinomialText1.setOptions(stringArray0);
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
  }

  /**
  //Test case number: 13
  /*Coverage entropy=2.352721876121416
  */
  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_useStopList = true;
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate("jm:1aLJAN%t^i}ncxI|");
      MockRandom mockRandom0 = new MockRandom();
      mockRandom0.longs(1265L, (-5672L), 1725L);
      Instances instances1 = instances0.resample(mockRandom0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1498);
      byte[] byteArray0 = new byte[1];
      byteArray0[0] = (byte) (-82);
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      instances1.add((Instance) binarySparseInstance1);
      MockRandom mockRandom1 = new MockRandom();
      mockRandom0.nextBytes(byteArray0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      MockFile mockFile0 = new MockFile("A>Q", ".arff");
      testInstances0.clone();
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2), (int[]) null, 1);
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance1);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 14
  /*Coverage entropy=2.2427276237316462
  */
  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_useStopList = true;
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      naiveBayesMultinomialText0.setPeriodicPruning(104);
      MockFile mockFile0 = new MockFile("weka/core/Capabilities.props");
      Capabilities.Capability capabilities_Capability0 = Capabilities.Capability.NOMINAL_CLASS;
      capabilities0.enableDependency(capabilities_Capability0);
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      naiveBayesMultinomialText0.getMinWordFrequency();
      naiveBayesMultinomialText0.tokenizerTipText();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(104);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance(binarySparseInstance0, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 15
  /*Coverage entropy=1.830738805564335
  */
  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      FileSystemHandling.shouldAllThrowIOExceptions();
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, (byte[]) null);
      MockRandom mockRandom0 = new MockRandom();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("", arrayList0, (byte)20);
      Instances instances1 = instances0.resample(mockRandom0);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      try { 
        naiveBayesMultinomialText0.buildClassifier(instances1);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // weka.classifiers.bayes.NaiveBayesMultinomialText: No attributes!
         //
         verifyException("weka.core.Capabilities", e);
      }
  }

  /**
  //Test case number: 16
  /*Coverage entropy=1.987739820187409
  */
  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_useStopList = true;
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate("jm:1aLJAN%t^i}ncxI|");
      MockRandom mockRandom0 = new MockRandom();
      mockRandom0.longs(1265L, (-5672L), 1725L);
      Instances instances1 = instances0.resample(mockRandom0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1498);
      byte[] byteArray0 = new byte[1];
      byteArray0[0] = (byte) (-82);
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      instances1.add((Instance) binarySparseInstance1);
      MockRandom mockRandom1 = new MockRandom();
      testInstances0.generate("");
      MockFile mockFile0 = new MockFile("@relation");
      int[] intArray0 = new int[3];
      intArray0[0] = 1498;
      intArray0[1] = (int) (byte)68;
      intArray0[2] = (int) (byte)107;
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2398.0), intArray0, (-1046));
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "");
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 17
  /*Coverage entropy=3.3849795270216836
  */
  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      Tokenizer tokenizer0 = naiveBayesMultinomialText0.getTokenizer();
      naiveBayesMultinomialText0.setTokenizer(tokenizer0);
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.pruneDictionary();
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      String[] stringArray0 = new String[1];
      stringArray0[0] = "}%T_2!:s1R";
      snowballStemmer0.setOptions(stringArray0);
      snowballStemmer0.setStemmer("");
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.setStemmer(snowballStemmer0);
      NaiveBayesMultinomialText.main(stringArray0);
      snowballStemmer0.stemmerTipText();
      naiveBayesMultinomialText1.getCapabilities();
      naiveBayesMultinomialText0.stopwordsTipText();
      snowballStemmer0.setStemmer((String) null);
      naiveBayesMultinomialText0.getLowercaseTokens();
      naiveBayesMultinomialText0.getNormalizeDocLength();
      naiveBayesMultinomialText0.getUseStopList();
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      
      naiveBayesMultinomialText1.getStopwords();
      naiveBayesMultinomialText1.setOptions(stringArray0);
      String string0 = naiveBayesMultinomialText1.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
  }

  /**
  //Test case number: 18
  /*Coverage entropy=0.9289738521096165
  */
  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      SGDText sGDText0 = new SGDText();
      File file0 = sGDText0.getStopwords();
      naiveBayesMultinomialText0.setStopwords(file0);
      MockFile mockFile0 = new MockFile("", "0;)5d6#qwl}");
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 19
  /*Coverage entropy=2.376742981833967
  */
  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setNorm(2.0);
      naiveBayesMultinomialText0.setMinWordFrequency(1581.20756);
      String[] stringArray0 = new String[7];
      stringArray0[0] = "-P";
      stringArray0[1] = "EL#e?";
      stringArray0[2] = "";
      stringArray0[3] = "zkJ{&}S";
      stringArray0[4] = "";
      NaiveBayesMultinomialText.main(stringArray0);
      stringArray0[5] = "LO]k-?6WekXT ]";
      stringArray0[6] = "stoplist";
      naiveBayesMultinomialText0.m_t = 0.0;
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: NumberFormatException");
      
      } catch(NumberFormatException e) {
         //
         // For input string: \"EL#e?\"
         //
         verifyException("java.lang.NumberFormatException", e);
      }
  }

  /**
  //Test case number: 20
  /*Coverage entropy=2.654686491681717
  */
  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      TestInstances testInstances0 = new TestInstances();
      TestInstances testInstances1 = new TestInstances();
      TestInstances testInstances2 = new TestInstances();
      testInstances0.getWords();
      Instances instances0 = testInstances2.generate("NeuroCOLT2 Technical Report NC2-TR-1998-030");
      testInstances1.setNumDate((-1));
      MockRandom mockRandom0 = new MockRandom();
      mockRandom0.ints((-1650), (-2));
      testInstances0.setClassType((-1));
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte)68;
      mockRandom0.nextBytes(byteArray0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      MockFile mockFile0 = new MockFile("The tokenizing algorithm to use on the strings.", " ");
      int[] intArray0 = new int[5];
      intArray0[0] = (int) (byte)16;
      intArray0[0] = (-1);
      intArray0[2] = (-1);
      intArray0[3] = (int) (byte)2;
      intArray0[4] = (-1);
      testInstances2.clone();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((byte)16, intArray0, (-1));
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      File file0 = MockFile.createTempFile(".arff", ".bsi");
      naiveBayesMultinomialText0.setStopwords(file0);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
  }

  /**
  //Test case number: 21
  /*Coverage entropy=2.0761045791007673
  */
  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      MockFile mockFile0 = new MockFile("");
      mockFile0.setReadOnly();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.pruneDictionary();
      mockFile0.toURL();
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.stopwordsTipText();
      URI uRI0 = MockURI.aHttpURI;
      URI uRI1 = MockURI.normalize(uRI0);
      MockFile mockFile1 = null;
      try {
        mockFile1 = new MockFile(uRI1);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // URI scheme is not \"file\"
         //
         verifyException("java.io.File", e);
      }
  }

  /**
  //Test case number: 22
  /*Coverage entropy=0.9004024235381879
  */
  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      DenseInstance denseInstance0 = new DenseInstance(161);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance(denseInstance0, true);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 23
  /*Coverage entropy=1.2961346570528898
  */
  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_normalize = true;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 24
  /*Coverage entropy=3.2173802155388804
  */
  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      naiveBayesMultinomialText0.tokenizerTipText();
      TestInstances testInstances0 = new TestInstances();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.setOptions(testInstances0.DEFAULT_WORDS);
      ArrayList<String> arrayList0 = new ArrayList<String>();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      int[] intArray0 = new int[7];
      intArray0[0] = (-1);
      intArray0[1] = (-1);
      intArray0[2] = 0;
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.normTipText();
      naiveBayesMultinomialText0.normTipText();
      FileSystemHandling.shouldAllThrowIOExceptions();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText1.stopwordsTipText();
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      try { 
        naiveBayesMultinomialText2.updateClassifier((Instance) null, false);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 25
  /*Coverage entropy=3.0424877744866063
  */
  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      Tokenizer tokenizer0 = naiveBayesMultinomialText0.getTokenizer();
      naiveBayesMultinomialText0.m_useStopList = false;
      naiveBayesMultinomialText0.m_tokenizer = tokenizer0;
      naiveBayesMultinomialText0.setUseWordFrequencies(false);
      naiveBayesMultinomialText0.toString();
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      naiveBayesMultinomialText0.m_leplace = 1085.156216512885;
      nGramTokenizer0.setDelimiters("indices[i]: ");
      naiveBayesMultinomialText0.setTokenizer(nGramTokenizer0);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText0.setMinWordFrequency(0.0);
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.getMinWordFrequency();
      SGDText sGDText0 = new SGDText();
      sGDText0.listOptions();
      Stemmer stemmer0 = sGDText0.getStemmer();
      naiveBayesMultinomialText0.setStemmer(stemmer0);
      naiveBayesMultinomialText0.reset();
      naiveBayesMultinomialText0.getStemmer();
      CoverTree coverTree0 = new CoverTree();
      BayesNet bayesNet0 = new BayesNet();
      bayesNet0.measureEntropyScore();
      Discretize discretize0 = bayesNet0.m_DiscretizeFilter;
      String[] stringArray0 = new String[6];
      stringArray0[0] = "The LNorm to use for document length normalization.";
      stringArray0[1] = "The LNorm to use for document length normalization.";
      stringArray0[2] = "indices[i]: ";
      stringArray0[3] = "9122";
      stringArray0[4] = "NaiveBayesMultinomialText: No model built yet.\n";
      stringArray0[5] = "NaiveBayesMultinomialText: No model built yet.\n";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(0.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 26
  /*Coverage entropy=2.9252939005386
  */
  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.stopwordsTipText();
      String string0 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
      
      naiveBayesMultinomialText0.useStopListTipText();
      TestInstances testInstances0 = new TestInstances();
      TestInstances testInstances1 = new TestInstances();
      TestInstances testInstances2 = new TestInstances();
      testInstances0.getWords();
      Instances instances0 = testInstances2.generate("NeuroCOLT2 Technical Report NC2-TR-1998-030");
      testInstances1.setNumDate((-1));
      MockRandom mockRandom0 = new MockRandom();
      mockRandom0.ints((-1650), (-2));
      testInstances0.setClassType((-1));
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte)68;
      mockRandom0.nextBytes(byteArray0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      MockFile mockFile0 = new MockFile("The tokenizing algorithm to use on the strings.", " ");
      int[] intArray0 = new int[5];
      intArray0[0] = (int) (byte)16;
      intArray0[0] = (-1);
      intArray0[2] = (-1);
      intArray0[3] = (int) (byte)2;
      intArray0[4] = (-1);
      testInstances2.clone();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((byte)16, intArray0, (-1));
      String string1 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string1);
      
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.setUseStopList(false);
      String string2 = naiveBayesMultinomialText1.normTipText();
      assertEquals("The norm of the instances after normalization.", string2);
      
      String string3 = naiveBayesMultinomialText1.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string3);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      
      String string4 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", string4);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 27
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test27()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = (-1429.74173038344);
      doubleArray0[1] = (-1429.74173038344);
      doubleArray0[2] = (-1429.74173038344);
      doubleArray0[3] = (-1.0);
      doubleArray0[4] = (-1429.74173038344);
      doubleArray0[5] = (-1429.74173038344);
      doubleArray0[6] = (-1429.74173038344);
      SparseInstance sparseInstance0 = new SparseInstance((-1429.74173038344), doubleArray0);
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) sparseInstance0, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 28
  /*Coverage entropy=2.042632211710285
  */
  @Test(timeout = 4000)
  public void test28()  throws Throwable  {
      String string0 = "";
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "");
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.getNormalizeDocLength();
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.toString();
      int[] intArray0 = new int[1];
      int int0 = 1;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance1);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 29
  /*Coverage entropy=3.399200109091609
  */
  @Test(timeout = 4000)
  public void test29()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.globalInfo();
      naiveBayesMultinomialText0.pruneDictionary();
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      naiveBayesMultinomialText0.useStopListTipText();
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      int[] intArray0 = new int[3];
      intArray0[0] = (-1934);
      intArray0[1] = (-1934);
      intArray0[2] = (-1934);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0.0, intArray0, (-1934));
      binarySparseInstance0.setValue(37, (double) (-1934));
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((-1214), intArray0, (-1934));
      Attribute attribute0 = new Attribute("8DCdLan'Wt");
      // Undeclared exception!
      try { 
        binarySparseInstance1.stringValue(attribute0);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // Attribute isn't nominal, string or date!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 30
  /*Coverage entropy=3.5933516469646523
  */
  @Test(timeout = 4000)
  public void test30()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_useStopList = true;
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("\tMaximum number of training instances maintained.\n\tTraining instances are dropped FIFO. (Default = no window)");
      naiveBayesMultinomialText0.getOptions();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.periodicPruningTipText();
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.normTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      
      String[] stringArray0 = new String[0];
      naiveBayesMultinomialText0.setOptions(stringArray0);
      NaiveBayesMultinomialText naiveBayesMultinomialText4 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText4.LNormTipText();
      naiveBayesMultinomialText3.normalizeDocLengthTipText();
      naiveBayesMultinomialText4.stopwordsTipText();
      naiveBayesMultinomialText3.useWordFrequenciesTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText5 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText5.normalizeDocLengthTipText();
      String string0 = naiveBayesMultinomialText5.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
  }

  /**
  //Test case number: 31
  /*Coverage entropy=2.41257681572198
  */
  @Test(timeout = 4000)
  public void test31()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.getRevision();
      Stopwords stopwords0 = naiveBayesMultinomialText0.m_stopwords;
      naiveBayesMultinomialText0.m_stopwords = null;
      naiveBayesMultinomialText0.setMinWordFrequency(2.0);
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.getNormalizeDocLength();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.reset();
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 32
  /*Coverage entropy=2.847205432620848
  */
  @Test(timeout = 4000)
  public void test32()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string0);
      
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      TestInstances testInstances0 = new TestInstances();
      TestInstances testInstances1 = new TestInstances();
      TestInstances testInstances2 = new TestInstances();
      Instances instances0 = testInstances2.generate("NeuroCOLT2 Technical Report NC2-TR-1998-030");
      testInstances1.setNumDate((-1));
      MockRandom mockRandom0 = new MockRandom();
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte)68;
      mockRandom0.nextBytes(byteArray0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      MockFile mockFile0 = new MockFile("");
      int[] intArray0 = new int[5];
      intArray0[0] = (int) (byte)16;
      intArray0[1] = (-1);
      intArray0[2] = (-1);
      intArray0[3] = (int) (byte)2;
      intArray0[4] = (-1);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((byte)16, intArray0, (-1));
      naiveBayesMultinomialText0.setUseWordFrequencies(false);
      String string1 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string1);
      
      naiveBayesMultinomialText0.stopwordsTipText();
      String string2 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string2);
      
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertArrayEquals(new double[] {0.5454545454545454, 0.4545454545454546}, doubleArray0, 0.01);
      
      String string3 = naiveBayesMultinomialText0.normTipText();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", string3);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 33
  /*Coverage entropy=3.1613371021644316
  */
  @Test(timeout = 4000)
  public void test33()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      String string0 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
      
      TestInstances testInstances0 = new TestInstances();
      naiveBayesMultinomialText0.stemmerTipText();
      String string1 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string1);
      
      String string2 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string2);
      
      naiveBayesMultinomialText0.setOptions(testInstances0.DEFAULT_WORDS);
      ArrayList<String> arrayList0 = new ArrayList<String>();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      int[] intArray0 = new int[7];
      intArray0[0] = (-1);
      intArray0[1] = (-1);
      intArray0[2] = 0;
      intArray0[3] = (-2);
      intArray0[4] = (-2);
      intArray0[5] = (-1);
      WordTokenizer wordTokenizer1 = new WordTokenizer();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      String string3 = naiveBayesMultinomialText1.normTipText();
      assertEquals("The norm of the instances after normalization.", string3);
      
      String string4 = naiveBayesMultinomialText1.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string4);
      
      naiveBayesMultinomialText1.setOptions(testInstances0.DEFAULT_WORDS);
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      String string5 = naiveBayesMultinomialText2.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string5);
      
      String string6 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string6);
      
      int int0 = naiveBayesMultinomialText1.getPeriodicPruning();
      assertEquals(0, int0);
      
      naiveBayesMultinomialText1.stemmerTipText();
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
  }

  /**
  //Test case number: 34
  /*Coverage entropy=2.8778952727983707
  */
  @Test(timeout = 4000)
  public void test34()  throws Throwable  {
      FileSystemHandling.shouldAllThrowIOExceptions();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(13);
      double[] doubleArray0 = new double[9];
      doubleArray0[1] = (double) 0;
      doubleArray0[7] = 4.5;
      doubleArray0[3] = (double) 0;
      doubleArray0[4] = (double) 1196;
      doubleArray0[5] = (double) 1196;
      doubleArray0[6] = (double) 0;
      doubleArray0[7] = (double) 0;
      doubleArray0[8] = (double) 0;
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance(1196);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.LNormTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normalizeDocLengthTipText();
      naiveBayesMultinomialText1.setNormalizeDocLength(true);
      naiveBayesMultinomialText1.normTipText();
      naiveBayesMultinomialText1.useStopListTipText();
      assertTrue(naiveBayesMultinomialText1.getNormalizeDocLength());
      
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 35
  /*Coverage entropy=3.5906696134368863
  */
  @Test(timeout = 4000)
  public void test35()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      String[] stringArray0 = new String[7];
      stringArray0[4] = "If true, ignores all words that are on the stoplist.";
      stringArray0[1] = "Whether to convert all tokens to lowercase";
      MockFile mockFile0 = new MockFile("Whether to convert all tokens to lowercase", "If true, ignores all words that are on the stoplist.");
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      FileSystemHandling.createFolder(evoSuiteFile0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("");
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.periodicPruningTipText();
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.normTipText();
      naiveBayesMultinomialText2.setNormalizeDocLength(true);
      assertTrue(naiveBayesMultinomialText2.getNormalizeDocLength());
      
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText1.m_tokenizer = (Tokenizer) wordTokenizer0;
      naiveBayesMultinomialText2.setOptions(stringArray1);
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText1.normalizeDocLengthTipText();
      naiveBayesMultinomialText1.normTipText();
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 36
  /*Coverage entropy=2.063983366979555
  */
  @Test(timeout = 4000)
  public void test36()  throws Throwable  {
      String string0 = "";
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "");
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setPeriodicPruning(65);
      naiveBayesMultinomialText0.getUseStopList();
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "");
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.toString();
      int[] intArray0 = new int[1];
      int int0 = 1;
      intArray0[0] = 1;
      SparseInstance sparseInstance0 = null;
      try {
        sparseInstance0 = new SparseInstance(1.0, (double[]) null, intArray0, 1);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.SparseInstance", e);
      }
  }

  /**
  //Test case number: 37
  /*Coverage entropy=3.3919807851269095
  */
  @Test(timeout = 4000)
  public void test37()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.stemmerTipText();
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      naiveBayesMultinomialText0.m_tokenizer = (Tokenizer) alphabeticTokenizer0;
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.normTipText();
      naiveBayesMultinomialText2.LNormTipText();
      Stopwords stopwords0 = naiveBayesMultinomialText0.m_stopwords;
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.m_stopwords = null;
      naiveBayesMultinomialText2.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText2.useWordFrequenciesTipText();
      naiveBayesMultinomialText2.debugTipText();
      naiveBayesMultinomialText1.tokenizerTipText();
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = 579.396684486;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(579.396684486, doubleArray0);
      try { 
        naiveBayesMultinomialText0.updateClassifier(binarySparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 38
  /*Coverage entropy=3.1613371021644316
  */
  @Test(timeout = 4000)
  public void test38()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      naiveBayesMultinomialText0.tokenizerTipText();
      TestInstances testInstances0 = new TestInstances();
      testInstances0.generate("The tokenizing algorithm to use on the strings.");
      naiveBayesMultinomialText0.stemmerTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.lowercaseTokensTipText();
      naiveBayesMultinomialText1.useStopListTipText();
      naiveBayesMultinomialText0.setLNorm((-1));
      naiveBayesMultinomialText1.setOptions(testInstances0.DEFAULT_WORDS);
      testInstances0.getOptions();
      ArrayList<String> arrayList0 = new ArrayList<String>();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      int[] intArray0 = new int[7];
      intArray0[0] = (-1);
      intArray0[1] = (-1);
      intArray0[2] = 0;
      intArray0[3] = (-1);
      intArray0[4] = (-2);
      intArray0[5] = (-1);
      WordTokenizer wordTokenizer1 = new WordTokenizer();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.normTipText();
      naiveBayesMultinomialText2.useWordFrequenciesTipText();
      naiveBayesMultinomialText2.setOptions(testInstances0.DEFAULT_WORDS);
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText3.LNormTipText();
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals((-1.0), naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 39
  /*Coverage entropy=3.170272076575783
  */
  @Test(timeout = 4000)
  public void test39()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.stemmerTipText();
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      naiveBayesMultinomialText0.m_tokenizer = (Tokenizer) alphabeticTokenizer0;
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.normTipText();
      naiveBayesMultinomialText2.LNormTipText();
      try { 
        naiveBayesMultinomialText2.updateClassifier((Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 40
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test40()  throws Throwable  {
      FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      TestInstances testInstances0 = new TestInstances();
      Instances instances0 = testInstances0.generate(" ");
      MockRandom mockRandom0 = new MockRandom();
      mockRandom0.longs(1265L, (-5672L), 1725L);
      Instances instances1 = instances0.resample(mockRandom0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1498);
      byte[] byteArray0 = new byte[3];
      byteArray0[0] = (byte) (-82);
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance((SparseInstance) binarySparseInstance0);
      instances1.add((Instance) binarySparseInstance1);
      MockRandom mockRandom1 = new MockRandom();
      testInstances0.getRelationalClassFormat();
      MockFile mockFile0 = new MockFile("@relation");
      int[] intArray0 = new int[3];
      intArray0[0] = 1498;
      intArray0[1] = (int) (byte)68;
      intArray0[2] = (int) (byte)107;
      BinarySparseInstance binarySparseInstance2 = new BinarySparseInstance((-2398.0), intArray0, (-1046));
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "");
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.listOptions();
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 41
  /*Coverage entropy=3.5908048399412813
  */
  @Test(timeout = 4000)
  public void test41()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      MockFile mockFile0 = new MockFile("Number of iterations to be performed.", "s8ZvJ7[1KXlHY");
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      FileSystemHandling.createFolder(evoSuiteFile0);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("");
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      String[] stringArray0 = naiveBayesMultinomialText2.getOptions();
      naiveBayesMultinomialText2.periodicPruningTipText();
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      naiveBayesMultinomialText2.setNorm(3.0);
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.normTipText();
      naiveBayesMultinomialText1.useWordFrequenciesTipText();
      naiveBayesMultinomialText2.setOptions(stringArray0);
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText1.normTipText();
      Random.setNextRandom(36);
  }

  /**
  //Test case number: 42
  /*Coverage entropy=1.0931471805599453
  */
  @Test(timeout = 4000)
  public void test42()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double double0 = naiveBayesMultinomialText0.getLNorm();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(2.0, double0, 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 43
  /*Coverage entropy=3.6597476222457437
  */
  @Test(timeout = 4000)
  public void test43()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string0);
      
      String string1 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string1);
      
      naiveBayesMultinomialText0.useStopListTipText();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("The stemming algorithm to use on the words.");
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText1.periodicPruningTipText();
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      naiveBayesMultinomialText0.m_tokenizer = (Tokenizer) alphabeticTokenizer0;
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText2.normTipText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      naiveBayesMultinomialText2.LNormTipText();
      naiveBayesMultinomialText2.normalizeDocLengthTipText();
      String string2 = naiveBayesMultinomialText0.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string2);
      
      String string3 = naiveBayesMultinomialText2.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string3);
      
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      naiveBayesMultinomialText1.tokenizerTipText();
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
  }

  /**
  //Test case number: 44
  /*Coverage entropy=1.830738805564335
  */
  @Test(timeout = 4000)
  public void test44()  throws Throwable  {
      String string0 = "";
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.toString();
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      naiveBayesMultinomialText0.setTokenizer(wordTokenizer0);
      int[] intArray0 = new int[1];
      int int0 = 1;
      SparseInstance sparseInstance0 = null;
      try {
        sparseInstance0 = new SparseInstance(1.0, (double[]) null, intArray0, 0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.SparseInstance", e);
      }
  }

  /**
  //Test case number: 45
  /*Coverage entropy=0.9289738521096165
  */
  @Test(timeout = 4000)
  public void test45()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/termite/projects/107_weka");
      FileSystemHandling.appendLineToFile(evoSuiteFile0, "");
      naiveBayesMultinomialText0.setStopwords((File) null);
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      CostSensitiveClassifier costSensitiveClassifier1 = new CostSensitiveClassifier();
      File file0 = costSensitiveClassifier1.getOnDemandDirectory();
      naiveBayesMultinomialText0.setStopwords(file0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }
}
